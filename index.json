[{"content":"","date":null,"permalink":"/","section":"Abing's Blog","summary":"","title":"Abing's Blog"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"背景介绍 # iLogtail 是阿里云日志服务（SLS）团队自研的可观测数据采集 Agent，拥有的轻量级、高性能、自动化配置等诸多生产级别特性，可以署于物理机、虚拟机、Kubernetes 等多种环境中来采集遥测数据。目前 iLogtail 已有千万级的安装量，每天采集数十 PB 的可观测数据，广泛应用于线上监控、问题分析/定位、运营分析、安全分析等多种场景，在实战中验证了其强大的性能和稳定性。\n为了满足用户对于拓展性的需求，iLogtail 采用了 Go 编写插件系统。在日志采集的场景中，每一条日志都对应了插件系统中一个日志对象。这个对象的生命周期随着在读取日志时开始，在发送日志后结束。然而，再处理大量的日志时，频繁的对象创建和销毁会对 Go 垃圾回收的性能产生一定的要求。\n在这篇文章中，我们将会介绍 iLogtail 中与 Go GC 相关的参数，以及如何根据不同的使用场景对这些参数进行配置。通过深入了解这些参数，用户可以更好地优化 iLogtail 的性能，提高日志采集的效率和准确性。\nGo 垃圾回收 # 在了解 iLogtail 对 Go GC 的管理和优化之前，我们先简单介绍一下 Go 垃圾回收的相关背景知识。\nGC 触发机制 # Go 会在以下三种情况下触发垃圾回收：\n堆大小触发：堆内存的分配达到控制器计算的触发堆大小。触发堆大小的计算公式如下： 触发堆大小 = live heap + (live heap + GC roots) * GOGC / 100 Live heap 是上次 GC 后堆内剩余活跃的对象大小，GC roots 是栈或者常量区中的对象大小，GOGC 是 Go 1.19 之前，关于垃圾回收唯一可配置的参数，默认为 100。\n时间触发：如果一定时间内没有触发，就会触发新的循环，该触发条件由 runtime.forcegcperiod 变量控制，默认为 2 分钟。但这个参数并提供没有接口进行修改。\n手动触发：手动调用 runtime.GC。\nSoft Memory Limit # 在 Go 1.19 中，除了 GOGC 之外，新引入了 Soft Memory Limit，GOMEMLIMIT 这一参数，用于解决 OOM（Out Of Memory）的问题。 首先，我们先来看一下 Go 1.19 之前存在的问题。\n上图很清楚地展示了 OOM 问题发生的场景。从下向上看，蓝色虚线表示 live heap 大小。黑色实线表示触发堆大小。红色虚线表示物理内存大小。在1、2处各发生了一次 GC，从而导致触发堆的大小增长。然而，当第二次 GC 改变触发堆大小后，就超过了实际物理内存大小。但 Go 内存管理器没有感知到这一点，仍然在不断地分配内存。直到达到 hard memory limit，导致 OOM。而在此期间由于没有达到触发堆大小（时间短的情况下，也不会触发定时GC），不会触发GC。\n而 GOMEMLIMIT 的引入就相当于在上图中引入了绿色虚线。为 Go 的内存使用大小设置了一个上限，保证了 Go 的内存使用不会超过物理内存。\n（下图仅为示意，GOMEMLIMIT 限制的不仅仅是堆大小，而是整个 Go 所使用的内存容量）\nGC CPU 占用控制 # 在 Go 1.19 之前，存在着一个问题叫作死亡螺旋（death spirals）。GC 回收器会临时征用用户的工作 goroutine 来进行垃圾回收的工作。当堆越大，GC 需要扫描的空间也就越大，消耗的 CPU 时间也越大。用户工作 goroutine 的 CPU 时间就越少，堆内的对象就一直不会被处理完，无法释放。最终，GC 可能会持续一直的运行。\n为了避免死亡螺旋，Go 1.19 采用了 leaky bucket，对 GC 所能够使用的 CPU 时间进行限制。\n算法实现了一个抽象的桶。当发生 GC 时，会向桶内增加 GC 所耗费的 CPU 时间。而当执行用户任务时，则会从桶中抽出时间。当桶内的水位达到一定阈值后，就会阻止 GC 征用用户 goroutine 来辅助 GC。该功能的作者将 GC 可使用的 CPU 时间占比（桶的水位）设置为了 50%。这避免了频繁 GC 导致的问题。但同时，由于 GC 次数会被强制降速（不能征用用户 goroutine 来辅助了），所以在快到达 soft memory limit 时，有可能稍微超出。\niLogtail 插件启动策略 # 在 Go 1.19 前，为了更加精细地管理 Go 内存使用情况，iLogtail 对于启动或者重启时需要处理大量堆积的历史日志的场景，设计了特殊的内存管理策略，避免启动时对内存造成很大的压力。iLogtail 的插件系统启动策略可以概括为以下两个阶段：\n启动阶段：设置触发堆的增长大小 GOGC 为较小的值，更加频繁地触发 GC（避免内存激增、避免 OOM） 正常阶段：启动一段时间后，恢复触发堆的增长大小为默认设置 然而，这个方法并不是完美的，在 iLogtail 升级 Go 1.19 之前仍然存在着以下两个问题：\n在启动阶段，通过更频繁的 GC 只能尽可能地缓解插件系统的内存使用量，并不能 100% 保证不出现 OOM 的情况。 在保证内存使用量（避免 OOM）的情况下，无法优化 GC 所占用的 CPU 时间。无论在启动或是正常运行阶段，如果为了通过减小 GC 触发堆的增长来避免 OOM，就很容易导致 GC 次数过于频繁，占用大量的 CPU 时间。 升级方案 # 为了优化以上两个问题，iLogtail 从 1.6.0 版本开始，升级支持了 Go 1.19，并向用户提供了 4个与 Go GC 相关的参数，其中两个为 Go 环境变量，两个为 iLogtail 环境变量。\nGOGC # GOGC 为 Go 原生的环境变量，也是在 Go 1.19 之前唯一可以对 Go GC 进行修改的参数。对于 iLogtail，该参数的作用是控制启动结束，策略更改后 GC 的触发堆增长大小。\n默认值为 100。\nGOMEMLIMIT # GOMEMLIMIT 为 Go 原生的环境变量，在 Go 1.19 加入。对于 iLogtail，该参数的作用是控制 Go 所能使用的内存上限。\n默认值为无限制。\nALIYUN_LOGTAIL_GOLANG_GC_PERCENT # ALIYUN_LOGTAIL_GOLANG_GC_PERCENT 为 iLogtail 自身的环境变量。该参数的作用是控制 iLogtail 在启动初始阶段，策略更改前 GC 的触发堆增长大小。\n默认值为 20。\nALIYUN_LOGTAIL_GOLANG_GC_PERCENT_RESUME_SEC # ALIYUN_LOGTAIL_GOLANG_GC_PERCENT_RESUME_SEC 为 iLogtail 自身的环境变量。该参数的作用是控制 iLogtail 启动初始阶段的时间长度。\n默认为 5 分钟，单位为秒。\n具体场景与方案建议 # 本章节结合了刚才所提到的现有启动策略中的两个问题，提出了如何通过配置以上的 4 个参数实现对 GC 优化。有些特殊的 iLogtail 使用场景没有办法统一处理，可以参考下文场景中的优化思路。\n场景一：启动时处理历史日志导致 OOM # 问题描述 # 在 iLogtail 启动或者重启时，有可能需要处理大量累积的历史日志。iLogtail 的内存使用率会出现显著的升高，甚至超出物理内存大小，导致 OOM。\n问题原因 # 一部分原因是因为短时间内处理大量的日志，Go 插件中创建了大量的对象，内存使用量升高。另一部分原因是 Go 的内存管理器无法感知物理内存的上限，会无限扩充可使用的堆的大小。\n解决方案 # 在这种场景下，配置 GOMEMLIMIT 即可解决。\n但 iLogtail 作为一个日志采集工具，往往与其他程序共享计算机资源，在很多场景下，用户可能比较难配置一个明确的内存使用上限。这种情况下，可以使用 iLogtail 现有的插件系统启动策略，设置 ALIYUN_LOGTAIL_GOLANG_GC_PERCENT 为较小的值。通过牺牲短时间内的 CPU 使用率，换取 iLogtail 的平滑启动。\n场景二：GC 占用 CPU 时间过多 # 问题描述 # 客户在使用 iLogtail 采集日志，向 Kafka 发送时，发现性能出现了比较严重的下降。通过 Go pprof 进行分析发现，Go GC （runtime.gcBgMarkWorker）占用了大量的CPU，阻塞了正常的日志处理。\n问题原因 # 在 iLogtail 中，每一条日志都对应了一个 Log 对象。这个对象的生命周期其实特别短，从 input 创建，到 processor，最终 flusher 发送后，这个对象就被释放了。\n对于 50,000 条日志每秒的日志处理来说，那么一秒钟就涉及到了 50,000 * 2 次的对象创建和释放。假设每个对象 1KB，那么每秒就需要申请并释放大概 50MB 内存。\n解决方案 # 在配置了 GOMEMLIMIT 的情况下，如果关闭了触发堆 GC，即 GOGC=off，就只会在内存到达 GOMEMLIMIT 时，才会触发 GC，就可以大大的减少 GC 的次数。\n所以，可以配置以下的优化方案：\n限制 GOMEMLIMIT 如果对于内存的使用量有一定要求，则同时配置 GOGC 为一个较大的值，例如 200、300。减少 GC 的触发频次。 如果对于内存的使用量没有限制，则同时配置 GOGC 为 off 场景三：日志中转服务 # 问题描述 # 用户使用 iLogtail 作为日志中转处理服务，从一个 Kafka 中读取日志，经过 iLogtail 简单处理后，发送到另一个 Kafka 中进行存储。在这个场景下，用户不需要处理历史累积的日志，更加需要 iLogtail 启动后可以更快的提供高性能的服务。\n问题原因 # iLogtail 默认的启动策略导致插件系统在启动时，会牺牲一定的 CPU 时间来换取内存安全。所以，会导致其在 iLogtail 刚启动的一小段时间内无法发挥全部的性能，提供最高效的服务。\n解决方案 # 这种情况下，需要关闭 iLogtail 的默认启动策略。可以将 ALIYUN_LOGTAIL_GOLANG_GC_PERCENT_RESUME_SEC 配置为 0，即在一开始就立刻进入正常运行的 GC 处理策略。\nGOMEMLIMIT 与 GOGC 配置建议 # 在配置 GOMEMLIMIT 时，需要注意 iLogtail 自身的内存使用参数 memory_usage_up_limit。该参数同时限制了 C++ 程序和 Go 插件系统总体的内存使用量，默认为 2GB。如果使用量超出该限制，会导致 iLogtail 发生重启。因此，GOMEMLIMIT 的值应当低于 memory_usage_up_limit。 配置 GOGC=off 但不设置 GOMEMLIMIT 会导致 Go 插件系统中只有定时 GC，不推荐进行这样的配置。 GOMEMLIMIT 设置了 Go 插件系统可使用的内存上限。但需要注意的是，Go 并不保证所使用的内存一定低于该上限。在 GC 发生拥塞的情况下，有可能由于垃圾回收不及时，导致使用量稍微超出该上限。所以，推荐将 GOMEMLIMIT 设为 Go 可使用内存上限的 95% ~ 96%。 当无法确定与 iLogtail 在相同环境下运行的其他程序的内存使用大小时，不推荐将 GOGC 设置为 off，因为会导致 iLogtail 插件占用大量无效内存却不释放。如果这种情况下出现 GC 频繁的问题，推荐将 GOGC 设置为一个较大的值。 参考资料 # https://github.com/golang/proposal/blob/master/design/48409-soft-memory-limit.md https://github.com/golang/go/issues/48409 https://netflixtechblog.medium.com ","date":"2023-10-27","permalink":"/posts/ilogtail-go-gc/","section":"Posts","summary":"背景介绍 # iLogtail 是阿里云日志服务（SLS）团队自研的可观测数据采集 Agent，拥有的轻量级、高性能、自动化配置等诸多生产级别特性，可以署于物理机、虚拟机、Kubernetes 等多种环境中来采集遥测数据。目前 iLogtail 已有千万级的安装量，每天采集数十 PB 的可观测数据，广泛应用于线上监控、问题分析/定位、运营分析、安全分析等多种场景，在实战中验证了其强大的性能和稳定性。","title":"iLogtail Go 插件内存 GC 优化最佳实践"},{"content":"","date":null,"permalink":"/tags/log/","section":"Tags","summary":"","title":"Log"},{"content":"","date":null,"permalink":"/categories/observability/","section":"Categories","summary":"","title":"Observability"},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"功能对比 # Filebeat Fluent Bit Vector 正则 Y Y Y 多种方式处理非匹配行 Y Y Y 多种方式聚合日志 Y 匹配到pattern时自动flush Y 日志达到一定数量自动flush Y 最大行数 Y 超时自动flush Y Y Y 日志之间插入换行 Y 内置模板 Y 聚合前解析结构 Y 总结：\n从功能方面，Filebeat 所支持的功能最多，其次是 Fluent Bit。 从配置方面，Filebeat 通过 negate 和 match 两个字段的排列组合简化了配置，但对于用户有一定理解成本。而 Fluent Bit 所提供的基于状态的配置比较清晰易懂，但会导致配置文件结构复杂。 从实现方面，前三者都采用了维护一个 buffer 的形式。 Filebeat # 文档 # 链接： https://www.elastic.co/guide/en/beats/filebeat/current/multiline-examples.html\n支持功能：\n多种方式聚合多行日志，例如，pattern，count，while_pattern\n正则匹配\n多种方式处理非匹配行\n个人理解：negate 相当于是区分判断 event 的主体是匹配行还是非匹配行。match 相当于是控制多行日志中的其他行附着到哪个主体（前or后）上。\n匹配到指定 pattern 时进行 flush 指定数量的日志聚合成一个 event 一个event最大行数 超时自动flush 多个event之间换行 样例：\nparsers: - multiline: type: pattern pattern: \u0026#39;^[[:space:]]\u0026#39; negate: false match: after 以下的 log 会被解析成一条： Exception in thread \u0026#34;main\u0026#34; java.lang.NullPointerException at com.example.myproject.Book.getTitle(Book.java:16) at com.example.myproject.Author.getBookTitles(Author.java:25) at com.example.myproject.Bootstrap.main(Bootstrap.java:14) 解释： 这里pattern匹配的是前面带有空格的行，因此，多行的开始Exception属于不匹配行。由于 negate 为 false，所以作为 event 的主体。后续匹配的at会聚合成一个事件。\n实现 # 链接：https://github.com/elastic/beats\nFilebeat (libbeat) 在 reader 模块实现了 multiline 的功能。\n根据配置项，在创建 parser 时，创建了 multiline 的 parser（reader）。根据不同的聚合模式，这里就会创建不同的 reader。 在处理日志时，会不断地调用 Next 方法。 Next 方法中的 state 函数，会根据当前的匹配状态（是否匹配了第一行）执行不同的逻辑。 如果没有 buffer 为空，则 readFirst 中会在 Message Buffer 中创建一个新的 Message。然后进入 readNext 逻辑。 readNext 中会不断从 file 的 reader 中读取日志，并添加到 Message Buffer 中。直至两种情况下，flush 日志，并将 state 重置为 readFirst： 匹配到强制 flush 的 pattern 根据 negate 和 match 无法匹配成一条 message Fluent Bit # 文档 # 链接：https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/multiline-parsing\n支持功能：\n内置多种类型日志的多行解析：docker，cri，go，python 和 java 正则匹配 支持匹配前解析，将文本解析成结构体 支持对结构体指定字段进行匹配 超时自动flush fluent bit 将多行匹配问题，建模成了一个状态机。每一条规则包含三个部分：\nstate name：一个multiline log的开始状态（start_state），还是继续状态（cont） 正则表达式 next state：这个状态后可能是什么状态 样例：\n# fluent bit 配置 [SERVICE] flush 1 log_level info parsers_file parsers_multiline.conf # 指定了 multiline 的规则 [INPUT] name tail path test.log read_from_head true multiline.parser multiline-regex-test # 指定了 multiline 的 parser [OUTPUT] name stdout match * # multiline 的规则 [MULTILINE_PARSER] name multiline-regex-test type regex flush_timeout 1000 # # Regex rules for multiline parsing # --------------------------------- # # configuration hints: # # - first state always has the name: start_state # - every field in the rule must be inside double quotes # # rules | state name | regex pattern | next state # ------|---------------|-------------------------------------------- rule \u0026#34;start_state\u0026#34; \u0026#34;/([a-zA-Z]+ \\d+ \\d+\\:\\d+\\:\\d+)(.*)/\u0026#34; \u0026#34;cont\u0026#34; rule \u0026#34;cont\u0026#34; \u0026#34;/^\\s+at.*/\u0026#34; \u0026#34;cont\u0026#34; # 可以被解析的规则 Dec 14 06:41:08 Exception in thread \u0026#34;main\u0026#34; java.lang.RuntimeException: Something has gone wrong, aborting! at com.myproject.module.MyProject.badMethod(MyProject.java:22) at com.myproject.module.MyProject.oneMoreMethod(MyProject.java:18) at com.myproject.module.MyProject.anotherMethod(MyProject.java:14) at com.myproject.module.MyProject.someMethod(MyProject.java:10) at com.myproject.module.MyProject.main(MyProject.java:6) 解释： start_state 是以时间格式的正则匹配的，匹配到后，进入第二条 rule（以at开头）。然后不断匹配，直到遇到某一行可以被 start_state 所匹配。\n实现 # 链接：https://github.com/fluent/fluent-bit/tree/master/src/multiline\nFluent bit 从 v1.8 开始，就开始在 core 中支持了 multiline。为了兼容之前版本，依然保留了以插件的形式支持。\n在初始化时，会自动将所有 built-in 的 parser 加入了 context 的 multiline_parsers 链表中。 接下来，会根据用户的配置，创建用户自定义的 parser，同样加入到链表中。这里 fluent bit 还做了一点优化，会维护一个 lru_parser，优先尝试使用上次的 parser 进行解析，避免对 multiline_parsers 进行查找。 经过一系列调用后，核心的处理逻辑位于函数 flb_ml_rule_process 中。 首先，会使用 start_state 的 rule 进行匹配。如果成功，则后续不断使用 cont 的 rule 进行匹配，并将日志加入到 buf 中。 在两种情况下，可能发生 flush，并回到 start_state 状态： 匹配的 rule 的后续状态为 start_state 当前日志被 start_state 的 rule 匹配 Vector # 文档 # 链接：https://vector.dev/docs/reference/configuration/sources/file/#multiline\nVector 将多行日志的处理理解成了一种聚合方式。\n支持功能：\n正则匹配 多种方式处理非匹配行 超时自动flush 样例：\n# vector 配置 [sources.my_file_source] type = \u0026#34;file\u0026#34; [sources.my_file_source.multiline] start_pattern = \u0026#34;^[^\\\\s]\u0026#34; mode = \u0026#34;continue_through\u0026#34; condition_pattern = \u0026#34;^[\\\\s]+from\u0026#34; timeout_ms = 1000 # 可以被解析的日志 foobar.rb:6:in `/\u0026#39;: divided by 0 (ZeroDivisionError) from foobar.rb:6:in `bar\u0026#39; from foobar.rb:2:in `foo\u0026#39; from foobar.rb:9:in `\u0026lt;main\u0026gt;\u0026#39; 解释：任何非空白开头的日志会被当做一个event的开始。然后匹配到 from 后，会根据 mode: continue_through 继续匹配。\n实现 # 链接： https://github.com/vectordotdev/vector/blob/master/src/line_agg.rs\nVector 中的多行处理是通过 LineAgg 类实现。 在处理日志时，会不断地调用其 poll_next 方法。该方法中，会不断地读取日志，并根据 inner 读取的结果进行不同的处理。 如果是 pending 状态，即没有新的日志需要处理。则会从 self.timeouts (实现上是一个 delayQueue）中去 flush 过期的日志。 如果是 ready 状态，则处理新的日志。如果当前 buffer 为空，则直接 insert。同时 insert 进 timeout。 如果当前 buffer 不为空，则根据配置采用不同的规则进行处理（add_next_line 或者 flush）。 ","date":"2023-05-12","permalink":"/posts/multiline/","section":"Posts","summary":"功能对比 # Filebeat Fluent Bit Vector 正则 Y Y Y 多种方式处理非匹配行 Y Y Y 多种方式聚合日志 Y 匹配到pattern时自动flush Y 日志达到一定数量自动flush Y 最大行数 Y 超时自动flush Y Y Y 日志之间插入换行 Y 内置模板 Y 聚合前解析结构 Y 总结：","title":"常见日志采集器多行日志处理能力对比"},{"content":"2022 流水账 # 1月份在学校里做实验，写论文，尝试自己从零开始实现并训练模型，结果失败了。同时，人际关系经受了重大失败。很难受，灰头土脸地滚回了家。\n2月份在家度过了春节后，又马不停蹄地回学校继续赶论文了。\n3月份上海封城。当时，谁也没能够想到这场封城会持续三个月之久。万幸的是，在封校的当前，我买的 iPad mini 恰好到货了，算是缓解了一点我之后枯燥的隔离日子。\n4月份在逼仄的宿舍里，坐着硬板凳，盯着13寸的 mac 屏幕，终于把人生中的第一篇论文投出去了。虽然当时也意识到了这篇论文属于仓促之作，中的可能性不是很高，但这个过程中还是学到了很多东西。另外，在投稿之后的假期中，入坑了原神。（确实好玩\n5月份，彻底进入了摆烂状态，开源项目也不搞了，横向项目的代码也不重构了，每天过得浑浑噩噩的，人生陷入了彻底的失败。每天也不学习，睡醒了就打游戏，然后再接着睡，是我整个人生最糟糕的时期。\n6月份，终于从学校逃了出来。但不幸的是，奶奶走了。这是我第一次面对亲人的离去。除了悲伤之外，心里总有种空落落的感觉。经过了半年，我才意识到这是失去的遗憾。从这之后，我才意识到了亲人的重要性。在之前的23年里，我一直认为亲人都是理所应当的存在，只要你回头，他们永远在那里。但转眼间，自己也到了有可能面对离别的年纪。现在每当回想起来，我感觉到的不只是悲伤，更多的是遗憾。没有珍惜奶奶还在的时光，没有做完一个孙子该做的事情，再也见不到奶奶了。除此之外，在奶奶病重的期间，我还在做着人生最后一个课程项目，一个设计的跨领域项目。在做这个项目的过程中，我也更加地意识到了自己的局限，自己是如此的普通，不是什么事情都能做好的。想要的太多，反而什么都得不到。更加应该珍惜眼前，珍惜家人。另外，对于有所争议的放开还是封控，经历过这件事后，我成为了坚定的放开派。如果不是封控导致只有一个无良医院接受病人，奶奶的手术也不会失败，也不会一个小手术就导致一直昏迷。在评价任何事情之前都要问一句，“那么，代价是什么呢？”\n7月份，回家之后工作状态异常的好，也算顺利的完成了横向项目的结项答辩。另外也加入了一个新的开源社区，做了一些小的贡献。\n8月份，进入了华子进行实习，由于第一次投稿的论文果然没中，实习过程中又重新开始优化。同时，顺利地完成了合作项目的专利申请。由于加入的是一个偏研究的lab，第一次见识到了博士们的工作方式。我认为博士的学历给他们带来的不仅仅是对一个领域的深入研究，更多的是对方法论的掌握。即使面对一个新的领域，他们做事的方法和态度仍然可以帮助他们快速上手。但对于华子的工作环境和方式，我。。。\n9月份，依旧在华子实习。论文的模型有了质的飞跃，生成出来的故事具有了一定的可读性，算是满足了甲方的需求。但在开源社区里就比较失败了，之前的设计被疯狂 challenge，自己在架构设计上还存在着很多的不足。另外，由于实习期间一直自己租房住，也有了充足的自省时间，对自己的各个方面都进行了思考。\n10月份，结束了实习，回到了学校。其实与在华子区别不大，依旧是在忙论文的事情。\n11月份，算法基本上没什么需要调整的了，然后开始了 writing 。上一个版本的论文，我导还在同时忙其他两篇论文（最后另外两篇都中了 honorable mention，太牛了），所以对我的写作没太高的要求。但这次不同，对我的论文各种 challenge。自己在这个过程中真的学到了很多关于如何表达自己思想的 tips，也算是一个很好的成长吧。\n12月份，依旧写论文，依旧痛苦。但对于疫情防控，国内突然转向。实验室也催促我们回家。在家里，我对自己的写作疯狂进行反思，终于算是摸到了一点门道。在这里，真的要感谢我导和我的大老板。他俩一个非常耐心的帮助我改论文，一个严厉地让我认识到自己的问题（虽然不一定是正确的方式）。也算是国内为数不多真的是把教育放在心里的实验室了。至此，也打消了我对读研的疑惑，不亏。\n总的来说，2022年的每个月都有着不同的失败。这一年，我的开源项目荒废，科研毫无成果，人际关系也毫无建树。彻底失败！\n2023 展望 # 2022 过得如此失败，虽然我也知道，时间只是人为定义的概念，我的状态不会随着2022年23点59分59秒的度过而突然发生质变。但对于 2023 年的展望，还是有一些期待的。\n找工 # 2023年对我来说是人生中最重要的一年。需要找到人生的第一份工作。对于这份工作的选择，大概有这几点考虑吧\n钱 （生活性）WLB （成长性）能够解决一些有挑战性的问题 （影响性）深入使用和定制化一些开源组件，并反馈到社区中 生活 # 2022年是我逐渐将重心转向生活的一年。2023年里，希望可以继续保持，逐渐把握好生活和工作的平衡。毕竟脱离了学生身份后，工作就只是人生的一部分，生活中还有很多零零碎碎的事情需要处理。其中，既包含了如何把生活过好，也包含了对亲人的关心。\n最后，2023年见！\n","date":"2022-12-31","permalink":"/posts/2022/","section":"Posts","summary":"2022 流水账 # 1月份在学校里做实验，写论文，尝试自己从零开始实现并训练模型，结果失败了。同时，人际关系经受了重大失败。很难受，灰头土脸地滚回了家。","title":"2022 年终总结 —— 彻底失败"},{"content":"","date":null,"permalink":"/tags/k8s/","section":"Tags","summary":"","title":"K8s"},{"content":" Sealos 是什么？ # Kubernetes（K8s）发展至今，已经成为了一个极其复杂的系统。而作为云原生的基石，涌现了一大批辅助工具，帮助用户快速搭建 k8s 集群。而其中，sealos 是一个以 kubernetes 为内核的云操作系统，kuberentes 生命周期管理是 sealos 的一个重要功能，sealos 可以非常方便的安装/升级/伸缩/备份恢复集群等。接下来，让我们通过一个例子来看看 sealos 的强大。\n如何使用 # 假设我们想要在 192.168.0.100，192.168.0.101 和 192.168.0.102 这三台机器上部署一主两从的 K8s 集群，那么使用 Sealos 的话，主要输入以下的命令：\nsudo sealos run labring/kubernetes:v1.24.0 labring/calico:v3.22.1 \\ --masters 192.168.0.100 \\ --nodes 192.168.0.101,192.168.0.102 \\ --passwd xxx 是的，Sealos 将一个复杂的 K8s 的集群部署简化成了短短一行命令，将部署体验拉到了极致。甚至不需要过多的文档解释，仅凭这一行命令就可以满足大多数普通的部署场景。\n原理 # 那么，如此强大的 Sealos 是如何运行的呢？除了刚才演示的 run 命令之外，Sealos 还提供了大量提升用户体验的命令，例如 create 创建镜像、reset 格式化集群等等。由于篇幅有限，本章节就以最核心的 run 命令为例，看看 Sealos 在背后替用户完成了哪些自动化的操作。（本文以 Sealos V4.1.0 代码为例）\nApplier # 首先，Sealos 会创建一个 Applier 结构体，负责了部署集群的核心逻辑。Applier 采用了 k8s 的声明式的设计思想，用户声明一个期望的集群状态，而 Applier 负责将集群现在的状态转换成用户期望的状态。\ntype Applier struct { ClusterDesired *v2.Cluster // 用户期望的集群状态 ClusterCurrent *v2.Cluster // 集群当前状态 ClusterFile clusterfile.Interface // 当前集群接口 Client kubernetes.Client CurrentClusterInfo *version.Info RunNewImages []string // run 命令新增的镜像名称 } clusterfile.Interface 是一个接口类型，Sealos 中通过 ClusterFile 实现了这一接口。因此，Applier 结构体中最重要的就是 Cluster 和 ClusterFile 这两个类型，它们定义了集群的状态和配置。接下来，我们展开介绍一下两者。\nCluster # type Cluster struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Spec ClusterSpec `json:\u0026#34;spec,omitempty\u0026#34;` Status ClusterStatus `json:\u0026#34;status,omitempty\u0026#34;` } type ClusterSpec struct { Image ImageList `json:\u0026#34;image,omitempty\u0026#34;` SSH SSH `json:\u0026#34;ssh\u0026#34;` Hosts []Host `json:\u0026#34;hosts,omitempty\u0026#34;` Env []string `json:\u0026#34;env,omitempty\u0026#34;` Command []string `json:\u0026#34;command,omitempty\u0026#34;` } type ClusterStatus struct { Phase ClusterPhase `json:\u0026#34;phase,omitempty\u0026#34;` Mounts []MountImage `json:\u0026#34;mounts,omitempty\u0026#34;` Conditions []ClusterCondition `json:\u0026#34;conditions,omitempty\u0026#34; ` } Cluster 的内容按照 K8s Resource 的格式进行了设计，这非常的 K8s 哈哈。在 ClusterSpec 中，定义了一系列用于部署 K8s 集群的参数，例如，镜像、SSH参数、节点等等。\n而在 ClusterStatus 中，Phase 定义了当前集群的状态，Mounts 定义了集群使用的镜像，Conditions 保存了集群中所发生的一系列事件。\nClusterFile # type ClusterFile struct { path string // 保存路径 customValues []string customSets []string customEnvs []string Cluster *v2.Cluster // 集群状态 Configs []v2.Config KubeConfig *runtime.KubeadmConfig // 集群配置 } ClusterFile 是真正被 Applier 操作的对象，以及持久化到文件中的内容。这里包含了所有集群的当前状态信息，同时还包含了 kubeconfig。这里的 kubeconfig 并不是我们平时操作 k8s 时所用的 config 文件，而是一系列用于搭建集群所需的配置项。在使用 kubeadm 时，这些配置项往往需要我们手动配置，而 Sealos 在这里会自动帮我们填写并应用于集群中。可以看出，Cluster 更像是 ClusterFile 的一个实例，记录了集群实时的状态。\n创建 Applier # 创建一个 Applier 会经过以下步骤：\n判断是否已经存在 ClusterFile ，如果存在，那么直接读取，构建出集群状态 Cluster。否则，初始化创建一个空的集群状态 Cluster。\n根据用户本次的参数，更新集群状态 Cluster 中的 spec，此时，Cluster 即为目标的集群状态。\n再次从文件中构建 ClusterFile，作为集群当前的状态和对象。\n构建 Applier 结构体返回。\nApply # 接下来，通过 Applier.Apply()，Sealos 开始正式的部署集群，使集群状态向目标靠近。首先，Sealos 会将当前集群的状态置为 ClusterInProcess。接下来，根据集群创建或是更新，分别进入两个分支。\ninitCluster # initCluster 负责从零开始创建一个集群。函数中会通过 CreateProcessor 去部署期望状态的集群。\ntype CreateProcessor struct { ClusterFile clusterfile.Interface // 当前集群对象 ImageManager types.ImageService // 处理镜像 ClusterManager types.ClusterService // 管理 clusterfile RegistryManager types.RegistryService // 管理镜像 registry Runtime runtime.Interface // kubeadm 对象 Guest guest.Interface // 基于 sealos 的应用对象 } CreateProcessor.Execute 接收期望的集群状态 ClusterDesired。接下来会执行一系列 pipeline，正式进入实际的集群部署过程中：\nCheck：检查集群的 host\nPreProcess：负责集群部署前的镜像预处理操作，在这里就会利用 CreateProcessor 中的各个 Manager。\n拉取镜像\n检查镜像格式\n使用 buildah 从 OCI 格式的镜像中创建 working container，并将容器挂载到 rootfs 上\n将容器的 manifest 添加到集群状态中\nRunConfig：将集群状态中的 working container 导出成 yaml 格式的配置并持久化到宿主机的文件系统中\nMountRootfs：将挂载的镜像内容按照类别，以 rootfs，addons，app 的顺序分发到每台机器上。\n这里需要介绍一下 sealos 镜像的一般结构，以最基础的 k8s 镜像为例：\nlabring/kubernetes - etc // 配置项 - scripts // 脚本 - init-containerd.sh - init-kube.sh - init-shim.sh - init-registry.sh - init.sh - Kubefile // dockerfile 语法，定义了镜像的执行逻辑 K8s 作为整个集群的基础，虽然最终镜像内的目录结构与其他一致，但其构建过程稍微有所不同。在 CI https://github.com/labring/cluster-image/blob/faca63809e7a3eae512100a1eb8f9b7384973175/.github/scripts/kubernetes.sh#L35 中，我们可以看到，k8s 镜像其实是合并了 cluster-image 仓库下的多个文件夹，containerd，rootfs 和 registry。这些独立的文件夹中包含有安装对应组件的脚本。\nSealos 在挂载一个镜像后，会首先执行 init.sh 脚本。例如，以下是 k8s 镜像的脚本中，分别按顺序执行了 init-containerd.sh 安装 containerd，init-shim.sh 安装 image-cri-shim 和 init-kube.sh 安装 kubelet。\nsource common.sh REGISTRY_DOMAIN=${1:-sealos.hub} REGISTRY_PORT=${2:-5000} # Install containerd chmod a+x init-containerd.sh bash init-containerd.sh ${REGISTRY_DOMAIN} ${REGISTRY_PORT} if [ $? != 0 ]; then error \u0026#34;====init containerd failed!====\u0026#34; fi chmod a+x init-shim.sh bash init-shim.sh if [ $? != 0 ]; then error \u0026#34;====init image-cri-shim failed!====\u0026#34; fi chmod a+x init-kube.sh bash init-kube.sh logger \u0026#34;init containerd rootfs success\u0026#34; 在 MountRootfs 这步中，只会执行 rootfs 和 addons 类型的 init.sh 脚本。这也很好理解，因为到目前为止，Sealos 仅仅在每台机器上安装成功了 kubelet，整个 k8s 集群还未可用。\nInit：初始化 k8s 集群。在这步中，其实也是执行了一系列的子操作。\nSealos 会从 ClusterFile 中加载 kubeadm 的配置，然后拷贝到 master0 上。\n根据 master0 的 hostname 生成证书以及 k8s 配置文件，例如 admin.conf，controller-manager.conf，scheduler.conf，kubelet.conf。\nSealos 将这些配置以及 rootfs 中的静态文件（主要是一些 policy 的配置）拷贝到 master0 上。\nSealos 通过 link 的方式将 rootfs 中的 registry 链接到宿主机的目录上，然后执行脚本 init-registry.sh，启动 registry 守护进程。\n最后也是最重要的，初始化 master0。首先，将 registry 的域名，api server的域名（IP 为 master0 的 IP）添加到 master0 宿主机上。然后，调用 kubeadm init 创建 k8s 集群。最后，将生成的管理员 kubeconfig 拷贝到 .kube/config。\nJoin：使用 kubeadm 将其余 master 和 node 加入现有的集群，然后更新 ClusterFile。此时，整个 k8s 集群就已经搭建完毕了。\nRunGuest: 运行所有类型为 app 的镜像的 CMD，安装所有应用。\n至此一个 k8s 集群以及基于这个集群的所有应用都被安装完毕。\nreconcileCluster # 第二个分支是负责集群的更新，大部分内容与 initCluster 都比较类似。执行主要包含了以下几步：\nConfirmOverrideApps: 确认是否覆盖已有的应用。\nPreProcess, RunConfig, MountRootfs, RunGuest: 都与 initCluster 类似。\nPostProcess: 执行一些安装后的操作，但目前似乎并没有进行任何操作。\n不仅仅如此\u0026hellip; # 经过上文的介绍，可以看到 Sealos 对于 Kubernetes 生命周期管理有着非常好的抽象，而不只是个简单的安装脚本，你甚至可以扩展其它的 runtime 来支持 k3s k0s 等，而大部分定制化只需要修改集群镜像而不用修改 sealos 的源代码。不仅如此，sealos 还可以让你像使用 PC 操作系统一样用云，各种分布式软件信手拈来，真正让用云的门槛降到足够低。\n","date":"2022-10-30","permalink":"/posts/sealos/","section":"Posts","summary":"Sealos 是什么？ # Kubernetes（K8s）发展至今，已经成为了一个极其复杂的系统。而作为云原生的基石，涌现了一大批辅助工具，帮助用户快速搭建 k8s 集群。而其中，sealos 是一个以 kubernetes 为内核的云操作系统，kuberentes 生命周期管理是 sealos 的一个重要功能，sealos 可以非常方便的安装/升级/伸缩/备份恢复集群等。接下来，让我们通过一个例子来看看 sealos 的强大。","title":"Sealos is All You Need —— 3分钟部署 Kubernetes"},{"content":"","date":null,"permalink":"/categories/xxx-is-all-you-need/","section":"Categories","summary":"","title":"XXX is All You Need"},{"content":"","date":null,"permalink":"/categories/awesome-tech-post/","section":"Categories","summary":"","title":"Awesome Tech Post"},{"content":"每期 Awesome Tech Post 都会摘录推荐 5 篇优质技术博客，对这些文章的内容进行提炼总结。每期覆盖领域各不相同，可能从后端到可视化，从工程到算法等等，但每篇文章都会对领域内的某一问题进行深入分析或提出独到见解。欢迎大家私信推荐文章~\nTL;DR # 本期内容：\n如何对 Kubernetes Operator 进行分布式 Tracing Cloud\nKubernetes 多集群管理与联邦 Cloud\n强化学习中对无效动作的 mask Reinforcement Learning\n一些深度学习中的采样方式和损失函数 Deep Learning\n如何在一个全新的领域开展学习 Soft Skill\n0. How to Monitor Kubernetes Operators By Distributed-Tracing? # 原文链接：https://yue9944882.github.io/posts/how-to-monitor-kubernetes-operator-by-distributed-tracing/\n在一个健全的系统中，应当对一条请求在完整生命周期中完成了哪些处理都进行监控追踪。随着现在大量的分布式应用和微服务的落地，一条请求可能跨越多个服务，甚至集群。对于这类请求的 tracing（追踪）问题就是 distributed tracing。Tracing 的整个流程可以被建模成一个树，其中每个节点是请求所经过的处理（根据监控的粒度，可以是一个服务，也可以是一个函数）。请求经过的每个处理被称作为 span。\n异步问题 # 在微服务中，一条请求 R 到达微服务1后，一般通过 HTTP 或 RPC，进一步请求到微服务2进行处理，再到微服务N。最后，沿着这一条链，进行反向的返回 response。显然，这是一个同步的过程，我们可以很清晰的看出 R 的处理流程，R 的 tracing 结果就是这条转发树。\n然而，与微服务不同，k8s operator 采用了完全不同的协作模式。K8s operator 不会与其他 operator 直接进行交互，而是生成一个 Event（例如，创建一个 Pod），k8s 会将这个 Event 放入到一个队列中。所有的 operator 都不断地轮询，从这个队列中获取符合自己过滤条件的 Event。显然，这是一个异步的过程。一异步，问题就麻烦了：\nEvent 的生成/消费不是线性的\nEvent 的生成和消费不是一一对应的。一方面，operator 可能将多个 Event 合并成一个任务，或者将一个 Event 分成多个任务。另一方面，一个任务由于重试策略可能会执行多次（在一个 operator 上产生多个 span）。\nEvent 循环\n当一个 operator 根据 Event 改变 k8s 的资源后，又会产生一个新的 Event（k8s 中有资源被改变）。这就提出了一个问题，什么是这次 tracing 的结束？详细的讨论可以参考原文。\nOperator 分类 # 每个 operator 都可能监听一个主要的 resource 和多个次要的 resource，因此，可以对 operator 进行如下的分类：\nType A：operator 只接收 Event。\nType B：operator 接收 Event，对 k8s 外的系统进行操作。\nType C：operator 只修改自己监听的资源。\nType D：operator 只修改不被自己监听的资源。\nType E：operator 修改任意的资源（Type C + Type D）。\n对于 Type A 和 Type B 来说，请求到他们这里就结束了，所以他们是 leaf span。\n对于 Type C 和 Type D 来说，由于不能确定有没有其他 operator 在监听同一个资源，所以无法判断其是否是 leaf span。对于 Type C 可以肯定的是，在 operator 完成最后一次 write 的时候，他仍会收到一个 Event（因为它所修改的资源正是自己监听的资源），并且会 drop 这个 Event（这个 Event 是由自身修改产生的，无意义）。因此，我们可以确定这个 operator 上多次 span 的 parent/child 关系。而对于 Type D，无法收到修改资源的最后一次 write 的 Event，所以，我们只能建立这个 operator 上多次 span 之间较弱的 link 关系。\n对于 Type E 来说，这是最复杂但又是最常见的类型。Type E 其实是 Type C 和 Type D 的组合，所以对于 Type E 操作的每个资源，我们可以按照资源的类型，将 Type E 当前的 span 暂时转换成 Type C 或 Type D 来处理。\n1. Kubernetes、集群联邦和资源分发 # 原文链接：https://draveness.me/kuberentes-federation/\nKubernetes 目前最多可以支持管理 5000 个节点，对于超过 5000 个节点的集群管理，就需要寻找其他方法对多个 K8s 集群进行管理。多集群其实不是一个新的概念，在很久之前，就在业界看到过 Mesos + K8s 的多集群管理方法。但是，多集群中的每个集群都相对独立，彼此之间没有联系，每个服务都是独立的运行在一个集群里的。而集群联邦则是在此基础上增加了两个重要的功能：跨集群的服务发现和跨集群的调度，使得一个多应用服务可以运行在多个集群上，进一步提升了服务的稳定性和可用性。\n文章中，作者以两个比较出名的集群联邦项目为例，介绍了目前集群联邦的方案：\nKubefed 会为每个原生资源（e.g. Deployment）生成对应的联邦资源（e.g. FederatedDeployment）作为管理。联邦资源中会包含 Template（资源信息）、Placement（所需部署的集群）和 Overrides（同步资源到集群时，需要覆写的属性）三个部分。在分发到下游集群时，Kubefed 再根据联邦资源生成具体的原生资源。\nKarmada 是 Kubefed 项目的延续，其中的概念也几乎全盘继承自 Kubefed。稍有不同的是，Karmada 保留了原生资源，并将 Kubefed 中联邦资源的 Placement 和 Override 抽离了出来，作为两个新的自定义资源 PropagationPolicy 和 OverriderPolicy。\n图片来自原文 对于任务调度来说，文章中提到了“因为上下文的不足，集群联邦不需要也没有办法保证调度的全局最优解，而提供跨集群的部署和故障转移就已经可以满足常见的需求了”。\n2. A Closer Look at Invalid Action Masking in Policy Gradient Algorithms # 原文链接：https://costa.sh/blog-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms.html\n本篇文章是作者对所发表的同名论文 https://arxiv.org/abs/2006.14171 的介绍。对强化学习稍微有所了解的同学应该都知道 Policy Gradient，属于强化学习的两大分类之一。而 Invalid Action（无效动作）是强化学习中经常遇到的问题，例如，在训练模型走迷宫时，前方有障碍物，那么前进这一动作就是 invalid 的。那么，在训练时，需要对模型过滤掉这类动作，也就是 masking。而本篇文章就在尝试解释 Policy Gradient 算法中 invalid action masking 的工作原理。虽然 masking 在很多论文里都用到了，但都只是一句话带过（我之前读到的几篇甚至不会提到这些细节），没有对 masking 的原理进行深入探索。这也是文章作者的 motivition 之一。\n简单来说，invalid action masking 就是在模型根据概率采样动作时，采用一个 mask 将 invalid action 的概率置为 0。文章中作者将 invalid action masking 建模成以下的函数 \\(inv_s\\)：\n\\(l(s)\\) 是状态 \\(s\\) 的 log 值。\\(inv_s\\) 在两种情况下都是可微的，在常数 \\(M\\) 时，梯度为0，因此，反向传播时不会更新模型有关 invalid action 相关的参数。\npolicy 在采样时的概率为：\n文章中还通过量化的实验结果来验证 invalid action masking 的有效性，详情可以阅读原文。\n3. 深度学习新的采样方式和损失函数\u0026ndash;论文笔记 # 原文链接：https://zhuanlan.zhihu.com/p/27748177\n本篇文章是对论文《Sampling matters in deep embedding learning》https://arxiv.org/pdf/1706.07567.pdf 的概述。论文主要解决的是 deep embedding learning 中的采样问题和损失函数问题。文章对论文的主要内容进行了很好的概述，这里就不再赘述了，就简单的罗列一些 insight 和文章中没解释清楚的部分：\nTriplet loss 优于 Contrastive loss 的原因有两点：\nconstrative loss 假设所有样本都符合相同分布，而 triplet loss 没有这个假设。因此，可以适应各种空间形状，一定程度上能够抵御噪声的影响\ntriplet loss 优化的目标是正负样本之间的相对距离差，即正样本之间的距离小于正负样本之间的距离。而 constrative loss 优化的目标是绝对距离，即所有的正样本之间的距离也要尽可能小。这是没有必要的。\n假设负样本均匀分布，我们也均匀随机采样。那么，采样的负样本 pairwise distance 符合如下的分布：\n换句话说，在高维空间里，采样得到的负样本 pairwise distance 基本上都是大于 \\(\\sqrt{2}\\) 的。论文针对这个问题，提出的方法是 distance weighted sampling。以距离概率值的倒数 \\(q(d)^{-1}\\) 作为样本采样的权重，这样在修正样本距离分布的 bias 的同时控制了 variance。\nTriplet loss 采用的是一种 hard negtive mining 的方法，也就是正负样本的区分是 hard 的。负样本的梯度通过如下的公式计算：\n梯度的方向取决于 \\(h_{an}\\) ，即 anchor 样本与负样本的向量差。那么，如果差向量的绝对值特别小，并且这个负样本是异常值，那对模型的梯度会造成很大的影响。\n上图中展示了随着 pairwise distance 的增加，各种 loss 中正负样本是如何变化的。蓝色实线是正样本，绿色虚线是负样本。对于图b，\\(D_{an}\\) 越小，梯度值也越趋向于 0。根据 triplet loss 的计算公式，我们可以发现，这导致了模型趋向于这个点的梯度（正）会很大，但是远离这个点的梯度（负）很小。论文提出了 margin based loss 来解决这一问题，即图 d 中的 loss。在 \\(D_{an}\\) 很小的时候，仍然保证了负样本的梯度为一个常数，这有点类似 ReLU 的思想。\n4. 如何在一个全新的领域展开学习 # 原文链接：https://ichn.xyz/blog/how-to-start-learning-in-a-new-area\n学习计算机最重要的点在于关注能力的成长。而所有能力中最重要的，莫过于学习的能力，学习能力是培养其他能力的元能力。文章作者在接触了大量计算机细分领域后，总结出了几点特定的套路：\n明确动机\n要有明确且实际的需求。一是可以解决兴趣使然导致的选择困难，二是在学习的过程中，感受到切实的正反馈。\n背景调查\n一旦明确了学习的动机和目标，应该更系统性地、刻意地对这个领域展开背景调查。\n知道即将学习的知识可以解决什么样的问题，这种解决手段和其他方式相比的优劣，这个领域和其他领域、特别是自己已经熟悉的领域的关系是怎样的，这个领域的发展历史和发展脉络是怎样的，有哪些独特且重要的概念？\n背景调查获取的信息通常是宏观或者碎片化的，这并不是真正的学习。但这个过程可以提高你对这个领域的熟悉程度，在你的话语体系和思考方式中加入这个领域的成分，并提高你对这个领域的品味与认知。\n资源汇集与整理\n了解这个领域有哪些重要的资料，更重要的是会拥有判断这个方向的学习资料的优劣的能力。\n制定计划，然后无情地执行\n如果执行学习计划中会有枯燥的感觉，就需要回顾自己的动机、目标，并稍稍跳出来重新审视一下学习计划。\n如果确认学习路径的正确性，就应该专注，而不是继续在这个领域中漫无目的的探索，这样才能进入深水区。\n","date":"2022-07-15","permalink":"/posts/awesome-tech-post-0/","section":"Posts","summary":"每期 Awesome Tech Post 都会摘录推荐 5 篇优质技术博客，对这些文章的内容进行提炼总结。每期覆盖领域各不相同，可能从后端到可视化，从工程到算法等等，但每篇文章都会对领域内的某一问题进行深入分析或提出独到见解。欢迎大家私信推荐文章~","title":"Awesome Tech Post 第0期"},{"content":"","date":null,"permalink":"/tags/cloud/","section":"Tags","summary":"","title":"Cloud"},{"content":"","date":null,"permalink":"/tags/deep-learning/","section":"Tags","summary":"","title":"Deep Learning"},{"content":"","date":null,"permalink":"/tags/reinforcement-learning/","section":"Tags","summary":"","title":"Reinforcement Learning"},{"content":"","date":null,"permalink":"/tags/soft-skill/","section":"Tags","summary":"","title":"Soft Skill"},{"content":"","date":null,"permalink":"/tags/access-control/","section":"Tags","summary":"","title":"Access Control"},{"content":"TL;DR # 访问控制框架 Casbin 的原理以及其内部组件的结构 以一个 RBAC 的简单例子介绍 Casbin 的用法 Casbin是什么？ # 访问控制 # 访问控制，顾名思义，是指判断一条请求是否可以访问受保护的资源的技术。在上图的例子中，我们的后台中有两个资源，Resource1和Resource2。它们可以是服务器、账号、图片、视频等等等等。但是，它们的相同特性是不能被所有用户都访问。比如 Resource1 属于用户 Alice，那么只有 Alice 能够访问它，Bob 则不能。因此，我们就需要对访问请求进行过滤，判断其是否被允许到达目标资源。在上面的例子中，Alice 发起了两个访问请求，分别想要访问 Resource1 和 Resource2。访问控制层需要做的工作就是允许访问 Resource1 的请求通过，而阻拦想要访问 Resource2 的请求，因为 Resource2 属于 Bob，Alice 是无法访问的。\n在实际应用中，访问控制问题往往会随着业务而变得非常复杂。而 Casbin 1 就是一个强大的、高效的开源访问控制框架。Casbin 在 Github 上已获得超过 10k+ star，并且有着非常完整的生态。基于 Casbin 可以轻松的实现一系列访问控制模型，如 RBAC，ABAC等等。\n原理——PML # Casbin 的底层原理基于其创建者罗杨博士所发表的一篇论文《PML: An Interpreter-Based Access Control Policy Language for Web Services》2\n设计目标 # 这篇论文主要关注于如何解决现实中云服务厂商有关权限校验所遇到的两个问题：\n每个云服务厂商都有着自己的一套权限检验规则。这对于在多个云环境都进行部署的用户来说，造成了很大的迁移和维护成本。 同样的，维护自己的一套权限校验规则对于云服务厂商来说，也是一个挑战。如果云服务厂商缺乏在这方面的相关经验，就很有可能造成安全漏洞。 既然文章的目标本质上是通过通用性来解决问题，作者也考虑了如何实现这个目标，提出了两个 independent 的设计要求：\nAccess Control Model Independent：PML 既需要支持用户可以在多个云服务厂商中使用同一个模型，也需要支持用户在不改变校验代码的同时，可以切换不同的模型。 Implementation Language Independent: PML 的设计不应该依赖于某种编程语言的特性。 因此，文中提出了一种新的权限校验语言——PML (PERM Modeling Language)，希望通过一种支持多种权限校验模型的配置语言来弥补这个 gap。 设计实现 # 在介绍 PML 的设计之前，我们可以先大致了解一下访问控制问题中，所涉及到的一些概念。\n一般来说，访问控制会涉及到两个部分：\nModel 访问控制模型。常见的模型有 ACL（Access Control List 访问控制列表），RBAC（Role-Based Access Control 基于角色的访问控制），ABAC（Attribute-Based Access Control 基于属性的访问控制）。对于一个应用来说，Model 的选择是与应用的业务逻辑是密切相关的，因此也是相对静态的。一旦代码编译完成，这部分是不会随着应用的运行而产生变化的。 Policy 访问控制规则。Policy 是和 Model 相对应的，每种不同的 Model，都会有不同格式的 Policy。而与 Model 完全相反的是，Policy 是相对动态的。在编写代码的过程中，我们只能去定义 Policy 的格式，而 Policy 的具体内容都是应用运行过程中添加或修改的。例如，有一个新用户注册了我们的应用。那么，我们就需要动态的为其添加一条 Policy。 我们可以将这两部分理解为传统应用中的代码和数据。有了这两部分后，再加上用户特定的校验逻辑，那么就可以完成访问控制任务。\nPERM 模型 # 当然，对于现实环境中复杂的情况，简单地将问题建模为这两部分肯定是不够的，因此，论文提出了一个新的元模型 PERM（Policy-Effect-Request-Matcher）。\nPERM 模型主要包含了 6 个主要的概念：\nRequest：访问请求定义。用户真实的访问请求，通常会包含 sub（访问者），obj（被访问的资源）， act（访问时所进行的操作）或其他用户自定义的属性。 Policy：访问控制规则定义。定义了需要对访问请求的哪些属性进行校验。 Policy Rule：访问控制规则实例。 Matcher：如何为一条 Request 匹配到其对应的 Policy Rule。 Effect：当一条 Request 匹配到了一条或多条 Policy Rule，如何判断其是否应该被允许。 Stub Function：在实际应用中，Request 实例 和 Policy Rule 的匹配往往无法通过简单的 == 等于来解决，例如通配符等等。所以 Stub Function 允许用户自定义一些复杂的匹配方法。 这六个更加详细的建模了访问控制的问题。我们也可以对其简单的分一下类，Request，Policy，Matcher，Effect 和 Stub Function 都是静态的，属于 Model 的一部分。通过这个五项的组合，ACL等常见的模型以及一些用户自定义的规则，都可以很轻易的表示出来。在最后一部分中，会以 RBAC 为例，介绍如何通过 PML 实现这样一个模型。\n而 Policy Rule 就属于动态变化的内容。在实际实现中，往往也是像数据一样，存储在数据库当中的。\n结构 # 与论文中的实现相比，目前 Casbin 的实现更加强大，支持了更多功能。所以，这里以 Casbin 主库（Go 版本），介绍 Casbin 是如何进行工作的。\n在应用启动时，Casbin 会读取用户已经定义好的 Model，其中会包含 Request, Policy, Matcher 和 Effector 四个部分的定义。同时，Casbin 会利用 Adapter，从数据源处读取 Policy 实例（也就是上文提到的 Policy Rule）。后文就将 Policy 实例简称为 Policy。\n对于 Policy 的存储和读取，Casbin 将其解耦到了独立的 Adapter 模块。通过使用不同的 Adapter（File, MySQL等等），可以从不同的数据源中读取 Policy。对于 Policy 比较多的场景，将所有的 Policy 同时加载进内存，确实会导致一定的性能损失。所以，在加载时，部分 Adapter 也提供 LoadFilteredPolicy 的接口，通过只加载 Policy 的一部分子集，减少这部分带来的性能瓶颈。\n在一条请求到来时，该请求首先会按照 Model 中的定义进行拆分。接下来，Matcher 会根据 Model 中定义的规则，与 Policy 进行匹配。除了支持 == 强匹配外，Matcher 还支持通过 Function 和 Role Manager 进行模糊匹配。Function 像用户提供了自定义匹配规则的接口。通过向 Matcher 传入自定义函数，Matcher 可以对 Request 与 Policy 之间进行一些复杂的匹配。\n对于 RBAC 等访问控制模型，除了单纯的用户与权限之间存在关系之外，用户与角色（Role）之间还存在着继承关系。Casbin 中采用了 Role Manager 来为一条 Request 的用户以及其对应角色（包含继承角色）寻找与其相关的 Policy。同时，Role Manager 也支持添加自定义的 Function，来对用户与角色之间进行复杂的匹配。\n在实际应用中，一条 Request 可能会匹配到多条 Policy。得到所有的 Policy 后，需要进一步将多条 Policy 的结果进行聚合，得到最终是否允许 Request。Effector 根据 Model 中配置的规则，对所有 Matched Policy 的 effect 项进行进一步的 eval。\n在很多场景下，访问控制服务会有多个实例。Casbin 支持对 Policy 进行增量更新，那么，就需要 Dispatcher 维护多个 Casbin 实例的 Policy 之间的一致性。Dispatcher 主要提供两部分的功能，一部分是 Casbin 的 API，另一部分是 Dispatcher 自身的 API，用来实现成员管理等一致性问题，可以通过 Raft 等共识算法实现。\nUsage # 在了解了 Casbin 的原理和结构后，我们可以开始利用 Casbin 来进行一些实践。本章以 RBAC 模型为例，构建一个简单的访问控制示例。RBAC (Role-Based Access Control) 模型是基于角色的访问控制模型。在 RBAC 的模型中，用户和资源之间存在着角色（Role），用户可以属于一个或多个角色，角色拥有权限去访问资源。\n经过前两章的介绍，我们可以将访问控制分为三个部分：Static，Dynamic 和 User-specific Logic。在使用 Casbin 时，也可以这样进行划分。首先，我们先来定义一个静态的 RBAC Model（model.conf）。\n[request_definition] r = sub, obj, act [policy_definition] p = sub, obj, act [role_definition] g = _, _ [policy_effect] e = some(where (p.eft == allow)) [matchers] m = g(r.sub, p.sub) \u0026amp;\u0026amp; r.obj == p.obj \u0026amp;\u0026amp; r.act == p.act 在这个模型中，分别定义了五部分内容。\nrequest_definition，定义了 Request 的结构。这份示例中包含了访问者（sub），被访问者（obj）和操作（act）。 policy_definition，定义了 Policy 的结构。通常，由于 Policy 和 Request 之间要进行匹配，所以两者的结构有一定的相似性。 role_definition，定义了 Role Manager。g 定义了一套 RBAC 系统，换句话说是一组用户角色继承关系的集合。在实际使用中，更类似于一个函数，判断输入的参数是否在这个集合中存在继承关系。 policy_effect，定义了如何对多个匹配到的 Policy 做合并。目前，Casbin 支持几个固定语法的合并模式，在官网 3 上有着详细的介绍。这些模式的含义也很好理解，模式的语法与自然语言或者 SQL 非常相近。例如，some(where (p.eft == allow)) 表示的是当任意一个 Policy 的 effect 是 allow，那么合并的结果即为 allow。 matchers，定义了如何匹配 Policy 和 Request。 定义公式的语法与常见语言中的布尔表达式相似，通过 == 可以将 Policy 和 Request 中的各项进行对比。 Policy # p, alice, data1, read p, bob, data2, write p, data2_admin, data2, read p, data2_admin, data2, write g, alice, data2_admin 接下来，我们可以按照 Model 中定义的 Policy 结构来编写 Policy 实例。例如，第一项 p, alice, data1, read 与 p = sub, obj, act 相对应，alice，data1，read 与 sub，obj，act 相对应。另外一条比较特殊的实例是最后一项，g, alice, data2_admin，定义了用户 alice 继承了 data2_admin 这一角色。\n我们可以将上面的 Policy 保存在文件 policy.csv 中。但一般来说，Policy 储存在数据库等等一些更加 organized 的外部存储中。\nUser Logic # Casbin 几乎支持所有的常见的编程语言，用户使用的逻辑也基本相似，主要通过 Enforcer 类来进行操作。\ne, err := casbin.NewEnforcer(\u0026#34;model.conf\u0026#34;, \u0026#34;policy.csv\u0026#34;) result1, _ := e.Enforce(\u0026#34;alice\u0026#34;, \u0026#34;data1\u0026#34;, \u0026#34;read\u0026#34;) fmt.Println(result1) result2, _ := e.Enforce(\u0026#34;alice\u0026#34;, \u0026#34;data1\u0026#34;, \u0026#34;write\u0026#34;) fmt.Println(result2) result3, _ := e.Enforce(\u0026#34;alice\u0026#34;, \u0026#34;data2\u0026#34;, \u0026#34;read\u0026#34;) fmt.Println(result3) 通过 Enforce 方法，开发者输入 Request，就可以得到这条请求是否可以通过。在上述例子中有 3 个 test case，分别验证了合法请求匹配，非法请求匹配，集成角色请求匹配。第一个 test case 对应了 policy.csv 中的第一条 Policy，第二个 test case 则没有 test case。第三个 test case 通过 g, alice, data2_admin 将 alice 与 data2_admin 的 Policy 关联起来，然后通过第三条 Policy p, data2_admin, data2, read 验证其为合法请求。\nReference # - [1] https://github.com/casbin/casbin - [2] https://arxiv.org/pdf/1903.09756.pdf - [3] https://casbin.org/docs/en/syntax-for-models#policy-effect ","date":"2022-06-22","permalink":"/posts/casbin/","section":"Posts","summary":"TL;DR # 访问控制框架 Casbin 的原理以及其内部组件的结构 以一个 RBAC 的简单例子介绍 Casbin 的用法 Casbin是什么？ # 访问控制 # 访问控制，顾名思义，是指判断一条请求是否可以访问受保护的资源的技术。在上图的例子中，我们的后台中有两个资源，Resource1和Resource2。它们可以是服务器、账号、图片、视频等等等等。但是，它们的相同特性是不能被所有用户都访问。比如 Resource1 属于用户 Alice，那么只有 Alice 能够访问它，Bob 则不能。因此，我们就需要对访问请求进行过滤，判断其是否被允许到达目标资源。在上面的例子中，Alice 发起了两个访问请求，分别想要访问 Resource1 和 Resource2。访问控制层需要做的工作就是允许访问 Resource1 的请求通过，而阻拦想要访问 Resource2 的请求，因为 Resource2 属于 Bob，Alice 是无法访问的。","title":"Casbin is All You Need"},{"content":"2021年这一年的时间里，发生了太多的事，如果不写下来的话，在脑海中只会有一些模糊的印象，不清楚自己取得了哪些成绩，又有哪里不尽人意，同时也可以从一个旁观者的视角，来观察自己这一年有没有虚度光阴。所以，就从2021年开始，开始每年对过去的一年进行一个记录吧~\n技术 # K8s # 在年初的时候，阅读了《Kubernetes in Action》这本书，受益匪浅，再加上实习中的实践，对 K8s 的了解更加深入了。K8s 作为目前 Cloud 的实际代名词，在应用开发中已经是无法替代的存在了。作为云原生的基础设施，对于 K8s 的学习，除了掌握其基础的操作，例如 Deployment、Ingress 等等组件的使用，来满足上层的应用开发以外，对于 K8s 本身的学习，也是十分重要的。因为 K8s 本身就是一个庞大复杂的系统，而且经过无数业界大佬的打磨，系统上的设计无疑是很多问题的 best practice。深入理解 K8s 的设计，对于未来设计自己的应用架构肯定会带来很大的帮助。\n自动化部署 # 我在实习期间负责的主要的一部分内容就是自动化部署。随着现在微服务化的趋势以及系统内第三方依赖组件的数量的增加，部署其实是一个非常麻烦的问题。在公有云的场景下，部分第三方组件或许可以通过外部服务的形式解决。但在私有云的情况下，部署需要同时部署这些第三方组件，无疑给部署增加了很大的复杂度。不仅仅如此，对于任何现代化的软件来说，部署都是需要进行标准化的。通过部署文档，类似手工作坊进行的部署方式，是无法持久的，人因的错误是整个部署过程中很大的不稳定因素。这方面也有很多做的比较好的开源项目，比如说 TiDB 的 TiUP 等等。\n可视化 # 实话说，在正式进入硕士学习之前，我对于可视化的了解可能都比较浅薄（虽然明知道这是自己硕士的研究方向）。在听完大老板的课之后，对于这个领域有了全新的认知。可视化这个领域虽然很小，但并不是像大众认为的只有简单的图表而已。首先，图表本身的设计有着很多问题值得研究。设计可能是一个主观感性的行为，但我大老板的想法是通过design space等方式，将其转化为理性，有逻辑的行为。我对这一点非常认同，给了我一种茅塞顿开的感觉，让我对之前很多看似感性的事情有了新的理解。其次，可视分析也是可视化中非常重要的一部分。很多问题，通过可视化的形式，就可以很清楚和容易地被分析出来。这一类系统，感觉在 BI 领域应该有很大的发展空间。\nCasbin # 一直以来想加入一个开源社区，为 developer 们做出一点点自己的贡献。于是通过一个活动加入到了 Casbin社区。Casbin 是一个权限校验框架，通过 well-defined 的 model 结构，支持对各种类型的校验模式（比如RBAC，ABAC等等）。除此之外，我觉得 Casbin 能够获得 10k+ star 的另一个原因是他的生态支持也太丰富了。各种语言，各种前端后端框架，各种数据库，只要是有需求，都会进行支持。其中另一个最重要的应该是 Casdoor，一个第三方统一身份认证框架，与 KeyCloak 相对标。\n这两个项目都非常有意义，Casbin 主要是像一个解析器一样，面对不同的 model 和 policy定义，需要提供正确的校验结果，我个人还挺喜欢慢慢推理，寻找哪一步解析错误的过程。而 Casdoor 则像一个业务系统，如何实现更多的 feature，如何提升易用性，与更多的第三方系统进行集成。\nOther # 除此之前，这一年间也多多少少接触了很多其他技术。实习的时候无所事事，开始写起来了 Augular，体验了一次像 Java 一样的前端的开发模式；暑假的时候参加 GSoC，借机接触了一下 Rust，对这门语言产生了深深的敬畏，但可惜没能深入学习下去；开学以后接触了很多设计领域的coding，做了一些数字媒体类似的课程作业\u0026hellip;\u0026hellip;\n生活 # 上半年实习的过程中，感觉自己逐渐放开了。在实习期间遇到了很多伙伴，大家一起快乐摸鱼，真的是度过了一段很快乐的时光。最重要的是毕业啦！最高学历终于从高中变成了本科。虽然毕业旅行因为疫情原因没能成行，但暑假在家快乐摸鱼，疯狂锻炼猎龙技术，还差一点把塞尔达的呀哈哈全收集，也算快乐的度过了最后一个暑假了。\n如果说上半年是我人生最快乐的时光，那么下半年开学后，就是我目前为止最痛苦的日子了。但无论是前后哪个阶段，我其实都成长了非常多，也算是值得庆幸的事，能够在接受到社会正式的毒打之前，提前成长。\n一方面的压力来自于课程，毕竟我是属于跨专业保研到了设创，难免在研究生阶段要接触到设计相关的课程。在组队上，天真地以为老师是技术背景出身的，项目可能也会是，结果就是在课程项目上被设计背景的同学吊打。在 DDL 前疯狂爆肝，但是也换不来特别高的成绩。研究生课程中没能抱上设计大佬们的大腿，没体验一次被带飞的感觉，算是比较可惜的了。\n另一方面最大的压力来自于实验室。大老板的痛骂确实是有效的。做事不够深入的问题，在我之前的面试以及实习过程中，都多次被前辈们提到过，但都采用相对和蔼的语气和态度。我也是属实有点抖m了，这种不痛不痒的批评根本不往心里去，也没有发自内心地去纠正自己的问题，非要等到现在的大老板爆骂自己，自己才意识到问题。。。\n年末到22年初的状态确实不太好，似乎每年的这段时间都很emo。在年底DDL结束之后，自己的节奏一时间也没调整过来。闲下来以后，脑子昏昏沉沉，没了目标，搞不清楚自己真正想要什么，不讨人喜欢。最后也算造成了不可弥补的错误吧，非常可惜。不过，人各有命吧，也学习到了很多，前后相比，算是成熟了很多了。\n拖拖拉拉，写完这篇总结的时候，2022年都已经过去四分之一了。总的来说，2022年可以用“翻天覆地”这个词来概括，在这个痛苦的过程中，也见到了一个不一样的自己。虽然2022年的开端不算顺利，很失败，但这些问题就留到明年的总结中来写吧。最后，希望在2023年总结今年的时候，可以用一个更加开心的关键词吧。\n","date":"2021-12-31","permalink":"/posts/2021/","section":"Posts","summary":"2021年这一年的时间里，发生了太多的事，如果不写下来的话，在脑海中只会有一些模糊的印象，不清楚自己取得了哪些成绩，又有哪里不尽人意，同时也可以从一个旁观者的视角，来观察自己这一年有没有虚度光阴。所以，就从2021年开始，开始每年对过去的一年进行一个记录吧~","title":"2021 年终总结 —— 翻天覆地"},{"content":"","date":null,"permalink":"/categories/paper-reading/","section":"Categories","summary":"","title":"Paper Reading"},{"content":"","date":null,"permalink":"/tags/time-series/","section":"Tags","summary":"","title":"Time Series"},{"content":"Shapelet 1 是一种常见的对时序数据进行建模的方法之一。从 2009 年在 KDD 上发布第一篇论文以来，其经过了各式各样的变形和改进。Time2graph 2 观察到 shapelet 具有 time-aware 的性质，因此提出了一种基于 shapelet 的时序数据的建模方法，将数据建模成图结构。\nWhat # Shapelet # A shapelet is a segment that is representative of a certain class. Shapelet 是一段具有代表性的子序列，它可以将一条时序数据的所有分段分成两类，一类与其相似，另一类与其不同。\nTime-aware shapelet # Time2graph 中所创新性提出的包含时间信息的 shapelet，利用图对其在时间维度上的变化进行捕捉。\nWhy # 传统的 shaplet 以及其改进形式都忽略了 shapelet 在不同的时间区间上的表现能力。这造成两点不足：\n相同 shapelet 在不同时间区间上的含义不同。 shapelet 之间的变化关系也表示了时序数据的一些信息。 How # Time-Aware Shapelet Extraction # Time-aware shapelet 的抽取可以分为以下三个步骤\n分段\n这里需要超参数如：每个分段的长度\n选取 Candidate\n这一部分在论文中没有叙述，但阅读源代码后，我们可以发现，针对所有分段后得到的片段，有两种方法从中得到 candidate shapelet。第一种是通过聚类，例如 K-Means，和 DTW 作为距离进行聚类，然后选取类中心的片段作为 candidate。第二种则是贪婪的方法，即遍历计算一个片段与其他所有片段之间的距离，然后选取平均最小的片段作为 candidate。\n计算评分\nTime2graph 设计了新的评分方法来衡量 shapelet 在时间维度上的重要度，这部分比较复杂，后文会进行详细的介绍。对所有的 shapelet candidate 打分后，我们就可以选取 top-k 作为真正的 shapelet。\nDistance # Time2graph 设计了两个需要训练的参数，local factor w 和 global factor u 来分别衡量 shapelet 内部每个元素的重要度以及 shapelet 在不同时间段上的重要度。\n训练的过程需要进行梯度下降，损失函数如下图所示 v: shapelet T: time series S∗(v, T): the set of distances with respect to a specific group T∗ g: differentiable function measures the distances between distributions of two finite sets 最后两项为 penalties。当梯度下降使得损失函数的值越来越小时，w 和 u 就越来越可以使得 shapelet 到 positive 和 negative 的距离之差越来越大，这样 shapelet 就越能代表一个类。\n接下来，详细介绍损失函数中所用到的两个距离函数。\nshapelet 与 segment 之间的距离 w: local factor 需要学习的参数 u: global factor 需要学习的参数 s: segment a: alignment，利用 DTW 可将长度为 i 的 v 和长度为 j 的 s 对齐成长度为 p 的 a1 和 a2 对于 DTW alignment，下面这张图形象地解释了整个对齐的过程\n对于序列 s1 和 s2，s1 中的一个点可能对应 s2 中的多个点，同时 s2 中的一个点也可能对应 s1 中的多个点。所以，两者对齐后，会得到一个新的长度的序列。而 shapelet 与 segment 的距离，其实就是计算两者对齐后的欧氏距离。\nshapelet 与 time series 之间的距离 这个就比较简单了，d 即为刚刚介绍的 shapelet 与 segment 之间的距离。shapelet 与 time series 的距离即为与 time series 的所有分段中的权重最小值。\nShapelet Evolution Graph # 接下来，就是要构建图来表示 shapelet 之间的转移关系。\n算法先将 shapelet 与 segment 关联起来，edge 的权重由以下这个公式计算\n公式计算的是 shapelet 与 segment 相关的概率，如果 shapelet 与 segment 之间的距离越近，那么相关的概率越高。\n接下来，算法将相邻的 segment 的 shapelet 连接起来，而 edge 的权重则是前后两个 segment 与各自 shapelet 之间关联概率的乘积，代表了从前一个 shapelet 迁移到后一个 shapelet 的概率。\n最后，算法将一个 shapelet 所有迁移出去的 edge 的权重进行归一化，使其和为 1。\n算法的伪代码如下所示\nRepresentation Learning # 最后，time2graph 采用了 DeepWalk 3的算法，将 shapelet 转换为 embedding 的形式。\n接下来，就可以利用 shapelet 的 embedding 来表示 segment 和 time series。\nsegment 的向量化表示如下面这个公式，就是其相关 shapelet 的 embedding 的概率权重和\n而 time series 的向量化表示就是将其所有 segment 的 embedding 的拼接。\n算法的伪代码如下所示\nApply Time2graph # 至此，我们就完整的了解了 time2graph 的训练过程。那么 time2graph 的应用过程也与之类似。\n首先，算法的输入仍然是一个 time series。算法会先对其进行分段，然后为每个 segment 分配相关的 shapelet。接下来，就可以得到 segment 和 time series 的 embedding，进行后续聚类或者其他操作。\nReference # - [1] Ye, Lexiang, and Eamonn Keogh. \"Time series shapelets: a new primitive for data mining.\" Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 2009. - [2] Cheng, Ziqiang, et al. \"Time2graph: Revisiting time series modeling with dynamic shapelets.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020. - [3] Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. \"Deepwalk: Online learning of social representations.\" Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 2014.","date":"2021-09-06","permalink":"/posts/time2graph/","section":"Posts","summary":"Shapelet 1 是一种常见的对时序数据进行建模的方法之一。从 2009 年在 KDD 上发布第一篇论文以来，其经过了各式各样的变形和改进。Time2graph 2 观察到 shapelet 具有 time-aware 的性质，因此提出了一种基于 shapelet 的时序数据的建模方法，将数据建模成图结构。","title":"Time2graph: Revisiting time series modeling with dynamic shapelets"},{"content":"","date":null,"permalink":"/categories/code-read/","section":"Categories","summary":"","title":"Code Read"},{"content":"","date":null,"permalink":"/tags/tidb/","section":"Tags","summary":"","title":"TiDB"},{"content":"TiDB Lightning 是一个将数据导入到 TiDB 中的工具，使用 Go 编写。支持 Local, Importer, TiDB 三种导入模式。在 TiDB 的官网中，对其原理有着详细的介绍。\n本文从代码的角度，带领大家走过一个数据导入的过程，所以只关注一些逻辑上重要的步骤，而一些其他的细节可能不会涉及到。\n首先，进入 main 函数，程序调用了两个主要的启动函数：GoServer 和 RunServer。\nGoServer # https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96\nGoServer 启动了一个 Api Server，有着以下这些 endpoint\n/web /metrics /debug /tasks /progress /pause /resume /loglevel Api Server 为整个 lightning 提供了控制，监控和查看进度等功能。是用户与 lightning 交互的入口。而当用户通过 POST /tasks 提交了一个数据导入任务时，Api Server 就会向队列中添加一个任务，而同样在监听着队列的 RunServer 就会开始执行。\nRunServer # https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194\nRunServer 是真正执行导入的地方。一个 for 循环一直从队列中获取任务配置。在获取到一个任务的配置后，开始正式执行导入任务。\nRegisterMySQL # 这个函数的命名有点迷惑，但所幸有注释。RegisterMySQL 同时包含了向 gomysql driver 配置 TLS 和解除配置的两个作用。在运行的一开始，注册 TiDB 的 TLS 配置，这样直接可以使用 sql.open() 连接 TiDB。在执行导入任务结束后，通过将 CAPath 置为空，删除 TLS 配置。\nGlue # Glue 将数据和导入粘合在一起。这里在创建 Glue 时，就将 TiDB 设为了导入模式。\nMyDumper # 确定好目标数据库也就是 backend 后，我们就可以看看数据导入的 frontend 了。MyDumper 是一款由 PingCAP 基于社区版，为 TiDB 定制化开发的数据导出工具。但是目前似乎已经不推荐使用了，推荐改用 dumpling 了。\n在这里创建 MyDumper 时，就会在 setup 中从 SQL 文件中。 这里会对表按从小到大进行排序，让小表之后先进行导入，这样可以避免大表导入时阻塞小表释放 index worker。\ntype mdLoaderSetup struct { loader *MDLoader dbSchemas []FileInfo tableSchemas []FileInfo viewSchemas []FileInfo tableDatas []FileInfo dbIndexMap map[string]int tableIndexMap map[filter.Table]int } Check # 初始化并配置好 MyDumper 后，在正式开跑前，程序还需要进行检查。这里检查了两项：1. CheckSystemRequirement 系统要求是否满足，lightning 对于内存要求似乎还挺高。2. CheckSchemaConflict 检查目标数据库的 Schema 是不是有冲突。\nRestoreController # lightning 的导入过程由 RestoreController 进行控制。Run 中的 opts 定义了导入过程所有的操作。\nCheckRequirements # 通过多态实现，三种 backend 执行不同的检查。\nSetGlobalVariables # 在 Server 模式下，目标数据库的 Collation 设置都可能不同，所以每次执行任务都需要设置一下。\nRestoreSchema # 从这里，lightning 真正开始导入数据。首先，导入 Schema。Schema 包含 database, table, view 三个部分，分别对应着之前从 MyDumper 中导出的三个部分。Lightning 使用 Glue 中封装的目标数据库连接，执行 SQL，导入 Schema 信息。\n这里 lightning 使用了一种非常 Go 风格的实现方法。先创建出多个 goroutine，每个 goroutine 都监听着同一个 channel。之后再不断向 channel 中加入要执行的 SQL 任务，从而实现一个类似于线程池的功能。\n在导入 Schema 成功后，RestoreSchema 还会负责初始化 Checkpoint，并创建一个 goroutine，用来监听 Checkpoint 的变化，将多个 Checkpoint 合并到一起。\n最后，RestoreSchema 还会根据 Schema 中包含的元信息，对之后导入数据时划分的 chunk 数量进行一个估计。\nRestoreTables # RestoreTables 负责从源数据库中导出数据到目标数据库中。\n首先，在 populateChunks 函数中，使用 MyDumper 将数据划分成多个 chunk，接着将 chunk 加入到其对应的数据 engine 中。然后添加一个索引 engine 的 Checkpoint。\n然后，通过 InsertEngineCheckpoint 创建每张表的 Checkpoint。\n然后，在 restoreEngines 中，先将源数据库的数据从导出的文件，并发地转化成 KV 格式的 engine，然后再导入到目标数据库中。关于 engine 的状态机模型，可以参考我的另一篇文章中的总结。这个函数中包含了 lightning 最核心的逻辑，所以比较复杂，大致可以分为以下几个过程：\n在 chunkRestore 的 restore 中，在 encodeLoop 中，利用 MyDumper 的 parser，将数据从不同的导出格式统一转换成 KV 格式，插入到 engine 中。此时，所有的 Checkpoint，包括 data 和 index，都已经写入完成。 接着，根据 engine 的状态机，关闭 engine 才能开始导入，所以我们现在需要将 engine 的状态置为 close。 关闭后，开始调用 backend 导入 engine。在所有的 data engine 导入完成后，开始导入 index engine。 这里，又使用了一种非常 Go 风格的方式实现线程池的功能。利用 channel 的缓冲特性，向其中添加 worker，使用时取走，使用完再放回到 channel 中。\n最后，对于某些需要执行 post process 的任务，执行一下相应的任务。\nFullCompact # 调用 TiKV 的接口，对所有新导入的数据进行压缩。但是由于之前导入数据的时候，会根据是否存在压缩任务，去尝试执行 level-1 的压缩，所以进行全量压缩的时候，需要等待之前启动的 level-1 压缩任务结束。\nSwitchToNormalMode # 将 TiKV 切换成正常模式\nCleanCheckpoints # 所有的导入任务都结束了，此时 checkpoint 已经无用了。所以，根据用户配置是否删除，处理 checkpoint。\n","date":"2021-03-19","permalink":"/posts/tidb-lightning/","section":"Posts","summary":"TiDB Lightning 是一个将数据导入到 TiDB 中的工具，使用 Go 编写。支持 Local, Importer, TiDB 三种导入模式。在 TiDB 的官网中，对其原理有着详细的介绍。","title":"TiDB Lightning 源码阅读"},{"content":"目标 # 介绍 K8s，Docker 概念以及原理 从 0 开始部署一个简单完整的服务 Docker是什么？ # Docker是由Google推出的Go语言进行开发实现，基于Linux内核的 namespace，对进程进行封装隔离，属于操作系统层面的容器化技术。\n三大核心概念 # 镜像（Image）\n容器（Container）\n仓库（Repository）\n从代码的角度来看，镜像就像一个类；容器是对象实例，运行时在系统中会有许多容器；仓库主要用于存储和维护这些镜像。\n为什么使用 Docker？ # 配置环境 开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性 应用隔离 机器上可能同时运行多个服务。如果服务之间没有隔离，一个服务出现异常，往往可能会导致其他服务也挂掉。同时，不同服务所依赖的环境也可能发生冲突。 原理 # 首先，要了解一下进程的命名空间。Linux 系统中的所有进程按照惯例是通过PID标识的，这意味着内核必须管理一个全局的PID列表。而且，所有调用者通过uname系统调用返回的系统相关信息（包括系统名称和有关内核的一些信息）都是相同的。\nLinux 的命名空间从内核层面上进行了虚拟化，对所有的全局资源进行一个抽象。本质上，建立了系统的不同视图。每一项全局资源都必须包装到命名空间的数据结构中，只有资源和包含资源的命名空间构成的二元组仍然是全局唯一的。不仅仅是 PID，Linux 通过同样的方法对其他资源也做了虚拟化处理。命名空间共有以下6种：\n借助 Linux 的命名空间，Docker 对进程进行隔离，可以从进程树的角度理解。\n每次在执行 docker start 或 docker run 的时候，其实是由 docker 的 daemon 进程 docker containerd，调用 Linux 系统调用 clone() 去创建新的进程。而创建进程的过程中就为新创建的进程分配了新的 Linux 命名空间。可以简单阅读一下 docker 的开源代码\n// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L17 // 创建容器的函数，其中又调用了设置 func (daemon *Daemon) ContainerStart(name string, hostConfig *containertypes.HostConfig, checkpoint string, checkpointDir string) error // https://github.com/moby/moby/blob/470ae8422fc6f1845288eb7572253b08f1e6edf8/daemon/oci_linux.go#L212 // 设置 Namespace func setNamespace(s *specs.Spec, ns specs.LinuxNamespace) { for i, n := range s.Linux.Namespaces { if n.Type == ns.Type { s.Linux.Namespaces[i] = ns return } } s.Linux.Namespaces = append(s.Linux.Namespaces, ns) } // https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L198 // 创建新的进程 pid, err := daemon.containerd.Start(context.Background(), container.ID, checkpointDir, container.StreamConfig.Stdin() != nil | | container.Config.Tty, container.InitializeStdio) 如何安装？ # https://docs.docker.com/get-docker/\nKubernetes是什么？ # Kubernetes 是 Google 于 2014 年基于其内部 Brog 系统开源的一个容器编排管理系统，可使用声明式的配置（以 yaml 文件的形式）自动地执行容器化应用程序的管理，包括部署、伸缩、负载均衡、回滚等。\n为什么叫 K8s？因为 Kubernetes，中间是8个字母。\nkubernetes 提供的功能：\n自动发布与伸缩：可以通过声明式的配置文件定义想要部署的容器 滚动升级与灰度发布：采用逐步替换的策略实现滚动升级 服务发现与负载均衡：Kubernetes 通过 DNS 名称或 IP 地址暴露容器的访问方式，并且可在同一容器组内实现负载分发与均衡 存储编排：Kubernetes 可以自动挂载指定的存储系统，如 local storage/nfs / 云存储等 故障恢复：Kubernetes 自动重启已经停机的容器，替换不满足健康检查的容器 密钥与配置管理：Kubernetes 可以存储与管理敏感信息，如 Docker Registry 的登录凭证，密码，ssh 密钥等 为什么使用 K8s？ # 大型单体应用被逐渐拆分成小的、可独立运行的组件。随着部署组件的增多和数据中心的增长，配置、管理和运维变得很困难。(微服务）\nK8s 的定义就是容器编排和管理引擎，解决了这些问题。\n如何安装？ # 由难到易(๑•̀ㅂ•́)و✧\nKubeadm: https://kubernetes.io/docs/reference/setup-tools/kubeadm/ MiniKube: Local kubernetes https://minikube.sigs.k8s.io/docs/start/ Kind: Kubernetes in Docker https://github.com/kubernetes-sigs/kind Docker-desktop（仅限 Mac）: 一键开启 其他版本的类 K8s 系统：\nK3s: https://github.com/k3s-io/k3s K0s: https://github.com/k0sproject/k0s Kubernetes 架构 # master # Master 负责管理服务来对整个系统进行管理与控制，包括\napiserver：作为整个系统的对外接口，提供一套 Restful API 供客户端调用，任何的资源请求 / 调用操作都是通过 kube-apiserver 提供的接口进行, 如 kubectl、kubernetes dashboard 等管理工具就是通过 apiserver 来实现对集群的管理 kube-scheduler：资源调度器，负责将容器组分配到哪些节点上 kube-controller-manager：管理控制器，集群中处理常规任务的后台线程，包括节点控制器（负责监听节点停机的事件并作出对应响应）、endpoint-controller（刷新服务与容器组的关联信息）、replication-controller（维护容器组的副本数为指定的数值）、Service Account \u0026amp; Token 控制器（负责为新的命名空间创建默认的 Service Account 以及 API Access Token） etcd：数据存储，存储集群所有的配置信息 coredns：实现集群内部通过服务名称进行容器组访问的功能 worker # Worker 负载执行 Master 分配的任务，包括\nkubelet：是工作节点上执行操作的代理程序，负责容器的生命周期管理，定期执行容器健康检查，并上报容器的运行状态 kube-proxy：是一个具有负载均衡能力的简单的网络访问代理，负责将访问某个服务的请求分配到工作节点的具体某个容器上（kube-proxy 也运行于 master node 上） Docker Daemon：Kubernetes 其实不局限于 Docker（即将取消），它支持任何实现了 Kubernetes 容器引擎接口的容器引擎，如 containerd、rktlet 网络通信 # 网络通信组件只需要符合 CNI （Container Network Interface）接口规范，主要作用在于给各个容器分配集群内 IP，使得其内网 IP 能够集群内唯一，并且可以互相访问，目前常用的有 Flannel，Calico等网络组件。\n简单介绍下比较常用的 Flannel 的原理。Flannel 运行在第3层网络层，基于 IPv4，创建一个大型内部网络，跨越集群中每个节点。每个节点组成一个子网，每个容器在内网中有唯一的IP。\n首先，Flannel 会为每台节点分配一个子网段。Flanneld 在 Docker 容器启动时修改其启动参数，将其 IP 限制在当前的子网段内，具体 IP 的分配仍是由 docker 进行。Flannel通过Etcd服务维护了一张节点间的路由表，详细记录了各节点子网网段，保证不同节点的子网网段不会重复。\n数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，flanneld服务监听在网卡的另外一端。\n源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。\n快速上手 K8s 概念 # 一些推荐的 K8s 概念介绍：\n微软的 50天 K8s 教程中（https://azure.microsoft.com/en-us/resources/kubernetes-learning-path/）通过动物园的形式介绍了一些 K8s 概念 http://aka.ms/k8s/LearnwithPhippy 综述PPT：https://www2.slideshare.net/BobKillen/kubernetes-a-comprehensive-overview-updated?from_action=save K8s 中的概念极多，比较零碎，这里通过一个简单的小例子，尽可能覆盖多的 K8s 概念。\n概览 # 例子使用一个开源的 fortune-teller 镜像（quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1） ，每次请求容器内的服务，服务会返回一句名言。希望在 MacOS 的环境下，展示一个应用在 K8s 中运行的全流程。\n准备环境 为了不影响大家本地的环境，这里使用 Kind 创建出一个独立的 K8s 集群，方便统一版本并且可以在完成快速清理掉。(Docker 双重隔离）\n安装 Kind 以及 gRpc 测试工具 brew install kind brew install grpcurl 拉取镜像 docker pull kindest/node:v1.16.15 docker pull quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1 创建 K8s 集群，因为 Kind 是在 Docker 容器里面创建的 K8s，所以宿主机访问，需要把端口暴露出来。Kind 会默认把 K8s apiserver 的端口暴露出来，用来给 kubectl 命令使用。但为了之后的测试，我们提前把几个端口在创建的时候就暴露出来。 Kind 同样支持通过yaml 的形式创建集群\nkind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane extraPortMappings: - containerPort: 32080 hostPort: 32080 - containerPort: 32443 hostPort: 32443 kind create cluster --name=fortune-teller --image=kindest/node:v1.16.15 --config kind-config.yaml 运行 Docker 版本 # 启动容器 docker run -p 50051:50051 quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1 -p 将容器内的 50051 端口映射到宿主机的 50051 端口\n测试应用是否正常运行，第一次运行时可能需要给 grpcurl 开启权限 grpcurl -plaintext 127.0.0.1:50051 build.stack.fortune.FortuneTeller/Predict 应用在收到请求以后，会返回一句名言\n将应用从 Docker 迁移到 K8s 中 # 与 Docker 中容器概念相对应的，K8s 中也有着容器的概念。对于虚拟化的容器来说，最佳实践是一个容器一个应用，但当一个服务需要多个应用组合完成时，简单的将多个应用部署到一个容器内，就破坏了应用之间的隔离性，所以 K8s 对于容器进行了一层封装，形成了 Pod 的概念。\nPod # Pod 是 Kubernetes 创建或部署的最小基本单元。一个 Pod 封装一个或多个应用容器、存储资源、一个独立的网络 IP 以及管理控制容器运行方式的策略选项。Pod 中的每个容器共享网络命名空间（包括 IP 与端口），Pod 内的容器可以使用 localhost 相互通信。Pod 可以指定一组共享存储卷 Volumes，Pod 中所有容器都可以访问共享的 Volumes。\n通过 Pod，用户就可以非常方便地控制容器之间的隔离性。\n有了 Pod 作为基础以后，K8s 就要实现它最重要的功能，对容器的编排管理。当服务需要扩容时，K8s 需要能够快速复制 Pod，当 Pod 挂掉了，K8s 需要能够自动重启。所以 K8s 由此衍生出了 ReplicaSet 的概念。\nReplicaSet # ReplicaSet 确保在任何时候都有按配置的 Pod 副本数在运行，通过标签选择器的方式对 Pod 进行筛选和管理。在旧的版本中还有一个 ReplicaController 的概念，RC 与 RS 两者功能完全相同，区别仅仅在于 RS 对于 Pod 的标签选择器更加强大。\n开头提到了 K8s 使用声明式的配置自动去管理容器，而 ReplicaSet 的内容却太过具体，涉及到了 Pod 的具体维护细节。所以 K8s 在 ReplicaSet 之上又衍生出声明式配置容器的概念，Deployment。\nDeployment # Deployment 为 Pod 与 ReplicaSet 提供了声明式的定义，描述你想要的目标状态是什么，Deployment controller 就会帮你将 Pod 与 ReplicaSet 的实际状态改变到你想要的目标状态。\n以 fortune-teller 为例子，可以编写一份下面这样的 Deployment 配置文件\napiVersion: apps/v1 # k8s api版本 kind: Deployment # 资源类型 metadata: name: fortune-teller-app # deployment 名字 namespace: default spec: replicas: 1 # Pod 副本数量 selector: matchLabels: k8s-app: fortune-teller-app # 管理标签中包含 k8s-app: fortune-teller-app 的 Pod template: # Pod 模板 metadata: labels: k8s-app: fortune-teller-app # Pod 标签 spec: # Pod 配置 containers: - image: quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1 imagePullPolicy: IfNotPresent name: fortune-teller-app ports: - containerPort: 50051 name: grpc protocol: TCP 将上面的内容保存到一份 yaml 文件中，执行以下命令，让 K8s 执行 yaml\nkubectl apply -f deployment.yaml 通过以下命令，我们就可以看到刚刚创建的 deployment\nkubectl get deployement 此时，K8s 已经自动根据 deployment 中配置的 Pod 模板和配置，创建了 Pod。通过以下命令，我们就可以看到 K8s 自动创建的 Pod\nkubectl get pods 因为 K8s 采用声明式的配置去管理 Pod，所以我们可以动态地去修改 deployment 的配置，K8s 会自动根据新的配置去管理 Pod。\nkubectl edit deployment fortune-teller-app 我们把配置文件中的副本数量修改为 2\n保存退出后，我们再次执行 kubectl get pods ，我们就可以看到 K8s 根据新的配置，创建了一个新的 Pod\n现在我们就有了两个 fortune-teller 的服务。在真实环境中，Pod 的调度由 K8s 进行管理，某个时刻服务可能在 Node1 上，而另一时刻服务可能就被调度到了 Node2 上。所以，访问具体 Pod 是一种不稳定的服务访问方法，而且目前大多数的后端服务都是无状态的服务，直接访问 Pod 也导致不能进行负载均衡。所以，K8s 在此基础上衍生出 Service 的概念。\nService # Service 可以看做一组提供相同服务的 Pod 的对外访问接口。Kubernetes 提供三种类型的 Service：\nNodePort： 集群外部可以通过 Node IP 与 Node Port 来访问具体某个 Pod，每台机器上都会暴露同样的端口 ClusterIP：指通过集群的内部 IP 暴露服务，服务只能够在集群内部可以访问，这也是默认的 ServiceType ExternalName：不指向 Pod，指向外部服务 Service 和 Deployment 是一对比较容易混淆的概念，两者都是对一组 Pod 进行管理，但它们两者之间的关系可以用下面这张图来概括 Service 是面向服务调用者，也就是外部访问 K8s。而 Deployment 是面向 K8s 底层引擎的，面向内部管理者。\nService 的配置文件格式与 Deployment 很类似\napiVersion: v1 kind: Service metadata: name: fortune-teller-service namespace: default spec: ports: - name: grpc port: 50051 protocol: TCP targetPort: 50051 selector: k8s-app: fortune-teller-app type: ClusterIP 同样的，我们通过 kubectl apply -f service.yaml 命令，可以创建 Service。通过 kubectl get service 可以查看到刚刚创建的 Service。\n接下来，我们登陆到一个 Pod 里去测试一下是否可以访问服务。 Netshoot 镜像中包含了一些网络测试的工具，我们可以直接进入一个 netshoot 容器内测试。采用 deployment 的方式创建 Pod\napiVersion: apps/v1 kind: Deployment metadata: name: netshoot namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: netshoot template: metadata: labels: k8s-app: netshoot spec: containers: - args: - 1000d command: - /bin/sleep image: nicolaka/netshoot name: netshoot 与 Docker 的命令类似，使用命令 kubectl cp grpcurl_1.6.0_linux_x86_64.tar.gz \u0026lt;pod name\u0026gt;:/ 复制工具到容器内。\n复制成功后，使用命令 kubectl exec -it bash 可以进入到容器内。\n解压\ncd / tar -zxf grpcurl_1.6.0_linux_x86_64.tar.gz 然后我们可以测试是否可以从 K8s 集群内访问 Service。对于 K8s 集群内部的服务，K8s 有自己的 DNS 组件，所以可以直接通过服务名访问。\n./grpcurl -plaintext fortune-teller-service:50051 build.stack.fortune.FortuneTeller/Predict 验证服务可以从集群内访问之后，我们就需要解决如何从集群外访问服务的问题，毕竟大多数服务是面向 K8s 集群外的用户的。其实目前我们已经了解了一种解决方案，就是使用 Nodeport 类型的 Service。但采用这种方法有几个缺点：\n每个端口只能是一种服务 端口范围只能是 30000-32767 如果节点 的 IP 地址发生变化，调用方需要能够察觉。 所以，K8s 为服务的外部访问路由提供了新的类型 Ingress。 Ingress # Ingress 其实是一种类似于路由表一样的配置，实际的路由工作需要 Ingress Controller 执行。K8s 本身并没有提供 Ingress Controller，目前常用的是通过 Nginx 实现的版本 https://github.com/kubernetes/ingress-nginx 。可以使用上面压缩包中的 ingress-controller.yaml 安装\nIngree nginx controller 通过宿主机暴露给外部访问的端口是随机的，所以我们修改 yaml，改成我们在一开始创建集群时映射的端口。\n通过 kubectl get svc -n ingress-nginx 我们就可以看到，暴露的是两个随机分配的端口\n我们手动将其改成 32080 和 32443。\nIngress nginx controller 同样是通过标签选择器的方式管理 Ingress。\n下面是一个简单的 Ingress，将发往 Host fortune.bytedance.com 的请求路由到 Service fortune-teller-service。\napiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/backend-protocol: GRPC name: fortune-ingress namespace: default spec: rules: - host: fortune.bytedance.com http: paths: - backend: serviceName: fortune-teller-service servicePort: grpc 安装 Ingress\nkubectl apply -f ingress.yaml kubectl get ingress Ingress nginx controller 对于 gRpc 默认只支持 SSL 的形式，而Fedlearner 中的 controller 做了一些定制化操作，使得通过 80 端口，只使用 HTTP2 也可以转发 gRpc。详情可以参考 https://github.com/bytedance/ingress-nginx/pull/2/files\n使用下面这个命令，我们可以测试一下从外部访问服务\ngrpcurl -insecure -servername \u0026#39;fortune.bytedance.com\u0026#39; 0.0.0.0:32443 build.stack.fortune.FortuneTeller/Predict 刚才也提到了，gRpc 通常是使用 SSL 进行加密的，SSL 的关键在于公钥，私钥以及证书的验证。通过文件系统的方式确实可以处理证书的问题，但 K8s 抽象出 Secret 这种资源，大大提高了对这类文件的管理和复用能力。\nSecret # Secret 解决了密码、token、密钥等敏感数据的存储问题，主要分为三种类型：\nService Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的 / run/secrets/kubernetes.io/serviceaccount 目录中 Opaque ：Base64 编码格式的 Secret，用来存储密码、密钥等 kubernetes.io/dockerconfigjson ：用来存储 docker registry 的认证信息 接下来，我们就来创建一个 Opaque 类型的 Secret，使得 ingress nginx controller 支持服务端的 SSL。 由于篇幅有限，这里简单介绍下证书相关的概念：\nCA：证书授权中心(certificate authority)，用来签发私钥，并验证公钥，私钥的合法性 私钥，公钥：私钥用于加密，公钥用于解密 Secret 支持不编写 yaml，直接从文件中创建 Secret\nkubectl create secret generic fortune-teller-ssl-verify \\ --from-file=ca.crt=CA.pem \\ --from-file=tls.crt=server-public.pem \\ --from-file=tls.key=server-private.key 修改 Ingress，使其使用新创建的 Secret 提供服务侧的 SSL。\napiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/backend-protocol: GRPC name: fortune-ingress namespace: default spec: rules: - host: fortune.test.com http: paths: - backend: serviceName: fortune-teller-service servicePort: grpc tls: - hosts: - fortune.test.com secretName: fortune-teller-ssl-verify 因为我们使用的是自签名的证书，不被公共的 CA 所信任，所以在发送请求是需要手动指定自己所信任的 CA。\ngrpcurl -cacert CA.pem \\ -servername \u0026#39;fortune.test.com\u0026#39; \\ 127.0.0.1:32443 \\ build.stack.fortune.FortuneTeller/Predict 参考 # https://draveness.me/docker/ https://www.yuque.com/kshare/2020/fe3b6f86-3b9b-48da-9770-897d838cbf41?language=zh-cn#Namespace https://network.51cto.com/art/201907/598970.htm https://blog.csdn.net/gatieme/article/details/51383322 ","date":"2021-03-15","permalink":"/posts/k8s-ramp-up/","section":"Posts","summary":"目标 # 介绍 K8s，Docker 概念以及原理 从 0 开始部署一个简单完整的服务 Docker是什么？ # Docker是由Google推出的Go语言进行开发实现，基于Linux内核的 namespace，对进程进行封装隔离，属于操作系统层面的容器化技术。","title":"Kubernetes is All You Need"},{"content":"","date":null,"permalink":"/categories/bug-fix/","section":"Categories","summary":"","title":"Bug Fix"},{"content":"问题 # 目前，当指定访问集群外部地址为 IP 时，ingress-nginx controller 的日志中存在大量的 DNS 报错的垃圾日志。虽然不影响正常运行（猜测可能会导致性能波动，对比见最后），但是查看 Nginx 日志 debug 时效率严重降低。\n原因 # 详细的讨论见： https://github.com/coredns/coredns/issues/2324\n简略总结下，导致访问外部 IP，Nginx 报 DNS 解析错误的原因在于 Kubernetes 自身的bug，缺少了一个验证。\n出现问题的情况是通过 ExternalName 类型的 Service 访问外部服务的。定义的 yaml 类似于下面这种：\nkind: Service apiVersion: v1 metadata: name: demo2 namespace: default spec: type: ExternalName externalName: xxx.xxx.xxx.xxx 对于 Kubernetes 的设计来说，ExternalName 就是一个域名。K8s 官方是这样介绍的\nExternalName: Maps the service to the contents of the externalName field (e.g. foo.bar.example.com), by returning aCNAMErecord with its value. No proxying of any kind is set up.\nNote:ExternalName accepts an IPv4 address string, but as a DNS names comprised of digits, not as an IP address. ExternalNames that resemble IPv4 addresses are not resolved by CoreDNS or ingress-nginx because ExternalName is intended to specify a canonical DNS name.\n从实现上来看，ExternalName 类型的 Service 其实就是在 CoreDNS 里的一条 CNAME 记录。 CNAME 是一条域名指向另一个域名的记录，在 K8s 中，这条 record 记载的就是 Service 名字指向 ExtenalName 的一个映射。\n但是，当 ExternalName 类型的 Service 中设定的是 IP 时，K8s 并没有对其进行判断，仍然允许其正常创建。\n同时，Nginx 本身存在着一个轮询机制，会不断的向 DNS 服务拉取记录进行缓存。 https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua\n在每次拉取缓存时会发生以下的过程：\nNginx 从 CoreDNS 拉取到了一个 CNAME 记录，例如：demo2 -\u0026gt; xxx.xxx.xxx.xxx 接着，Nginx 尝试解析 xxx.xxx.xxx.xxx 这个域名，CoreDNS 自然是对这个长成 IP 样子的域名解析不出来的，于是解析失败，导致报错 至于为什么 DNS 解析失败之后，Nginx 仍然能够成功转发请求，原因是 ingress-nginx controller 在实现上并没有对这个进行区分。\n首先，先简单介绍下 controller 的原理。Ingress-nginx controller 一直监听着 k8s 系统中的 ingress 资源。当有新的 ingress 创建时，controller 会开始更新 Nginx 的配置文件，向其中添加转发规则，并重启 Nginx。\n下面是 controller 解析指向 ExternalName 的 ingress，然后创建 upstream 的逻辑 https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52\n可以看到，controller 是没有强制解析 ExternalName 成域名的，所以写进 nginx.conf 的 upstream 也是 ip 形式，这样 nginx 会自然地将 ExternalName 解析成 IP，从而可以正常工作。\n解决方法 # 在上面提到的 Github Issue 的讨论中，有大佬已经给出了解决方法，就是通过 Service without selectors 的方式。 https://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors\n常见的 K8s Service 都是通过标签选择器，选择一系列 Pod 作为后端，K8s endpoint controller 会自动根据 Service 的声明去为 Service 的每个端口创建一个 endpoint。Endpoint 是 K8s 中实际进行服务路由的资源。\n而创建 Service without selectors，就需要我们手动去创建一个与 Service 同名的 endpoint。这样就不需要指定 Service 为 ExternalName 的类型，CoreDNS 中就会将其视作一条 A 记录，而不是一条 CNAME 记录。Nginx 拉取 DNS 缓存时也不会把 IP 当做域名了。\napiVersion: v1 kind: Service metadata: name: demo2 namespace: default spec: clusterIP: None ports: - name: grpc port: 32443 protocol: TCP --- kind: Endpoints apiVersion: v1 metadata: name: demo2 namespace: default subsets: - addresses: - ip: xxx.xxx.xxx.xxx ports: - port: 32443 name: grpc protocol: TCP 对比 # 在两个对等的集群发生通信时，demo1 修复，demo2不修复，对比两侧的 CPU 使用情况\ndemo1： demo2： demo2 大约比 demo1 消耗 CPU 多 0.020 个核。虽然这个报错会稍微增加一点 CPU 的使用量，但并不多。\n","date":"2021-03-13","permalink":"/posts/ingress-nginx-bug-fix/","section":"Posts","summary":"问题 # 目前，当指定访问集群外部地址为 IP 时，ingress-nginx controller 的日志中存在大量的 DNS 报错的垃圾日志。虽然不影响正常运行（猜测可能会导致性能波动，对比见最后），但是查看 Nginx 日志 debug 时效率严重降低。","title":"Kubernetes Ingress Nginx DNS 报错日志 Bug Fix"},{"content":"","date":null,"permalink":"/tags/nginx/","section":"Tags","summary":"","title":"Nginx"},{"content":"本文主要介绍下 K8s 的 CronJob，还有其中的一些小坑。\n概念 # Cronjob 是 K8s 定时通过 cronjob controller 定时去创建 Job 实现的。 创建 Cronjob 的一个例子：\napiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \u0026#34;*/1 * * * *\u0026#34; jobTemplate: spec: template: spec: containers: - name: hello image: busybox imagePullPolicy: IfNotPresent args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure Cronjob controller 工作原理 # startingDeadlineSeconds 是一个很重要的参数，其配置了一个周期创建job时，多长时间算作失败。\nController 每10秒轮询一次 cronjob 对于每个 cronjob，计算从上次被调度 lastScheduleTime 到现在错过了多少次调度。如果大于100次，则将其状态置为 FailedNeedsStart 对于其他 cronjob 计算当前是否还在其 lastScheduleTime + startingDeadlineSeconds 内，如果在，则进行调度。如果不在，则发送一条 event \u0026ldquo;Missed starting window for {cronjob name}. Missed scheduled time to start a job {scheduledTime}\u0026rdquo; Tips # 时间 因为 Cronjob 实际上是通过 controller 去管理的，所以其时间是 kube-controller-manager 的时间。 命名 Cronjob 的命名要遵循 DNS subdomain name，并且不能超过 52 个字符。因为 Cronjob Controller 会在其创建的 Job 后再拼接 11 个字符 （ K8s 对 Job 命名的限制是 63 个字符） 幂等性 根据配置的重启和健康规则的不同，K8s 启动 Cronjob 时只能保证 about 一次，有时可能会启动多次，有时可能会没有启动，所以需要 cronjob 保证幂等性。 以下是 cronjob 可能会发生的两种异常情况： 触发多次 concurrencyPolicy 配置为 Allow，并且 startingDeadlineSeconds 不设置或者设置为很大。这样，controller 在多次轮询中可能都会查看到 cronjob 符合再次运行的条件，从而创建多个 job。 触发0次 concurrencyPolicy 配置为 Forbid，这样 controller 就会等到上一次cronjob结束之后才会进行下一次调度。 自定义 Crontroller 从 kubernetes 1.20 开始支持 删除运行成功的 Job 配置 https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/ successfulJobsHistoryLimit: 0 failedJobsHistoryLimit: 0 ","date":"2021-03-08","permalink":"/posts/k8s-cronjob/","section":"Posts","summary":"本文主要介绍下 K8s 的 CronJob，还有其中的一些小坑。","title":"Kubernetes CronJob"},{"content":"","date":null,"permalink":"/tags/spring/","section":"Tags","summary":"","title":"Spring"},{"content":"场景 # 使用 Spring Cloud Eureka 搭建服务注册中心，使用 Zuul 搭建服务网关，一套比较传统的微服务架构。 服务注册中心的地址为 http://localhost:8888，Zuul 网关地址为 http://localhost:8080， 另外搭建一个服务名为 metadata-service 的服务，地址为 http://localhost:8088。\n问题 # 在 metadata-service 中提供一个测试的接口\n@RestController public class MetadataController { ​ @GetMapping(value = \u0026#34;/test\u0026#34;) public int getTest() { return 1; } } 使用 Postman 进行测试，结果发现直接请求 http://localhost:8088/test 即 metadata-service 的地址，可以正常得到结果\n而通过网关，使用 Zuul 默认路由规则，调用服务，会出现 404 的错误\n分析 # 首先，我们可以先通过 http://localhost:8888 查看服务是否注册到了服务注册中心\n可以看到没有任何问题。 那么，我们再检查网关有没有获取到 metadata-service 的路由。可以通过 http://localhost:8080/actuator/routes 查看（actuator默认是关闭的，可以通过配置 management.endpoints.web.exposure.include=* 开启）。\n同样，我们可以看到没有任何问题。 那么，就很奇怪了🤨，服务本身没有任何问题，直接调用也可以访问，而通过网关一转发，为什么就 404 了呢？在网上查了一下午，也没有找到有人遇到过类似的问题。。。😱 问题的关键在我关闭服务后再次请求 http://localhost:8088/test 时终于找到了。正常情况下，关闭了服务后，应该没有返回的 response，但发出请求过后仍然是 404\n那么，就很明显了，有另一个进程也在监听 8088 端口 ！！！ 但还是很奇怪，那为什么服务启动的时候没有报端口被占用的错误呢？？？ 重新启动服务，使用 lsof -i tcp:8088 （Mac OS）查看端口占用情况\n果然有两个进程同时在监听，而一个是 IPv4，一个是 IPv6的。 首先，根据这篇文章 https://blog.csdn.net/jiyiqinlovexx/article/details/50959351 的解释，多个进程是完全可以同时监听同一个端口的。 而从 Java 7 开始，默认使用 IPv6 而不是 IPv4 （https://stackoverflow.com/questions/35470838/localhost-vs-127-0-0-1-in-spring-framework），所以对于 Spring 的 localhost 来说，其实真正使用的 IP 地址是 ::1，而不是 127.0.0.1 。使用 Postman 进行测试，可以发现 http://[::1]:8088/test 得到正常结果，而 http://127.0.0.1:8088/test 则为 404 。这就完美地解释了开启服务与停止服务，返回结果不同的问题，Spring 服务所对应的正是那个 IPv6 的进程。 那么，为什么网关转发就到了 IPv4 呢？我们再来看一下服务注册中心里的信息\n可以看到其实 Eureka 保存的是每个服务的 IP 地址是本机的 IPv4 的内网地址，而不是保存域名，这就是问题的关键。我们可以使用 Postman 发送请求 http://localhost:8080/metadata-service/test 后，使用命令 lsof -i tcp:8088 进行验证。\n可以看到的确是向内网 IP 地址，而不是向 localhost 转发请求。\n解决方案 # 至此，问题的原因已经完全清楚了，果然程序都是 debug de 出来的。 最简单的方法也很清楚了，换个端口号就 OK 了。\n如果本文有错误或者理解不对的地方，欢迎指正！！！😆\n那么，占了 8088 端口的 IPv4 进程是哪个程序呢？🤨\n。。。。Hadoop 出来挨打！！！😭😭😭\n","date":"2021-03-07","permalink":"/posts/spring-ipv6/","section":"Posts","summary":"场景 # 使用 Spring Cloud Eureka 搭建服务注册中心，使用 Zuul 搭建服务网关，一套比较传统的微服务架构。 服务注册中心的地址为 http://localhost:8888，Zuul 网关地址为 http://localhost:8080， 另外搭建一个服务名为 metadata-service 的服务，地址为 http://localhost:8088。","title":"Spring Cloud IPv6端口问题排坑"},{"content":"正在寻找2024届校招岗位 ❤️ # 本科就读于同济大学软件学院，目前硕士就读于同济大学设计与创意学院人工智能与数据设计专业 IDvX 实验室。\n实习经历： # 阿里云日志服务，后端研发实习生 宽德投资，AI平台开发实习生 华为云，算法研发实习生 字节跳动，后端研发实习生 阿里巴巴，后端研发实习生 开源经历： # iLogtail Sealos Casddor Casbin TiKV 公众号 # Misc # ISFP 尼康党 OP（胡桃，甘雨，宵宫） 崩铁 轻度二次元 (巨人, 鬼灭, JoJo\u0026hellip;) 塞尔达YYDS ","date":null,"permalink":"/about/","section":"Abing's Blog","summary":"正在寻找2024届校招岗位 ❤️ # 本科就读于同济大学软件学院，目前硕士就读于同济大学设计与创意学院人工智能与数据设计专业 IDvX 实验室。","title":"About"}]