{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"themes/one-paper/source/css/a11y-dark.min.css","path":"css/a11y-dark.min.css","modified":1,"renderable":1},{"_id":"themes/one-paper/source/css/fonts.css","path":"css/fonts.css","modified":1,"renderable":1},{"_id":"themes/one-paper/source/css/jquery.fancybox.min.css","path":"css/jquery.fancybox.min.css","modified":1,"renderable":1},{"_id":"themes/one-paper/source/css/markdown.css","path":"css/markdown.css","modified":1,"renderable":1},{"_id":"themes/one-paper/source/css/reset.css","path":"css/reset.css","modified":1,"renderable":1},{"_id":"themes/one-paper/source/css/style.css","path":"css/style.css","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600.woff","path":"fonts/montserrat-v23-latin-600.woff","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600.woff2","path":"fonts/montserrat-v23-latin-600.woff2","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600italic.woff","path":"fonts/montserrat-v23-latin-600italic.woff","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600italic.woff2","path":"fonts/montserrat-v23-latin-600italic.woff2","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-italic.woff","path":"fonts/montserrat-v23-latin-italic.woff","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-italic.woff2","path":"fonts/montserrat-v23-latin-italic.woff2","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-regular.woff","path":"fonts/montserrat-v23-latin-regular.woff","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-regular.woff2","path":"fonts/montserrat-v23-latin-regular.woff2","modified":1,"renderable":1},{"_id":"themes/one-paper/source/img/favicon.png","path":"img/favicon.png","modified":1,"renderable":1},{"_id":"themes/one-paper/source/img/one-paper.png","path":"img/one-paper.png","modified":1,"renderable":1},{"_id":"themes/one-paper/source/js/highlight.min.js","path":"js/highlight.min.js","modified":1,"renderable":1},{"_id":"themes/one-paper/source/js/highlightjs-line-numbers.js","path":"js/highlightjs-line-numbers.js","modified":1,"renderable":1},{"_id":"themes/one-paper/source/js/jquery.fancybox.min.js","path":"js/jquery.fancybox.min.js","modified":1,"renderable":1},{"_id":"themes/one-paper/source/js/jquery.min.js","path":"js/jquery.min.js","modified":1,"renderable":1},{"_id":"themes/one-paper/source/js/wrapImage.js","path":"js/wrapImage.js","modified":1,"renderable":1},{"_id":"source/asset/gzh.jpeg","path":"asset/gzh.jpeg","modified":1,"renderable":0},{"_id":"source/asset/wechat.jpg","path":"asset/wechat.jpg","modified":1,"renderable":0},{"_id":"source/images/favicon.png","path":"images/favicon.png","modified":1,"renderable":0},{"_id":"source/images/logo.png","path":"images/logo.png","modified":1,"renderable":0},{"_id":"source/asset/AwesomeTechPost0/0a98b72fdc1acab504726847f894f6f9df8a699e.png","path":"asset/AwesomeTechPost0/0a98b72fdc1acab504726847f894f6f9df8a699e.png","modified":1,"renderable":0},{"_id":"source/asset/AwesomeTechPost0/2022-06-30-00-00-42-image.png","path":"asset/AwesomeTechPost0/2022-06-30-00-00-42-image.png","modified":1,"renderable":0},{"_id":"source/asset/AwesomeTechPost0/2022-07-12-22-28-28-image.png","path":"asset/AwesomeTechPost0/2022-07-12-22-28-28-image.png","modified":1,"renderable":0},{"_id":"source/asset/AwesomeTechPost0/2022-07-14-21-55-41-image.png","path":"asset/AwesomeTechPost0/2022-07-14-21-55-41-image.png","modified":1,"renderable":0},{"_id":"source/asset/AwesomeTechPost0/2022-07-14-21-56-23-image.png","path":"asset/AwesomeTechPost0/2022-07-14-21-56-23-image.png","modified":1,"renderable":0},{"_id":"source/asset/AwesomeTechPost0/2022-07-14-23-21-08-image.png","path":"asset/AwesomeTechPost0/2022-07-14-23-21-08-image.png","modified":1,"renderable":0},{"_id":"source/asset/AwesomeTechPost0/bd74f1ae5c04b74ee412affefedd54fceffb790d.png","path":"asset/AwesomeTechPost0/bd74f1ae5c04b74ee412affefedd54fceffb790d.png","modified":1,"renderable":0},{"_id":"source/asset/AwesomeTechPost0/f5026649684b309b85e2f5880e0a84bfdd7c01de.png","path":"asset/AwesomeTechPost0/f5026649684b309b85e2f5880e0a84bfdd7c01de.png","modified":1,"renderable":0},{"_id":"source/asset/AwesomeTechPost0/fdad5d725c64c0b589b8d107e5e228d285497474.png","path":"asset/AwesomeTechPost0/fdad5d725c64c0b589b8d107e5e228d285497474.png","modified":1,"renderable":0},{"_id":"source/asset/casbin-ramp-up/2022-06-23-17-50-00-image.png","path":"asset/casbin-ramp-up/2022-06-23-17-50-00-image.png","modified":1,"renderable":0},{"_id":"source/asset/casbin-ramp-up/detail.png","path":"asset/casbin-ramp-up/detail.png","modified":1,"renderable":0},{"_id":"source/asset/casbin-ramp-up/overview.png","path":"asset/casbin-ramp-up/overview.png","modified":1,"renderable":0},{"_id":"source/asset/casbin-ramp-up/pml.png","path":"asset/casbin-ramp-up/pml.png","modified":1,"renderable":0},{"_id":"source/asset/ingress-nginx-bug-fix/demo1.png","path":"asset/ingress-nginx-bug-fix/demo1.png","modified":1,"renderable":0},{"_id":"source/asset/ingress-nginx-bug-fix/demo2.png","path":"asset/ingress-nginx-bug-fix/demo2.png","modified":1,"renderable":0},{"_id":"source/asset/ingress-nginx-bug-fix/error.png","path":"asset/ingress-nginx-bug-fix/error.png","modified":1,"renderable":0},{"_id":"source/asset/ingress-nginx-bug-fix/nginx-code.png","path":"asset/ingress-nginx-bug-fix/nginx-code.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/1.png","path":"asset/k8s-ramp-up/1.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/10.png","path":"asset/k8s-ramp-up/10.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/11.png","path":"asset/k8s-ramp-up/11.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/12.png","path":"asset/k8s-ramp-up/12.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/13.png","path":"asset/k8s-ramp-up/13.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/14.png","path":"asset/k8s-ramp-up/14.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/15.png","path":"asset/k8s-ramp-up/15.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/16.png","path":"asset/k8s-ramp-up/16.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/17.png","path":"asset/k8s-ramp-up/17.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/18.png","path":"asset/k8s-ramp-up/18.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/19.png","path":"asset/k8s-ramp-up/19.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/2.png","path":"asset/k8s-ramp-up/2.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/20.png","path":"asset/k8s-ramp-up/20.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/3.png","path":"asset/k8s-ramp-up/3.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/4.png","path":"asset/k8s-ramp-up/4.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/5.png","path":"asset/k8s-ramp-up/5.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/6.png","path":"asset/k8s-ramp-up/6.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/7.png","path":"asset/k8s-ramp-up/7.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/8.png","path":"asset/k8s-ramp-up/8.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/9.png","path":"asset/k8s-ramp-up/9.png","modified":1,"renderable":0},{"_id":"source/asset/sealos-ramp-up/1ef54cfebd1c76bc8ecfd9897f9f127107b6e555.png","path":"asset/sealos-ramp-up/1ef54cfebd1c76bc8ecfd9897f9f127107b6e555.png","modified":1,"renderable":0},{"_id":"source/asset/sealos-ramp-up/313c52bda7b27b5b64a479e488404bcef56b2669.png","path":"asset/sealos-ramp-up/313c52bda7b27b5b64a479e488404bcef56b2669.png","modified":1,"renderable":0},{"_id":"source/asset/sealos-ramp-up/c668a66b40dbc788695a9cbb8ec3f76a9897503a.png","path":"asset/sealos-ramp-up/c668a66b40dbc788695a9cbb8ec3f76a9897503a.png","modified":1,"renderable":0},{"_id":"source/asset/sealos-ramp-up/f6ce5cbedc6aa338007cd4633935ad371086271c.png","path":"asset/sealos-ramp-up/f6ce5cbedc6aa338007cd4633935ad371086271c.png","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/1.jpg","path":"asset/spring_ipv6/1.jpg","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/2.jpg","path":"asset/spring_ipv6/2.jpg","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/3.jpg","path":"asset/spring_ipv6/3.jpg","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/4.jpg","path":"asset/spring_ipv6/4.jpg","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/5.jpg","path":"asset/spring_ipv6/5.jpg","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/6.jpg","path":"asset/spring_ipv6/6.jpg","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/7.jpg","path":"asset/spring_ipv6/7.jpg","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/8.jpg","path":"asset/spring_ipv6/8.jpg","modified":1,"renderable":0},{"_id":"source/asset/tidb-lightning/1.png","path":"asset/tidb-lightning/1.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/dist_seg.png","path":"asset/time2graph/dist_seg.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/dist_series.png","path":"asset/time2graph/dist_series.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/dtw.png","path":"asset/time2graph/dtw.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/edge_weight.png","path":"asset/time2graph/edge_weight.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/graph.png","path":"asset/time2graph/graph.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/graph_algo.png","path":"asset/time2graph/graph_algo.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/loss_function.png","path":"asset/time2graph/loss_function.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/representation.png","path":"asset/time2graph/representation.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/segment.png","path":"asset/time2graph/segment.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/transition.png","path":"asset/time2graph/transition.png","modified":1,"renderable":0}],"Cache":[{"_id":"source/_posts/2021.md","hash":"af0e92037b53db0c5e6210d92a7d4c204c0593a2","modified":1668962802025},{"_id":"source/_posts/2022-00.md","hash":"74fd97ff516a5bdcc7a0f4be42c458d1b282a614","modified":1668962802025},{"_id":"source/_posts/2022-01.md","hash":"01dd4541302f01a051004322d47270e5232d3e26","modified":1668962802025},{"_id":"source/_posts/2022-02.md","hash":"ca393f144e1cf2ed87c27130fd88895f9ae42e3a","modified":1668962802025},{"_id":"source/_posts/Spring IPv6.md","hash":"b4adb3b6cbd5cc1c495a0989dab33cd62be7f3f7","modified":1668962802025},{"_id":"source/_posts/awesome-tech-post-0.md","hash":"9d83f222f61b40c503bc155fdb1ad7d131a5e486","modified":1668962802025},{"_id":"source/_posts/casbin-ramp-up.md","hash":"cf8892acf50f5ccbb8af4da856630b45dc75ae66","modified":1668962802025},{"_id":"source/_posts/ingress-nginx-bug-fix.md","hash":"1950e0dfb7868fd1f37ed7bd6f2e9cc858c690f9","modified":1668962802025},{"_id":"source/_posts/k8s-cronjob.md","hash":"dd7c5483287ea4507364f6a12456fea261e2ff74","modified":1668962802025},{"_id":"source/_posts/k8s-ramp-up.md","hash":"6bec53ebfdf62ad12bc6f14d9ab1e818603c49ca","modified":1668962802025},{"_id":"source/_posts/sealos-casdoor.md","hash":"366a98139012fb94963ce8978d77de5de4a27916","modified":1668962802025},{"_id":"source/_posts/sealos-ramp-up.md","hash":"3a7af65e9c7a369322107b4e7ead1273f634ecfd","modified":1668962802025},{"_id":"source/_posts/tidb-lightning.md","hash":"e7489c069ae87d361fabae3db5d3fc551dd5b742","modified":1668962802025},{"_id":"source/_posts/time2graph.md","hash":"b43727631ac7f026ffec630b32a296a7f83b19ff","modified":1668962802025},{"_id":"source/about/index.md","hash":"823efc6759db6c3a93b90aed1a6fa8143ee5b30a","modified":1668962802025},{"_id":"source/asset/gzh.jpeg","hash":"f4b4ef81c04cbb3db2b6c3cb98aa5b27bf7a51c8","modified":1668962802033},{"_id":"source/images/favicon.png","hash":"560e57ba077640c78fd41236652e7534816ec009","modified":1668962802065},{"_id":"source/weekly/index.md","hash":"c0e2f7f6a6d653cc86574505c7ce5056946f1fb5","modified":1668962802065},{"_id":"source/asset/AwesomeTechPost0/0a98b72fdc1acab504726847f894f6f9df8a699e.png","hash":"a2991eed6c61c508dbbe0ead891e54ee26bfaa20","modified":1668962802025},{"_id":"source/asset/AwesomeTechPost0/2022-07-14-21-55-41-image.png","hash":"283d63824ba2e1e21497121cb6a5fa31927ddc7f","modified":1668962802029},{"_id":"source/asset/AwesomeTechPost0/2022-07-14-21-56-23-image.png","hash":"67c20872a89386124e28b8ead6a48bc65c69c765","modified":1668962802029},{"_id":"source/asset/AwesomeTechPost0/2022-07-14-23-21-08-image.png","hash":"953980bf439f425610cdad9ce2a4b3a4ad9fe7f4","modified":1668962802029},{"_id":"source/asset/AwesomeTechPost0/bd74f1ae5c04b74ee412affefedd54fceffb790d.png","hash":"aa7d2fdfe38cecd6a54a3be339e169169c5d1219","modified":1668962802029},{"_id":"source/asset/AwesomeTechPost0/f5026649684b309b85e2f5880e0a84bfdd7c01de.png","hash":"7b87586157f919bf244602b8a04899ea78f139b2","modified":1668962802029},{"_id":"source/asset/AwesomeTechPost0/fdad5d725c64c0b589b8d107e5e228d285497474.png","hash":"953980bf439f425610cdad9ce2a4b3a4ad9fe7f4","modified":1668962802029},{"_id":"source/asset/casbin-ramp-up/2022-06-23-17-50-00-image.png","hash":"85dbee6e8a4e8a580796afdaca0d519c4b81b982","modified":1668962802029},{"_id":"source/asset/ingress-nginx-bug-fix/demo1.png","hash":"d1c15eebd8aa34dedff6d51450cc5a64d4279816","modified":1668962802033},{"_id":"source/asset/ingress-nginx-bug-fix/demo2.png","hash":"07e0faffc6e009107087727cd35b12c85b5881af","modified":1668962802033},{"_id":"source/asset/k8s-ramp-up/1.png","hash":"1145d712a9399a51a454f775f974e5862b44f98c","modified":1668962802037},{"_id":"source/asset/k8s-ramp-up/10.png","hash":"d3921df8340cdf5c5d9de6753980ed05dc4ecd06","modified":1668962802037},{"_id":"source/asset/k8s-ramp-up/11.png","hash":"49505e81f7ed4f201325cdca173f92402e1d694e","modified":1668962802037},{"_id":"source/asset/k8s-ramp-up/13.png","hash":"0e547f7ed382f42d554e2ddc2f56b1b55a35121a","modified":1668962802041},{"_id":"source/asset/k8s-ramp-up/14.png","hash":"236020ab61732ffb54c7f73d108627cab79e3780","modified":1668962802041},{"_id":"source/asset/k8s-ramp-up/15.png","hash":"13dfe7c37ce4ae14152f31b65cd78b98258417c8","modified":1668962802041},{"_id":"source/asset/k8s-ramp-up/16.png","hash":"bfc1602be4127162d5a27c53abcbe08565ed38a0","modified":1668962802041},{"_id":"source/asset/k8s-ramp-up/17.png","hash":"7fdada3095564db64c1a5766ebe60400040185e1","modified":1668962802041},{"_id":"source/asset/k8s-ramp-up/18.png","hash":"7097617a943f79f44293b875bd380d0343f7a1ed","modified":1668962802041},{"_id":"source/asset/k8s-ramp-up/2.png","hash":"67564259b9f4cc49111db67cf344e3c315220d04","modified":1668962802041},{"_id":"source/asset/k8s-ramp-up/3.png","hash":"9b61eafd03c74e9f1581294ae784e3b388a43060","modified":1668962802041},{"_id":"source/asset/k8s-ramp-up/7.png","hash":"3039d3abcb057f70af3f0544990973af1d32028b","modified":1668962802045},{"_id":"source/asset/k8s-ramp-up/8.png","hash":"fbd07cba2115dc1eec09d47d8206212f71c5739c","modified":1668962802045},{"_id":"source/asset/k8s-ramp-up/9.png","hash":"f2a631454ba533aace37b5434455bc67e3245010","modified":1668962802045},{"_id":"source/asset/sealos-ramp-up/f6ce5cbedc6aa338007cd4633935ad371086271c.png","hash":"0d91c0b024ddf29db77f8111cf9730e2949fa5b0","modified":1668962802057},{"_id":"source/asset/spring_ipv6/1.jpg","hash":"906e5182a059d051b7ffab73fcfe166b28b31dbe","modified":1668962802057},{"_id":"source/asset/spring_ipv6/2.jpg","hash":"c18e62cbf298d52caa5b0d4faf4d9a692ed6da35","modified":1668962802057},{"_id":"source/asset/spring_ipv6/3.jpg","hash":"bbbcd41f80a39c98639a646aade9d0bda86eda8b","modified":1668962802057},{"_id":"source/asset/spring_ipv6/4.jpg","hash":"352ccca707eeb3283ca4ac10d527957e7857abae","modified":1668962802061},{"_id":"source/asset/spring_ipv6/5.jpg","hash":"ef9044806637a7c8859d68c3a6278759d80d1fac","modified":1668962802061},{"_id":"source/asset/spring_ipv6/6.jpg","hash":"b8aafcca7247b087cdc9e0882c698738f95c21eb","modified":1668962802061},{"_id":"source/asset/spring_ipv6/7.jpg","hash":"3d111ed99d1fa8bbcb514391c34543693b023d56","modified":1668962802061},{"_id":"source/asset/spring_ipv6/8.jpg","hash":"071e4d22f20b22683d78ca8b328484e239f791c2","modified":1668962802061},{"_id":"source/asset/tidb-lightning/1.png","hash":"a9ea9c9a6539847d38a68e77d82c6ce1f3fec0d4","modified":1668962802061},{"_id":"source/asset/time2graph/dist_seg.png","hash":"f193d5d4793b8661a110a33741cf6d5e378011a3","modified":1668962802061},{"_id":"source/asset/time2graph/dist_series.png","hash":"0e899b88e99c163dd7ea2f0a649e0aaee56c99eb","modified":1668962802061},{"_id":"source/asset/time2graph/edge_weight.png","hash":"f513a48ad11a011510687d036da37b4501f7074e","modified":1668962802061},{"_id":"source/asset/time2graph/loss_function.png","hash":"f193d5d4793b8661a110a33741cf6d5e378011a3","modified":1668962802061},{"_id":"source/asset/time2graph/segment.png","hash":"320e2b64f2caa5168692778a6b87c72eb12d8964","modified":1668962802065},{"_id":"themes/one-paper/LICENSE","hash":"aad1dcb7deccd18a89508fa2ad78101dafa10cc9","modified":1668962802065},{"_id":"themes/one-paper/README.md","hash":"39b4a0d2b37a27f5d247339b8e851ed4d89e0851","modified":1668962802065},{"_id":"themes/one-paper/_config.yml","hash":"e914bbf20dbc81fab5eb67a3c923a1dfa168741d","modified":1668962802065},{"_id":"themes/one-paper/layout/index.ejs","hash":"15ec2c9a8474f74bd92ddee056c2372d655bc477","modified":1668962802065},{"_id":"themes/one-paper/layout/index1.ejs","hash":"8effbac188b6d6d30fce5ba9e795b0a5ec3ccd5c","modified":1668962802065},{"_id":"themes/one-paper/layout/layout.ejs","hash":"a0a7b76a1d18f39e684ed5ae204fc627e51ceef9","modified":1668962802065},{"_id":"themes/one-paper/layout/post.ejs","hash":"e8c1a330c20a1283b18a4b36739414c0b295bdc9","modified":1668962802065},{"_id":"themes/one-paper/layout/weekly.ejs","hash":"28c0e63d067fe8b36b652e1f0bf4e31ba2ae7255","modified":1668962802065},{"_id":"themes/one-paper/layout/_partial/footer.ejs","hash":"87498f5131b82e48e48a7ed05e35490c6cdd30c1","modified":1668962802065},{"_id":"themes/one-paper/layout/_partial/head.ejs","hash":"5ec2ff031eabb2b8f94434be021cf915466c6806","modified":1668962802065},{"_id":"themes/one-paper/layout/_partial/header.ejs","hash":"2b54259f35942eeb1b99482c2a4517152fc87f0c","modified":1668962802065},{"_id":"themes/one-paper/layout/_partial/post-header.ejs","hash":"f6fa471122459b25a9948405d72cb5f197d2fc25","modified":1668962802065},{"_id":"themes/one-paper/source/css/a11y-dark.min.css","hash":"e0a3294faa7dfa1eae300caea5a01f438b643b93","modified":1668962802065},{"_id":"themes/one-paper/source/css/fonts.css","hash":"bd6171c8de8d9f4efafca3802c4d20099d7fca1c","modified":1668962802065},{"_id":"themes/one-paper/source/css/jquery.fancybox.min.css","hash":"1be9b79be02a1cfc5d96c4a5e0feb8f472babd95","modified":1668962802065},{"_id":"themes/one-paper/source/css/markdown.css","hash":"1ea92ff8823ba9ae529c2037e247041562c20416","modified":1668962802065},{"_id":"themes/one-paper/source/css/reset.css","hash":"d65e6739126106967ea5da4639f2abb3c94f84fc","modified":1668962802065},{"_id":"themes/one-paper/source/css/style.css","hash":"d5257d08fb84149d7ba5d5df0a1ece83e06d8523","modified":1668962802065},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600.woff","hash":"925d9f095488dc77dd84e8414422f0113f4628a9","modified":1668962802065},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600.woff2","hash":"2fe30978041c41a2994ac0fd491e83d32a3203b7","modified":1668962802065},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600italic.woff","hash":"c0e80c18fac1cd10469c4f922ad92e81fc8b3b94","modified":1668962802065},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600italic.woff2","hash":"1f24e9edcccd42d4694a4020d6a8f9b9cb28f471","modified":1668962802065},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-italic.woff","hash":"fca5ee87a17c57eb53265da1c2c75db7305ad69c","modified":1668962802065},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-italic.woff2","hash":"ce1eae3f714702a82c1e9c05b5ba302a9e91ac20","modified":1668962802065},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-regular.woff","hash":"285adda1da1fc15583ad53160d66032aeccb45ea","modified":1668962802065},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-regular.woff2","hash":"f7eefafb7bfdc6b5572714fa267268b845a67cf4","modified":1668962802065},{"_id":"themes/one-paper/source/img/favicon.png","hash":"0845678601e8b144ae45c448a25650f4d3d2182d","modified":1668962802065},{"_id":"themes/one-paper/source/js/highlightjs-line-numbers.js","hash":"690e96133591495fa847d828573bd0576b2d168a","modified":1668962802069},{"_id":"source/asset/k8s-ramp-up/20.png","hash":"2b79e55f2b4022ab98679f6cc9ce315cc93c82d4","modified":1668962802041},{"_id":"source/images/logo.png","hash":"e592e037815d3557acb05c4d64156dcf2d100b11","modified":1668962802065},{"_id":"source/asset/AwesomeTechPost0/2022-06-30-00-00-42-image.png","hash":"9e663e841b4096322459f283d7cf7ee07088ca94","modified":1668962802025},{"_id":"source/asset/casbin-ramp-up/pml.png","hash":"67b4266d1be6921cb896a25476ad95706dbf180b","modified":1668962802033},{"_id":"source/asset/sealos-ramp-up/313c52bda7b27b5b64a479e488404bcef56b2669.png","hash":"1e8a8b00d1f4c9f0fbed531f7115a960d015461d","modified":1668962802057},{"_id":"source/asset/sealos-ramp-up/c668a66b40dbc788695a9cbb8ec3f76a9897503a.png","hash":"75d76e3fa2f5863cf2030839b5f40013004777d0","modified":1668962802057},{"_id":"source/asset/time2graph/dtw.png","hash":"8a4ae6f36b3c32b2093159e15484b87e9a3f3897","modified":1668962802061},{"_id":"source/asset/time2graph/graph_algo.png","hash":"59b2cb4afea4e79669eb16389f89ec5d82a775c9","modified":1668962802061},{"_id":"source/asset/time2graph/representation.png","hash":"cc40893416b246fb57a65fe6a338aa310f05df73","modified":1668962802061},{"_id":"source/asset/time2graph/transition.png","hash":"d942684dfc94087b7131fa3ccefffce571fcec62","modified":1668962802065},{"_id":"themes/one-paper/source/js/highlight.min.js","hash":"d264ad16bdf39cfec2b06c20223b87fcb37ad27b","modified":1668962802069},{"_id":"themes/one-paper/source/js/jquery.fancybox.min.js","hash":"6181412e73966696d08e1e5b1243a572d0f22ba6","modified":1668962802069},{"_id":"source/asset/wechat.jpg","hash":"6bc1d7fab98a60524d136f1387805d233fc2ad53","modified":1668962802065},{"_id":"source/asset/AwesomeTechPost0/2022-07-12-22-28-28-image.png","hash":"cc586729882b6705fd57e9404d9f43cf640abfac","modified":1668962802029},{"_id":"source/asset/casbin-ramp-up/overview.png","hash":"3f0f7dc338aa80ffed952127f7bb876db21f63a7","modified":1668962802033},{"_id":"themes/one-paper/source/js/wrapImage.js","hash":"ab59573784993b30de8216fe5db08cd6d6c20c33","modified":1668962802069},{"_id":"source/asset/k8s-ramp-up/19.png","hash":"f190e9ea5ce21409f6aa6acf21698cedf1296f16","modified":1668962802041},{"_id":"source/asset/k8s-ramp-up/4.png","hash":"c9a7ff67e06993bc632029af641f27bfe8409796","modified":1668962802041},{"_id":"source/asset/k8s-ramp-up/6.png","hash":"3718c5ba7b63809eccfb9c92ffd3063b8fbd0309","modified":1668962802045},{"_id":"source/asset/time2graph/graph.png","hash":"f82e9eef1608afb0116be9358690665a1cd32fef","modified":1668962802061},{"_id":"source/asset/ingress-nginx-bug-fix/error.png","hash":"5816144a23ecd7690a25adc2492973eb920ed1e9","modified":1668962802033},{"_id":"source/asset/ingress-nginx-bug-fix/nginx-code.png","hash":"67bdbe585956405d3780bad1b08e686b824c5321","modified":1668962802033},{"_id":"themes/one-paper/source/js/jquery.min.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1668962802069},{"_id":"source/asset/k8s-ramp-up/5.png","hash":"5187e67252cf14c89b7397833ad2618c6b89421b","modified":1668962802045},{"_id":"source/asset/casbin-ramp-up/detail.png","hash":"d6348e860505ce70f269432e2ca8a03079453069","modified":1668962802029},{"_id":"themes/one-paper/source/img/one-paper.png","hash":"eadd349e5316a154099cb06e41abe5a105940e37","modified":1668962802069},{"_id":"source/asset/k8s-ramp-up/12.png","hash":"c873022232968a954dfd899a7d717c155615c71e","modified":1668962802041},{"_id":"source/asset/sealos-ramp-up/1ef54cfebd1c76bc8ecfd9897f9f127107b6e555.png","hash":"a23181cbce60ad4f931ee79d27cf8424922c63b3","modified":1668962802057},{"_id":"public/sitemap.xml","hash":"8089ea1e0d6d8427eaf70766dd41eb44316995cd","modified":1668962821267},{"_id":"public/sitemap.txt","hash":"7521de7ec3a71ae94d3783be5e695d494530afa9","modified":1668962821267},{"_id":"public/submit_urls.txt","hash":"4a269e376a899083538a82df552b0fba847edbce","modified":1668962821267},{"_id":"public/about/index.html","hash":"520a3ba1547f22f496445c099b3e619b6fd1b43b","modified":1668962821267},{"_id":"public/weekly/index.html","hash":"b37a844874bb77cc98fae41b5f336890862d1dc0","modified":1668962821267},{"_id":"public/2022/11/20/2022-02/index.html","hash":"af2f23bdf8bcf0cf0f28a64b0e6a18e8bc8b99c1","modified":1668962821267},{"_id":"public/2022/11/13/2022-01/index.html","hash":"ddef6b19159af6db638a37e2210303b2b1167218","modified":1668962821267},{"_id":"public/2022/11/06/2022-00/index.html","hash":"77d3241b433d06e8ed5408a23c3d266e076b5e0b","modified":1668962821267},{"_id":"public/2022/09/04/sealos-casdoor/index.html","hash":"002caae124412a9d22d59e0da312aa8135ed9e3b","modified":1668962821267},{"_id":"public/2022/06/01/2021/index.html","hash":"e4cd3c1c62a078ac7fc0e147c8a56432be7d1a45","modified":1668962821267},{"_id":"public/2021/09/06/time2graph/index.html","hash":"84ef93f60287d333a1e8ebcf3812844ca56a3081","modified":1668962821267},{"_id":"public/2021/03/19/tidb-lightning/index.html","hash":"bf042009de45afb22b3ae5bdbb4755b60b29b726","modified":1668962821267},{"_id":"public/2021/03/13/ingress-nginx-bug-fix/index.html","hash":"69f85222d954d98f58964fa4aff946f4d1a8ff27","modified":1668962821267},{"_id":"public/2021/03/08/k8s-cronjob/index.html","hash":"f757916081a6246c433592af1d439e95156ad9cc","modified":1668962821267},{"_id":"public/2021/03/07/Spring IPv6/index.html","hash":"a150f4436acea626dab24fbdcc42ca1df1088774","modified":1668962821267},{"_id":"public/archives/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/archives/page/2/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/archives/2021/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/archives/2021/03/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/archives/2021/09/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/archives/2022/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/archives/2022/06/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/archives/2022/07/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/archives/2022/09/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/archives/2022/10/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/archives/2022/11/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/page/2/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/tags/weekly/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/tags/2021/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/tags/2022-00/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/tags/2022-01/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/tags/2022-02/index.html","hash":"c85c98a9d271aee8b25db2df78159da7d04c3c49","modified":1668962821267},{"_id":"public/2022/10/30/sealos-ramp-up/index.html","hash":"d49cd8d2b04fb8528cc4dd4b348ddbc882e731fd","modified":1668962821267},{"_id":"public/2022/07/15/awesome-tech-post-0/index.html","hash":"ae42059f40281864ed3f62dd303ab6847a516a9a","modified":1668962821267},{"_id":"public/2022/06/22/casbin-ramp-up/index.html","hash":"baba22b9ba91e30544abd3bcfca87754d4be4f9d","modified":1668962821267},{"_id":"public/2021/03/15/k8s-ramp-up/index.html","hash":"90182980de5f97520172e745bf2a89ea09333e51","modified":1668962821267},{"_id":"public/fonts/montserrat-v23-latin-600.woff","hash":"925d9f095488dc77dd84e8414422f0113f4628a9","modified":1668962821267},{"_id":"public/fonts/montserrat-v23-latin-600.woff2","hash":"2fe30978041c41a2994ac0fd491e83d32a3203b7","modified":1668962821267},{"_id":"public/fonts/montserrat-v23-latin-600italic.woff","hash":"c0e80c18fac1cd10469c4f922ad92e81fc8b3b94","modified":1668962821267},{"_id":"public/fonts/montserrat-v23-latin-600italic.woff2","hash":"1f24e9edcccd42d4694a4020d6a8f9b9cb28f471","modified":1668962821267},{"_id":"public/fonts/montserrat-v23-latin-italic.woff","hash":"fca5ee87a17c57eb53265da1c2c75db7305ad69c","modified":1668962821267},{"_id":"public/fonts/montserrat-v23-latin-italic.woff2","hash":"ce1eae3f714702a82c1e9c05b5ba302a9e91ac20","modified":1668962821267},{"_id":"public/fonts/montserrat-v23-latin-regular.woff","hash":"285adda1da1fc15583ad53160d66032aeccb45ea","modified":1668962821267},{"_id":"public/fonts/montserrat-v23-latin-regular.woff2","hash":"f7eefafb7bfdc6b5572714fa267268b845a67cf4","modified":1668962821267},{"_id":"public/img/favicon.png","hash":"0845678601e8b144ae45c448a25650f4d3d2182d","modified":1668962821267},{"_id":"public/asset/gzh.jpeg","hash":"f4b4ef81c04cbb3db2b6c3cb98aa5b27bf7a51c8","modified":1668962821267},{"_id":"public/images/favicon.png","hash":"560e57ba077640c78fd41236652e7534816ec009","modified":1668962821267},{"_id":"public/asset/AwesomeTechPost0/0a98b72fdc1acab504726847f894f6f9df8a699e.png","hash":"a2991eed6c61c508dbbe0ead891e54ee26bfaa20","modified":1668962821267},{"_id":"public/asset/AwesomeTechPost0/2022-07-14-21-55-41-image.png","hash":"283d63824ba2e1e21497121cb6a5fa31927ddc7f","modified":1668962821267},{"_id":"public/asset/AwesomeTechPost0/2022-07-14-21-56-23-image.png","hash":"67c20872a89386124e28b8ead6a48bc65c69c765","modified":1668962821267},{"_id":"public/asset/AwesomeTechPost0/2022-07-14-23-21-08-image.png","hash":"953980bf439f425610cdad9ce2a4b3a4ad9fe7f4","modified":1668962821267},{"_id":"public/asset/AwesomeTechPost0/bd74f1ae5c04b74ee412affefedd54fceffb790d.png","hash":"aa7d2fdfe38cecd6a54a3be339e169169c5d1219","modified":1668962821267},{"_id":"public/asset/AwesomeTechPost0/f5026649684b309b85e2f5880e0a84bfdd7c01de.png","hash":"7b87586157f919bf244602b8a04899ea78f139b2","modified":1668962821267},{"_id":"public/asset/AwesomeTechPost0/fdad5d725c64c0b589b8d107e5e228d285497474.png","hash":"953980bf439f425610cdad9ce2a4b3a4ad9fe7f4","modified":1668962821267},{"_id":"public/asset/casbin-ramp-up/2022-06-23-17-50-00-image.png","hash":"85dbee6e8a4e8a580796afdaca0d519c4b81b982","modified":1668962821267},{"_id":"public/asset/ingress-nginx-bug-fix/demo1.png","hash":"d1c15eebd8aa34dedff6d51450cc5a64d4279816","modified":1668962821267},{"_id":"public/asset/ingress-nginx-bug-fix/demo2.png","hash":"07e0faffc6e009107087727cd35b12c85b5881af","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/1.png","hash":"1145d712a9399a51a454f775f974e5862b44f98c","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/10.png","hash":"d3921df8340cdf5c5d9de6753980ed05dc4ecd06","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/11.png","hash":"49505e81f7ed4f201325cdca173f92402e1d694e","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/13.png","hash":"0e547f7ed382f42d554e2ddc2f56b1b55a35121a","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/14.png","hash":"236020ab61732ffb54c7f73d108627cab79e3780","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/15.png","hash":"13dfe7c37ce4ae14152f31b65cd78b98258417c8","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/16.png","hash":"bfc1602be4127162d5a27c53abcbe08565ed38a0","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/17.png","hash":"7fdada3095564db64c1a5766ebe60400040185e1","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/18.png","hash":"7097617a943f79f44293b875bd380d0343f7a1ed","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/19.png","hash":"f190e9ea5ce21409f6aa6acf21698cedf1296f16","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/2.png","hash":"67564259b9f4cc49111db67cf344e3c315220d04","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/20.png","hash":"2b79e55f2b4022ab98679f6cc9ce315cc93c82d4","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/3.png","hash":"9b61eafd03c74e9f1581294ae784e3b388a43060","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/7.png","hash":"3039d3abcb057f70af3f0544990973af1d32028b","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/8.png","hash":"fbd07cba2115dc1eec09d47d8206212f71c5739c","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/9.png","hash":"f2a631454ba533aace37b5434455bc67e3245010","modified":1668962821267},{"_id":"public/asset/sealos-ramp-up/f6ce5cbedc6aa338007cd4633935ad371086271c.png","hash":"0d91c0b024ddf29db77f8111cf9730e2949fa5b0","modified":1668962821267},{"_id":"public/asset/spring_ipv6/1.jpg","hash":"906e5182a059d051b7ffab73fcfe166b28b31dbe","modified":1668962821267},{"_id":"public/asset/spring_ipv6/2.jpg","hash":"c18e62cbf298d52caa5b0d4faf4d9a692ed6da35","modified":1668962821267},{"_id":"public/asset/spring_ipv6/3.jpg","hash":"bbbcd41f80a39c98639a646aade9d0bda86eda8b","modified":1668962821267},{"_id":"public/asset/spring_ipv6/4.jpg","hash":"352ccca707eeb3283ca4ac10d527957e7857abae","modified":1668962821267},{"_id":"public/asset/spring_ipv6/5.jpg","hash":"ef9044806637a7c8859d68c3a6278759d80d1fac","modified":1668962821267},{"_id":"public/asset/spring_ipv6/6.jpg","hash":"b8aafcca7247b087cdc9e0882c698738f95c21eb","modified":1668962821267},{"_id":"public/asset/spring_ipv6/7.jpg","hash":"3d111ed99d1fa8bbcb514391c34543693b023d56","modified":1668962821267},{"_id":"public/asset/spring_ipv6/8.jpg","hash":"071e4d22f20b22683d78ca8b328484e239f791c2","modified":1668962821267},{"_id":"public/asset/tidb-lightning/1.png","hash":"a9ea9c9a6539847d38a68e77d82c6ce1f3fec0d4","modified":1668962821267},{"_id":"public/asset/time2graph/dist_seg.png","hash":"f193d5d4793b8661a110a33741cf6d5e378011a3","modified":1668962821267},{"_id":"public/asset/time2graph/dist_series.png","hash":"0e899b88e99c163dd7ea2f0a649e0aaee56c99eb","modified":1668962821267},{"_id":"public/asset/time2graph/edge_weight.png","hash":"f513a48ad11a011510687d036da37b4501f7074e","modified":1668962821267},{"_id":"public/asset/time2graph/loss_function.png","hash":"f193d5d4793b8661a110a33741cf6d5e378011a3","modified":1668962821267},{"_id":"public/asset/time2graph/segment.png","hash":"320e2b64f2caa5168692778a6b87c72eb12d8964","modified":1668962821267},{"_id":"public/images/logo.png","hash":"e592e037815d3557acb05c4d64156dcf2d100b11","modified":1668962821267},{"_id":"public/asset/AwesomeTechPost0/2022-06-30-00-00-42-image.png","hash":"9e663e841b4096322459f283d7cf7ee07088ca94","modified":1668962821267},{"_id":"public/asset/casbin-ramp-up/pml.png","hash":"67b4266d1be6921cb896a25476ad95706dbf180b","modified":1668962821267},{"_id":"public/asset/sealos-ramp-up/313c52bda7b27b5b64a479e488404bcef56b2669.png","hash":"1e8a8b00d1f4c9f0fbed531f7115a960d015461d","modified":1668962821267},{"_id":"public/asset/sealos-ramp-up/c668a66b40dbc788695a9cbb8ec3f76a9897503a.png","hash":"75d76e3fa2f5863cf2030839b5f40013004777d0","modified":1668962821267},{"_id":"public/asset/time2graph/dtw.png","hash":"8a4ae6f36b3c32b2093159e15484b87e9a3f3897","modified":1668962821267},{"_id":"public/asset/time2graph/graph_algo.png","hash":"59b2cb4afea4e79669eb16389f89ec5d82a775c9","modified":1668962821267},{"_id":"public/asset/time2graph/representation.png","hash":"cc40893416b246fb57a65fe6a338aa310f05df73","modified":1668962821267},{"_id":"public/asset/time2graph/transition.png","hash":"d942684dfc94087b7131fa3ccefffce571fcec62","modified":1668962821267},{"_id":"public/css/a11y-dark.min.css","hash":"e0a3294faa7dfa1eae300caea5a01f438b643b93","modified":1668962821267},{"_id":"public/css/fonts.css","hash":"bd6171c8de8d9f4efafca3802c4d20099d7fca1c","modified":1668962821267},{"_id":"public/css/jquery.fancybox.min.css","hash":"1be9b79be02a1cfc5d96c4a5e0feb8f472babd95","modified":1668962821267},{"_id":"public/css/markdown.css","hash":"1ea92ff8823ba9ae529c2037e247041562c20416","modified":1668962821267},{"_id":"public/css/reset.css","hash":"d65e6739126106967ea5da4639f2abb3c94f84fc","modified":1668962821267},{"_id":"public/css/style.css","hash":"d5257d08fb84149d7ba5d5df0a1ece83e06d8523","modified":1668962821267},{"_id":"public/js/highlightjs-line-numbers.js","hash":"690e96133591495fa847d828573bd0576b2d168a","modified":1668962821267},{"_id":"public/js/wrapImage.js","hash":"ab59573784993b30de8216fe5db08cd6d6c20c33","modified":1668962821267},{"_id":"public/js/highlight.min.js","hash":"d264ad16bdf39cfec2b06c20223b87fcb37ad27b","modified":1668962821267},{"_id":"public/js/jquery.fancybox.min.js","hash":"6181412e73966696d08e1e5b1243a572d0f22ba6","modified":1668962821267},{"_id":"public/js/jquery.min.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1668962821267},{"_id":"public/asset/wechat.jpg","hash":"6bc1d7fab98a60524d136f1387805d233fc2ad53","modified":1668962821267},{"_id":"public/asset/AwesomeTechPost0/2022-07-12-22-28-28-image.png","hash":"cc586729882b6705fd57e9404d9f43cf640abfac","modified":1668962821267},{"_id":"public/asset/casbin-ramp-up/overview.png","hash":"3f0f7dc338aa80ffed952127f7bb876db21f63a7","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/4.png","hash":"c9a7ff67e06993bc632029af641f27bfe8409796","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/6.png","hash":"3718c5ba7b63809eccfb9c92ffd3063b8fbd0309","modified":1668962821267},{"_id":"public/asset/time2graph/graph.png","hash":"f82e9eef1608afb0116be9358690665a1cd32fef","modified":1668962821267},{"_id":"public/asset/ingress-nginx-bug-fix/error.png","hash":"5816144a23ecd7690a25adc2492973eb920ed1e9","modified":1668962821267},{"_id":"public/asset/ingress-nginx-bug-fix/nginx-code.png","hash":"67bdbe585956405d3780bad1b08e686b824c5321","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/5.png","hash":"5187e67252cf14c89b7397833ad2618c6b89421b","modified":1668962821267},{"_id":"public/img/one-paper.png","hash":"eadd349e5316a154099cb06e41abe5a105940e37","modified":1668962821267},{"_id":"public/asset/casbin-ramp-up/detail.png","hash":"d6348e860505ce70f269432e2ca8a03079453069","modified":1668962821267},{"_id":"public/asset/k8s-ramp-up/12.png","hash":"c873022232968a954dfd899a7d717c155615c71e","modified":1668962821267},{"_id":"public/asset/sealos-ramp-up/1ef54cfebd1c76bc8ecfd9897f9f127107b6e555.png","hash":"a23181cbce60ad4f931ee79d27cf8424922c63b3","modified":1668962821267}],"Category":[],"Data":[],"Page":[{"title":"ABOUT","_content":"[<img class=\"not-scale\" src=\"https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white\" alt=\"https://github.com/Abingcbc\" style=\"display:inline\">](https://github.com/Abingcbc) [<img class=\"not-scale\" src=\"https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&amp;logo=gmail&amp;logoColor=white\" alt=\"mailto:abingcbc626@gmail.com\" style=\"display:inline\">](mailto:abingcbc626@gmail.com) [<img class=\"not-scale\" src=\"https://img.shields.io/badge/linkedin-%230077B5.svg?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white\" alt=\"https://www.linkedin.com/in/bingchang-chen-8b3812183/\" style=\"display:inline\">](https://www.linkedin.com/in/bingchang-chen-8b3812183/) [<img class=\"not-scale\" src=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" alt=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" style=\"display:inline\">](https://www.zhihu.com/people/llll-48-29) [<img  class=\"not-scale\" src=\"https://img.shields.io/badge/Twitter-%231DA1F2.svg?style=for-the-badge&logo=Twitter&logoColor=white\" alt=\"https://img.shields.io/badge/Twitter-%231DA1F2.svg?style=for-the-badge&logo=Twitter&logoColor=white\" style=\"display:inline\">](https://twitter.com/abingcbc)\n<hr>\n\n### 正在寻找2023年暑期实习 ❤️\n### Open for 2023 Summer Internship ❤️\n\n<hr>\n\n本科就读于同济大学软件学院，目前硕士就读于同济大学设计与创意学院人工智能与数据设计专业 IDvX 实验室 https://idvxlab.com/。\n\n#### 公众号\n\n<img src=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" alt=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" style=\"width:300px;margin:0 auto\">\n\n#### Misc\n\n- ISFP 懒狗\n- 尼康党\n- 原批\n- 轻度二次元 (巨人, 鬼灭, JoJo...)\n- 塞尔达YYDS\n\n","source":"about/index.md","raw":"---\ntitle: ABOUT\n---\n[<img class=\"not-scale\" src=\"https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white\" alt=\"https://github.com/Abingcbc\" style=\"display:inline\">](https://github.com/Abingcbc) [<img class=\"not-scale\" src=\"https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&amp;logo=gmail&amp;logoColor=white\" alt=\"mailto:abingcbc626@gmail.com\" style=\"display:inline\">](mailto:abingcbc626@gmail.com) [<img class=\"not-scale\" src=\"https://img.shields.io/badge/linkedin-%230077B5.svg?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white\" alt=\"https://www.linkedin.com/in/bingchang-chen-8b3812183/\" style=\"display:inline\">](https://www.linkedin.com/in/bingchang-chen-8b3812183/) [<img class=\"not-scale\" src=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" alt=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" style=\"display:inline\">](https://www.zhihu.com/people/llll-48-29) [<img  class=\"not-scale\" src=\"https://img.shields.io/badge/Twitter-%231DA1F2.svg?style=for-the-badge&logo=Twitter&logoColor=white\" alt=\"https://img.shields.io/badge/Twitter-%231DA1F2.svg?style=for-the-badge&logo=Twitter&logoColor=white\" style=\"display:inline\">](https://twitter.com/abingcbc)\n<hr>\n\n### 正在寻找2023年暑期实习 ❤️\n### Open for 2023 Summer Internship ❤️\n\n<hr>\n\n本科就读于同济大学软件学院，目前硕士就读于同济大学设计与创意学院人工智能与数据设计专业 IDvX 实验室 https://idvxlab.com/。\n\n#### 公众号\n\n<img src=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" alt=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" style=\"width:300px;margin:0 auto\">\n\n#### Misc\n\n- ISFP 懒狗\n- 尼康党\n- 原批\n- 轻度二次元 (巨人, 鬼灭, JoJo...)\n- 塞尔达YYDS\n\n","date":"2022-11-20T16:46:42.025Z","updated":"2022-11-20T16:46:42.025Z","path":"about/index.html","comments":1,"layout":"page","_id":"clapldfco000018mz61b43dgl","content":"<p><a href=\"https://github.com/Abingcbc\"><img class=\"not-scale\" src=\"https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white\" alt=\"https://github.com/Abingcbc\" style=\"display:inline\"></a> <a href=\"mailto:abingcbc626@gmail.com\"><img class=\"not-scale\" src=\"https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&amp;logo=gmail&amp;logoColor=white\" alt=\"mailto:abingcbc626@gmail.com\" style=\"display:inline\"></a> <a href=\"https://www.linkedin.com/in/bingchang-chen-8b3812183/\"><img class=\"not-scale\" src=\"https://img.shields.io/badge/linkedin-%230077B5.svg?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white\" alt=\"https://www.linkedin.com/in/bingchang-chen-8b3812183/\" style=\"display:inline\"></a> <a href=\"https://www.zhihu.com/people/llll-48-29\"><img class=\"not-scale\" src=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" alt=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" style=\"display:inline\"></a> <a href=\"https://twitter.com/abingcbc\"><img  class=\"not-scale\" src=\"https://img.shields.io/badge/Twitter-%231DA1F2.svg?style=for-the-badge&logo=Twitter&logoColor=white\" alt=\"https://img.shields.io/badge/Twitter-%231DA1F2.svg?style=for-the-badge&logo=Twitter&logoColor=white\" style=\"display:inline\"></a></p>\n<hr>\n<h3 id=\"正在寻找2023年暑期实习-️\"><a class=\"markdownIt-Anchor\" href=\"#正在寻找2023年暑期实习-️\"></a> 正在寻找2023年暑期实习 ❤️</h3>\n<h3 id=\"open-for-2023-summer-internship-️\"><a class=\"markdownIt-Anchor\" href=\"#open-for-2023-summer-internship-️\"></a> Open for 2023 Summer Internship ❤️</h3>\n<hr>\n<p>本科就读于同济大学软件学院，目前硕士就读于同济大学设计与创意学院人工智能与数据设计专业 IDvX 实验室 <a href=\"https://idvxlab.com/%E3%80%82\">https://idvxlab.com/。</a></p>\n<h4 id=\"公众号\"><a class=\"markdownIt-Anchor\" href=\"#公众号\"></a> 公众号</h4>\n<img src=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" alt=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" style=\"width:300px;margin:0 auto\">\n<h4 id=\"misc\"><a class=\"markdownIt-Anchor\" href=\"#misc\"></a> Misc</h4>\n<ul>\n<li>ISFP 懒狗</li>\n<li>尼康党</li>\n<li>原批</li>\n<li>轻度二次元 (巨人, 鬼灭, JoJo…)</li>\n<li>塞尔达YYDS</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://github.com/Abingcbc\"><img class=\"not-scale\" src=\"https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white\" alt=\"https://github.com/Abingcbc\" style=\"display:inline\"></a> <a href=\"mailto:abingcbc626@gmail.com\"><img class=\"not-scale\" src=\"https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&amp;logo=gmail&amp;logoColor=white\" alt=\"mailto:abingcbc626@gmail.com\" style=\"display:inline\"></a> <a href=\"https://www.linkedin.com/in/bingchang-chen-8b3812183/\"><img class=\"not-scale\" src=\"https://img.shields.io/badge/linkedin-%230077B5.svg?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white\" alt=\"https://www.linkedin.com/in/bingchang-chen-8b3812183/\" style=\"display:inline\"></a> <a href=\"https://www.zhihu.com/people/llll-48-29\"><img class=\"not-scale\" src=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" alt=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" style=\"display:inline\"></a> <a href=\"https://twitter.com/abingcbc\"><img  class=\"not-scale\" src=\"https://img.shields.io/badge/Twitter-%231DA1F2.svg?style=for-the-badge&logo=Twitter&logoColor=white\" alt=\"https://img.shields.io/badge/Twitter-%231DA1F2.svg?style=for-the-badge&logo=Twitter&logoColor=white\" style=\"display:inline\"></a></p>\n<hr>\n<h3 id=\"正在寻找2023年暑期实习-️\"><a class=\"markdownIt-Anchor\" href=\"#正在寻找2023年暑期实习-️\"></a> 正在寻找2023年暑期实习 ❤️</h3>\n<h3 id=\"open-for-2023-summer-internship-️\"><a class=\"markdownIt-Anchor\" href=\"#open-for-2023-summer-internship-️\"></a> Open for 2023 Summer Internship ❤️</h3>\n<hr>\n<p>本科就读于同济大学软件学院，目前硕士就读于同济大学设计与创意学院人工智能与数据设计专业 IDvX 实验室 <a href=\"https://idvxlab.com/%E3%80%82\">https://idvxlab.com/。</a></p>\n<h4 id=\"公众号\"><a class=\"markdownIt-Anchor\" href=\"#公众号\"></a> 公众号</h4>\n<img src=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" alt=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" style=\"width:300px;margin:0 auto\">\n<h4 id=\"misc\"><a class=\"markdownIt-Anchor\" href=\"#misc\"></a> Misc</h4>\n<ul>\n<li>ISFP 懒狗</li>\n<li>尼康党</li>\n<li>原批</li>\n<li>轻度二次元 (巨人, 鬼灭, JoJo…)</li>\n<li>塞尔达YYDS</li>\n</ul>\n"},{"layout":"weekly","title":"Weekly Report","_content":"","source":"weekly/index.md","raw":"---\nlayout: weekly\ntitle: Weekly Report\n---","date":"2022-11-20T16:46:42.065Z","updated":"2022-11-20T16:46:42.065Z","path":"weekly/index.html","comments":1,"_id":"clapldfcw000218mz69av5fv5","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"2021 年终总结 —— 翻天覆地","date":"2022-06-01T00:19:16.000Z","updated":"2022-06-01T00:19:16.000Z","comments":0,"_content":"2021年这一年的时间里，发生了太多的事，如果不写下来的话，在脑海中只会有一些模糊的印象，不清楚自己取得了哪些成绩，又有哪里不尽人意，同时也可以从一个旁观者的视角，来观察自己这一年有没有虚度光阴。所以，就从2021年开始，开始每年对过去的一年进行一个记录吧~\n## 技术\n### K8s\n在年初的时候，阅读了《Kubernetes in Action》这本书，受益匪浅，再加上实习中的实践，对 K8s 的了解更加深入了。K8s 作为目前 Cloud 的实际代名词，在应用开发中已经是无法替代的存在了。作为云原生的基础设施，对于 K8s 的学习，除了掌握其基础的操作，例如 Deployment、Ingress 等等组件的使用，来满足上层的应用开发以外，对于 K8s 本身的学习，也是十分重要的。因为 K8s 本身就是一个庞大复杂的系统，而且经过无数业界大佬的打磨，系统上的设计无疑是很多问题的 best practice。深入理解 K8s 的设计，对于未来设计自己的应用架构肯定会带来很大的帮助。\n### 自动化部署\n我在实习期间负责的主要的一部分内容就是自动化部署。随着现在微服务化的趋势以及系统内第三方依赖组件的数量的增加，部署其实是一个非常麻烦的问题。在公有云的场景下，部分第三方组件或许可以通过外部服务的形式解决。但在私有云的情况下，部署需要同时部署这些第三方组件，无疑给部署增加了很大的复杂度。不仅仅如此，对于任何现代化的软件来说，部署都是需要进行标准化的。通过部署文档，类似手工作坊进行的部署方式，是无法持久的，人因的错误是整个部署过程中很大的不稳定因素。这方面也有很多做的比较好的开源项目，比如说 TiDB 的 TiUP 等等。\n### 可视化\n实话说，在正式进入硕士学习之前，我对于可视化的了解可能都比较浅薄（虽然明知道这是自己硕士的研究方向）。在听完大老板的课之后，对于这个领域有了全新的认知。可视化这个领域虽然很小，但并不是像大众认为的只有简单的图表而已。首先，图表本身的设计有着很多问题值得研究。设计可能是一个主观感性的行为，但我大老板的想法是通过design space等方式，将其转化为理性，有逻辑的行为。我对这一点非常认同，给了我一种茅塞顿开的感觉，让我对之前很多看似感性的事情有了新的理解。其次，可视分析也是可视化中非常重要的一部分。很多问题，通过可视化的形式，就可以很清楚和容易地被分析出来。这一类系统，感觉在 BI 领域应该有很大的发展空间。\n### Casbin\n一直以来想加入一个开源社区，为 developer 们做出一点点自己的贡献。于是通过一个活动加入到了 Casbin社区。Casbin 是一个权限校验框架，通过 well-defined 的 model 结构，支持对各种类型的校验模式（比如RBAC，ABAC等等）。除此之外，我觉得 Casbin 能够获得 10k+ star 的另一个原因是他的生态支持也太丰富了。各种语言，各种前端后端框架，各种数据库，只要是有需求，都会进行支持。其中另一个最重要的应该是 Casdoor，一个第三方统一身份认证框架，与 KeyCloak 相对标。\n\n这两个项目都非常有意义，Casbin 主要是像一个解析器一样，面对不同的 model 和 policy定义，需要提供正确的校验结果，我个人还挺喜欢慢慢推理，寻找哪一步解析错误的过程。而 Casdoor 则像一个业务系统，如何实现更多的 feature，如何提升易用性，与更多的第三方系统进行集成。\n### Other\n除此之前，这一年间也多多少少接触了很多其他技术。实习的时候无所事事，开始写起来了 Augular，体验了一次像 Java 一样的前端的开发模式；暑假的时候参加 GSoC，借机接触了一下 Rust，对这门语言产生了深深的敬畏，但可惜没能深入学习下去；开学以后接触了很多设计领域的coding，做了一些数字媒体类似的课程作业......\n## 生活\n上半年实习的过程中，感觉自己逐渐放开了。在实习期间遇到了很多伙伴，大家一起快乐摸鱼，真的是度过了一段很快乐的时光。最重要的是毕业啦！最高学历终于从高中变成了本科。虽然毕业旅行因为疫情原因没能成行，但暑假在家快乐摸鱼，疯狂锻炼猎龙技术，还差一点把塞尔达的呀哈哈全收集，也算快乐的度过了最后一个暑假了。\n\n如果说上半年是我人生最快乐的时光，那么下半年开学后，就是我目前为止最痛苦的日子了。但无论是前后哪个阶段，我其实都成长了非常多，也算是值得庆幸的事，能够在接受到社会正式的毒打之前，提前成长。\n\n一方面的压力来自于课程，毕竟我是属于跨专业保研到了设创，难免在研究生阶段要接触到设计相关的课程。在组队上，天真地以为老师是技术背景出身的，项目可能也会是，结果就是在课程项目上被设计背景的同学吊打。在 DDL 前疯狂爆肝，但是也换不来特别高的成绩。研究生课程中没能抱上设计大佬们的大腿，没体验一次被带飞的感觉，算是比较可惜的了。\n\n另一方面最大的压力来自于实验室。大老板的痛骂确实是有效的。做事不够深入的问题，在我之前的面试以及实习过程中，都多次被前辈们提到过，但都采用相对和蔼的语气和态度。我也是属实有点抖m了，这种不痛不痒的批评根本不往心里去，也没有发自内心地去纠正自己的问题，非要等到现在的大老板爆骂自己，自己才意识到问题。。。\n\n年末到22年初的状态确实不太好，似乎每年的这段时间都很emo。在年底DDL结束之后，自己的节奏一时间也没调整过来。闲下来以后，脑子昏昏沉沉，没了目标，搞不清楚自己真正想要什么，不讨人喜欢。最后也算造成了不可弥补的错误吧，非常可惜。不过，人各有命吧，也学习到了很多，前后相比，算是成熟了很多了。\n\n拖拖拉拉，写完这篇总结的时候，2022年都已经过去四分之一了。总的来说，2022年可以用“翻天覆地”这个词来概括，在这个痛苦的过程中，也见到了一个不一样的自己。虽然2022年的开端不算顺利，很失败，但这些问题就留到明年的总结中来写吧。最后，希望在2023年总结今年的时候，可以用一个更加开心的关键词吧。","source":"_posts/2021.md","raw":"---\ntitle: 2021 年终总结 —— 翻天覆地\ndate: 2022-06-01 00:19:16\nupdated: 2022-06-01 00:19:16\ncomments: false\ntags: \n  - weekly\n  - 2021\n---\n2021年这一年的时间里，发生了太多的事，如果不写下来的话，在脑海中只会有一些模糊的印象，不清楚自己取得了哪些成绩，又有哪里不尽人意，同时也可以从一个旁观者的视角，来观察自己这一年有没有虚度光阴。所以，就从2021年开始，开始每年对过去的一年进行一个记录吧~\n## 技术\n### K8s\n在年初的时候，阅读了《Kubernetes in Action》这本书，受益匪浅，再加上实习中的实践，对 K8s 的了解更加深入了。K8s 作为目前 Cloud 的实际代名词，在应用开发中已经是无法替代的存在了。作为云原生的基础设施，对于 K8s 的学习，除了掌握其基础的操作，例如 Deployment、Ingress 等等组件的使用，来满足上层的应用开发以外，对于 K8s 本身的学习，也是十分重要的。因为 K8s 本身就是一个庞大复杂的系统，而且经过无数业界大佬的打磨，系统上的设计无疑是很多问题的 best practice。深入理解 K8s 的设计，对于未来设计自己的应用架构肯定会带来很大的帮助。\n### 自动化部署\n我在实习期间负责的主要的一部分内容就是自动化部署。随着现在微服务化的趋势以及系统内第三方依赖组件的数量的增加，部署其实是一个非常麻烦的问题。在公有云的场景下，部分第三方组件或许可以通过外部服务的形式解决。但在私有云的情况下，部署需要同时部署这些第三方组件，无疑给部署增加了很大的复杂度。不仅仅如此，对于任何现代化的软件来说，部署都是需要进行标准化的。通过部署文档，类似手工作坊进行的部署方式，是无法持久的，人因的错误是整个部署过程中很大的不稳定因素。这方面也有很多做的比较好的开源项目，比如说 TiDB 的 TiUP 等等。\n### 可视化\n实话说，在正式进入硕士学习之前，我对于可视化的了解可能都比较浅薄（虽然明知道这是自己硕士的研究方向）。在听完大老板的课之后，对于这个领域有了全新的认知。可视化这个领域虽然很小，但并不是像大众认为的只有简单的图表而已。首先，图表本身的设计有着很多问题值得研究。设计可能是一个主观感性的行为，但我大老板的想法是通过design space等方式，将其转化为理性，有逻辑的行为。我对这一点非常认同，给了我一种茅塞顿开的感觉，让我对之前很多看似感性的事情有了新的理解。其次，可视分析也是可视化中非常重要的一部分。很多问题，通过可视化的形式，就可以很清楚和容易地被分析出来。这一类系统，感觉在 BI 领域应该有很大的发展空间。\n### Casbin\n一直以来想加入一个开源社区，为 developer 们做出一点点自己的贡献。于是通过一个活动加入到了 Casbin社区。Casbin 是一个权限校验框架，通过 well-defined 的 model 结构，支持对各种类型的校验模式（比如RBAC，ABAC等等）。除此之外，我觉得 Casbin 能够获得 10k+ star 的另一个原因是他的生态支持也太丰富了。各种语言，各种前端后端框架，各种数据库，只要是有需求，都会进行支持。其中另一个最重要的应该是 Casdoor，一个第三方统一身份认证框架，与 KeyCloak 相对标。\n\n这两个项目都非常有意义，Casbin 主要是像一个解析器一样，面对不同的 model 和 policy定义，需要提供正确的校验结果，我个人还挺喜欢慢慢推理，寻找哪一步解析错误的过程。而 Casdoor 则像一个业务系统，如何实现更多的 feature，如何提升易用性，与更多的第三方系统进行集成。\n### Other\n除此之前，这一年间也多多少少接触了很多其他技术。实习的时候无所事事，开始写起来了 Augular，体验了一次像 Java 一样的前端的开发模式；暑假的时候参加 GSoC，借机接触了一下 Rust，对这门语言产生了深深的敬畏，但可惜没能深入学习下去；开学以后接触了很多设计领域的coding，做了一些数字媒体类似的课程作业......\n## 生活\n上半年实习的过程中，感觉自己逐渐放开了。在实习期间遇到了很多伙伴，大家一起快乐摸鱼，真的是度过了一段很快乐的时光。最重要的是毕业啦！最高学历终于从高中变成了本科。虽然毕业旅行因为疫情原因没能成行，但暑假在家快乐摸鱼，疯狂锻炼猎龙技术，还差一点把塞尔达的呀哈哈全收集，也算快乐的度过了最后一个暑假了。\n\n如果说上半年是我人生最快乐的时光，那么下半年开学后，就是我目前为止最痛苦的日子了。但无论是前后哪个阶段，我其实都成长了非常多，也算是值得庆幸的事，能够在接受到社会正式的毒打之前，提前成长。\n\n一方面的压力来自于课程，毕竟我是属于跨专业保研到了设创，难免在研究生阶段要接触到设计相关的课程。在组队上，天真地以为老师是技术背景出身的，项目可能也会是，结果就是在课程项目上被设计背景的同学吊打。在 DDL 前疯狂爆肝，但是也换不来特别高的成绩。研究生课程中没能抱上设计大佬们的大腿，没体验一次被带飞的感觉，算是比较可惜的了。\n\n另一方面最大的压力来自于实验室。大老板的痛骂确实是有效的。做事不够深入的问题，在我之前的面试以及实习过程中，都多次被前辈们提到过，但都采用相对和蔼的语气和态度。我也是属实有点抖m了，这种不痛不痒的批评根本不往心里去，也没有发自内心地去纠正自己的问题，非要等到现在的大老板爆骂自己，自己才意识到问题。。。\n\n年末到22年初的状态确实不太好，似乎每年的这段时间都很emo。在年底DDL结束之后，自己的节奏一时间也没调整过来。闲下来以后，脑子昏昏沉沉，没了目标，搞不清楚自己真正想要什么，不讨人喜欢。最后也算造成了不可弥补的错误吧，非常可惜。不过，人各有命吧，也学习到了很多，前后相比，算是成熟了很多了。\n\n拖拖拉拉，写完这篇总结的时候，2022年都已经过去四分之一了。总的来说，2022年可以用“翻天覆地”这个词来概括，在这个痛苦的过程中，也见到了一个不一样的自己。虽然2022年的开端不算顺利，很失败，但这些问题就留到明年的总结中来写吧。最后，希望在2023年总结今年的时候，可以用一个更加开心的关键词吧。","slug":"2021","published":1,"layout":"post","photos":[],"link":"","_id":"clapldfcs000118mzf5909wet","content":"<p>2021年这一年的时间里，发生了太多的事，如果不写下来的话，在脑海中只会有一些模糊的印象，不清楚自己取得了哪些成绩，又有哪里不尽人意，同时也可以从一个旁观者的视角，来观察自己这一年有没有虚度光阴。所以，就从2021年开始，开始每年对过去的一年进行一个记录吧~</p>\n<h2 id=\"技术\"><a class=\"markdownIt-Anchor\" href=\"#技术\"></a> 技术</h2>\n<h3 id=\"k8s\"><a class=\"markdownIt-Anchor\" href=\"#k8s\"></a> K8s</h3>\n<p>在年初的时候，阅读了《Kubernetes in Action》这本书，受益匪浅，再加上实习中的实践，对 K8s 的了解更加深入了。K8s 作为目前 Cloud 的实际代名词，在应用开发中已经是无法替代的存在了。作为云原生的基础设施，对于 K8s 的学习，除了掌握其基础的操作，例如 Deployment、Ingress 等等组件的使用，来满足上层的应用开发以外，对于 K8s 本身的学习，也是十分重要的。因为 K8s 本身就是一个庞大复杂的系统，而且经过无数业界大佬的打磨，系统上的设计无疑是很多问题的 best practice。深入理解 K8s 的设计，对于未来设计自己的应用架构肯定会带来很大的帮助。</p>\n<h3 id=\"自动化部署\"><a class=\"markdownIt-Anchor\" href=\"#自动化部署\"></a> 自动化部署</h3>\n<p>我在实习期间负责的主要的一部分内容就是自动化部署。随着现在微服务化的趋势以及系统内第三方依赖组件的数量的增加，部署其实是一个非常麻烦的问题。在公有云的场景下，部分第三方组件或许可以通过外部服务的形式解决。但在私有云的情况下，部署需要同时部署这些第三方组件，无疑给部署增加了很大的复杂度。不仅仅如此，对于任何现代化的软件来说，部署都是需要进行标准化的。通过部署文档，类似手工作坊进行的部署方式，是无法持久的，人因的错误是整个部署过程中很大的不稳定因素。这方面也有很多做的比较好的开源项目，比如说 TiDB 的 TiUP 等等。</p>\n<h3 id=\"可视化\"><a class=\"markdownIt-Anchor\" href=\"#可视化\"></a> 可视化</h3>\n<p>实话说，在正式进入硕士学习之前，我对于可视化的了解可能都比较浅薄（虽然明知道这是自己硕士的研究方向）。在听完大老板的课之后，对于这个领域有了全新的认知。可视化这个领域虽然很小，但并不是像大众认为的只有简单的图表而已。首先，图表本身的设计有着很多问题值得研究。设计可能是一个主观感性的行为，但我大老板的想法是通过design space等方式，将其转化为理性，有逻辑的行为。我对这一点非常认同，给了我一种茅塞顿开的感觉，让我对之前很多看似感性的事情有了新的理解。其次，可视分析也是可视化中非常重要的一部分。很多问题，通过可视化的形式，就可以很清楚和容易地被分析出来。这一类系统，感觉在 BI 领域应该有很大的发展空间。</p>\n<h3 id=\"casbin\"><a class=\"markdownIt-Anchor\" href=\"#casbin\"></a> Casbin</h3>\n<p>一直以来想加入一个开源社区，为 developer 们做出一点点自己的贡献。于是通过一个活动加入到了 Casbin社区。Casbin 是一个权限校验框架，通过 well-defined 的 model 结构，支持对各种类型的校验模式（比如RBAC，ABAC等等）。除此之外，我觉得 Casbin 能够获得 10k+ star 的另一个原因是他的生态支持也太丰富了。各种语言，各种前端后端框架，各种数据库，只要是有需求，都会进行支持。其中另一个最重要的应该是 Casdoor，一个第三方统一身份认证框架，与 KeyCloak 相对标。</p>\n<p>这两个项目都非常有意义，Casbin 主要是像一个解析器一样，面对不同的 model 和 policy定义，需要提供正确的校验结果，我个人还挺喜欢慢慢推理，寻找哪一步解析错误的过程。而 Casdoor 则像一个业务系统，如何实现更多的 feature，如何提升易用性，与更多的第三方系统进行集成。</p>\n<h3 id=\"other\"><a class=\"markdownIt-Anchor\" href=\"#other\"></a> Other</h3>\n<p>除此之前，这一年间也多多少少接触了很多其他技术。实习的时候无所事事，开始写起来了 Augular，体验了一次像 Java 一样的前端的开发模式；暑假的时候参加 GSoC，借机接触了一下 Rust，对这门语言产生了深深的敬畏，但可惜没能深入学习下去；开学以后接触了很多设计领域的coding，做了一些数字媒体类似的课程作业…</p>\n<h2 id=\"生活\"><a class=\"markdownIt-Anchor\" href=\"#生活\"></a> 生活</h2>\n<p>上半年实习的过程中，感觉自己逐渐放开了。在实习期间遇到了很多伙伴，大家一起快乐摸鱼，真的是度过了一段很快乐的时光。最重要的是毕业啦！最高学历终于从高中变成了本科。虽然毕业旅行因为疫情原因没能成行，但暑假在家快乐摸鱼，疯狂锻炼猎龙技术，还差一点把塞尔达的呀哈哈全收集，也算快乐的度过了最后一个暑假了。</p>\n<p>如果说上半年是我人生最快乐的时光，那么下半年开学后，就是我目前为止最痛苦的日子了。但无论是前后哪个阶段，我其实都成长了非常多，也算是值得庆幸的事，能够在接受到社会正式的毒打之前，提前成长。</p>\n<p>一方面的压力来自于课程，毕竟我是属于跨专业保研到了设创，难免在研究生阶段要接触到设计相关的课程。在组队上，天真地以为老师是技术背景出身的，项目可能也会是，结果就是在课程项目上被设计背景的同学吊打。在 DDL 前疯狂爆肝，但是也换不来特别高的成绩。研究生课程中没能抱上设计大佬们的大腿，没体验一次被带飞的感觉，算是比较可惜的了。</p>\n<p>另一方面最大的压力来自于实验室。大老板的痛骂确实是有效的。做事不够深入的问题，在我之前的面试以及实习过程中，都多次被前辈们提到过，但都采用相对和蔼的语气和态度。我也是属实有点抖m了，这种不痛不痒的批评根本不往心里去，也没有发自内心地去纠正自己的问题，非要等到现在的大老板爆骂自己，自己才意识到问题。。。</p>\n<p>年末到22年初的状态确实不太好，似乎每年的这段时间都很emo。在年底DDL结束之后，自己的节奏一时间也没调整过来。闲下来以后，脑子昏昏沉沉，没了目标，搞不清楚自己真正想要什么，不讨人喜欢。最后也算造成了不可弥补的错误吧，非常可惜。不过，人各有命吧，也学习到了很多，前后相比，算是成熟了很多了。</p>\n<p>拖拖拉拉，写完这篇总结的时候，2022年都已经过去四分之一了。总的来说，2022年可以用“翻天覆地”这个词来概括，在这个痛苦的过程中，也见到了一个不一样的自己。虽然2022年的开端不算顺利，很失败，但这些问题就留到明年的总结中来写吧。最后，希望在2023年总结今年的时候，可以用一个更加开心的关键词吧。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>2021年这一年的时间里，发生了太多的事，如果不写下来的话，在脑海中只会有一些模糊的印象，不清楚自己取得了哪些成绩，又有哪里不尽人意，同时也可以从一个旁观者的视角，来观察自己这一年有没有虚度光阴。所以，就从2021年开始，开始每年对过去的一年进行一个记录吧~</p>\n<h2 id=\"技术\"><a class=\"markdownIt-Anchor\" href=\"#技术\"></a> 技术</h2>\n<h3 id=\"k8s\"><a class=\"markdownIt-Anchor\" href=\"#k8s\"></a> K8s</h3>\n<p>在年初的时候，阅读了《Kubernetes in Action》这本书，受益匪浅，再加上实习中的实践，对 K8s 的了解更加深入了。K8s 作为目前 Cloud 的实际代名词，在应用开发中已经是无法替代的存在了。作为云原生的基础设施，对于 K8s 的学习，除了掌握其基础的操作，例如 Deployment、Ingress 等等组件的使用，来满足上层的应用开发以外，对于 K8s 本身的学习，也是十分重要的。因为 K8s 本身就是一个庞大复杂的系统，而且经过无数业界大佬的打磨，系统上的设计无疑是很多问题的 best practice。深入理解 K8s 的设计，对于未来设计自己的应用架构肯定会带来很大的帮助。</p>\n<h3 id=\"自动化部署\"><a class=\"markdownIt-Anchor\" href=\"#自动化部署\"></a> 自动化部署</h3>\n<p>我在实习期间负责的主要的一部分内容就是自动化部署。随着现在微服务化的趋势以及系统内第三方依赖组件的数量的增加，部署其实是一个非常麻烦的问题。在公有云的场景下，部分第三方组件或许可以通过外部服务的形式解决。但在私有云的情况下，部署需要同时部署这些第三方组件，无疑给部署增加了很大的复杂度。不仅仅如此，对于任何现代化的软件来说，部署都是需要进行标准化的。通过部署文档，类似手工作坊进行的部署方式，是无法持久的，人因的错误是整个部署过程中很大的不稳定因素。这方面也有很多做的比较好的开源项目，比如说 TiDB 的 TiUP 等等。</p>\n<h3 id=\"可视化\"><a class=\"markdownIt-Anchor\" href=\"#可视化\"></a> 可视化</h3>\n<p>实话说，在正式进入硕士学习之前，我对于可视化的了解可能都比较浅薄（虽然明知道这是自己硕士的研究方向）。在听完大老板的课之后，对于这个领域有了全新的认知。可视化这个领域虽然很小，但并不是像大众认为的只有简单的图表而已。首先，图表本身的设计有着很多问题值得研究。设计可能是一个主观感性的行为，但我大老板的想法是通过design space等方式，将其转化为理性，有逻辑的行为。我对这一点非常认同，给了我一种茅塞顿开的感觉，让我对之前很多看似感性的事情有了新的理解。其次，可视分析也是可视化中非常重要的一部分。很多问题，通过可视化的形式，就可以很清楚和容易地被分析出来。这一类系统，感觉在 BI 领域应该有很大的发展空间。</p>\n<h3 id=\"casbin\"><a class=\"markdownIt-Anchor\" href=\"#casbin\"></a> Casbin</h3>\n<p>一直以来想加入一个开源社区，为 developer 们做出一点点自己的贡献。于是通过一个活动加入到了 Casbin社区。Casbin 是一个权限校验框架，通过 well-defined 的 model 结构，支持对各种类型的校验模式（比如RBAC，ABAC等等）。除此之外，我觉得 Casbin 能够获得 10k+ star 的另一个原因是他的生态支持也太丰富了。各种语言，各种前端后端框架，各种数据库，只要是有需求，都会进行支持。其中另一个最重要的应该是 Casdoor，一个第三方统一身份认证框架，与 KeyCloak 相对标。</p>\n<p>这两个项目都非常有意义，Casbin 主要是像一个解析器一样，面对不同的 model 和 policy定义，需要提供正确的校验结果，我个人还挺喜欢慢慢推理，寻找哪一步解析错误的过程。而 Casdoor 则像一个业务系统，如何实现更多的 feature，如何提升易用性，与更多的第三方系统进行集成。</p>\n<h3 id=\"other\"><a class=\"markdownIt-Anchor\" href=\"#other\"></a> Other</h3>\n<p>除此之前，这一年间也多多少少接触了很多其他技术。实习的时候无所事事，开始写起来了 Augular，体验了一次像 Java 一样的前端的开发模式；暑假的时候参加 GSoC，借机接触了一下 Rust，对这门语言产生了深深的敬畏，但可惜没能深入学习下去；开学以后接触了很多设计领域的coding，做了一些数字媒体类似的课程作业…</p>\n<h2 id=\"生活\"><a class=\"markdownIt-Anchor\" href=\"#生活\"></a> 生活</h2>\n<p>上半年实习的过程中，感觉自己逐渐放开了。在实习期间遇到了很多伙伴，大家一起快乐摸鱼，真的是度过了一段很快乐的时光。最重要的是毕业啦！最高学历终于从高中变成了本科。虽然毕业旅行因为疫情原因没能成行，但暑假在家快乐摸鱼，疯狂锻炼猎龙技术，还差一点把塞尔达的呀哈哈全收集，也算快乐的度过了最后一个暑假了。</p>\n<p>如果说上半年是我人生最快乐的时光，那么下半年开学后，就是我目前为止最痛苦的日子了。但无论是前后哪个阶段，我其实都成长了非常多，也算是值得庆幸的事，能够在接受到社会正式的毒打之前，提前成长。</p>\n<p>一方面的压力来自于课程，毕竟我是属于跨专业保研到了设创，难免在研究生阶段要接触到设计相关的课程。在组队上，天真地以为老师是技术背景出身的，项目可能也会是，结果就是在课程项目上被设计背景的同学吊打。在 DDL 前疯狂爆肝，但是也换不来特别高的成绩。研究生课程中没能抱上设计大佬们的大腿，没体验一次被带飞的感觉，算是比较可惜的了。</p>\n<p>另一方面最大的压力来自于实验室。大老板的痛骂确实是有效的。做事不够深入的问题，在我之前的面试以及实习过程中，都多次被前辈们提到过，但都采用相对和蔼的语气和态度。我也是属实有点抖m了，这种不痛不痒的批评根本不往心里去，也没有发自内心地去纠正自己的问题，非要等到现在的大老板爆骂自己，自己才意识到问题。。。</p>\n<p>年末到22年初的状态确实不太好，似乎每年的这段时间都很emo。在年底DDL结束之后，自己的节奏一时间也没调整过来。闲下来以后，脑子昏昏沉沉，没了目标，搞不清楚自己真正想要什么，不讨人喜欢。最后也算造成了不可弥补的错误吧，非常可惜。不过，人各有命吧，也学习到了很多，前后相比，算是成熟了很多了。</p>\n<p>拖拖拉拉，写完这篇总结的时候，2022年都已经过去四分之一了。总的来说，2022年可以用“翻天覆地”这个词来概括，在这个痛苦的过程中，也见到了一个不一样的自己。虽然2022年的开端不算顺利，很失败，但这些问题就留到明年的总结中来写吧。最后，希望在2023年总结今年的时候，可以用一个更加开心的关键词吧。</p>\n"},{"title":"被DDL追杀","date":"2022-11-06T23:00:00.000Z","updated":"2022-11-06T23:00:00.000Z","comments":0,"_content":"**2022.10.30 - 2022.11.06**\n看到很多大佬在写周报，把日子过得特别明白，所以从2022年11月的第一个星期开始，尝试记录一下。\n\n## 紧迫感\n由于上周末的放浪，导致这周五和导师的周会的汇报内容毫无进展。到了周一着手开始做才发现，内容比想象中的工作量要大的多。接下来的三天，变成了廉价的数据工人，一个一个处理数据。还好紧赶慢赶在周会前，把schedule上的工作都做了差不多。也不记得什么时候开始（或许是大三或者大四），已经很久没有这种要做不完的紧迫感了。但这周明显感觉到了自己有所不同，心里紧张的同时，还多了一份从容吧（竟然还第一时间把原神更新的主线打完了...），不会因为这种紧迫打乱了自己的节奏。\n\n## 拖延症\n在狂赶 DDL 的过程中，也一直在给自己定一系列的小目标，比如今晚收集完所有数据集就睡觉。然而，现实却是收集了一两份之后，就开始玩手机，然后就...在没完成小目标的情况下去睡觉了。之前也一直吐槽自己这个问题，从来没有按照导师的schedule 完成过，导致到 DDL 交出的结果往往差强人意。一定要改正！\n\n## 太菜辣\n上周被开源社区的大佬疯狂 diss 代码，这周又被接着 diss。真的是自己太菜了，主要的问题可能有以下几点吧\n1. 不同模块之间的依赖没有梳理清楚，高耦合\n2. 要先对齐需求再去做，比如，想要的是一个通用的服务，那就不能把一些默认值给写死\n3. Polish! Polish Again! 写完的代码多想想能不能优化逻辑\n\n## 焦虑\n作为一个摆烂 ISFP 人，还是感觉自己无时无刻不在焦虑。最近一直在为两件事焦虑，第一件是找工作。鉴于今年秋招的惨况，真的对明年的行情不敢抱有特别好的期望，所以还是想尽早开始做准备。之前计划每天一题，但由于上周赶 DDL 而搁置了。一定要捡回来，找工作应该是自己未来一年中最重要的事了，毕竟是应届生的第一份工作。第二件事是社交。一转眼，本科时候一起孤寡的同学都已经在莺莺燕燕了，而自己还是一个人在宿舍里发烂发臭...太失败了\n","source":"_posts/2022-00.md","raw":"---\ntitle: 被DDL追杀\ndate: 2022-11-06 23:00:00\nupdated: 2022-11-06 23:00:00\ncomments: false\ntags: \n  - weekly\n  - 2022-00\n---\n**2022.10.30 - 2022.11.06**\n看到很多大佬在写周报，把日子过得特别明白，所以从2022年11月的第一个星期开始，尝试记录一下。\n\n## 紧迫感\n由于上周末的放浪，导致这周五和导师的周会的汇报内容毫无进展。到了周一着手开始做才发现，内容比想象中的工作量要大的多。接下来的三天，变成了廉价的数据工人，一个一个处理数据。还好紧赶慢赶在周会前，把schedule上的工作都做了差不多。也不记得什么时候开始（或许是大三或者大四），已经很久没有这种要做不完的紧迫感了。但这周明显感觉到了自己有所不同，心里紧张的同时，还多了一份从容吧（竟然还第一时间把原神更新的主线打完了...），不会因为这种紧迫打乱了自己的节奏。\n\n## 拖延症\n在狂赶 DDL 的过程中，也一直在给自己定一系列的小目标，比如今晚收集完所有数据集就睡觉。然而，现实却是收集了一两份之后，就开始玩手机，然后就...在没完成小目标的情况下去睡觉了。之前也一直吐槽自己这个问题，从来没有按照导师的schedule 完成过，导致到 DDL 交出的结果往往差强人意。一定要改正！\n\n## 太菜辣\n上周被开源社区的大佬疯狂 diss 代码，这周又被接着 diss。真的是自己太菜了，主要的问题可能有以下几点吧\n1. 不同模块之间的依赖没有梳理清楚，高耦合\n2. 要先对齐需求再去做，比如，想要的是一个通用的服务，那就不能把一些默认值给写死\n3. Polish! Polish Again! 写完的代码多想想能不能优化逻辑\n\n## 焦虑\n作为一个摆烂 ISFP 人，还是感觉自己无时无刻不在焦虑。最近一直在为两件事焦虑，第一件是找工作。鉴于今年秋招的惨况，真的对明年的行情不敢抱有特别好的期望，所以还是想尽早开始做准备。之前计划每天一题，但由于上周赶 DDL 而搁置了。一定要捡回来，找工作应该是自己未来一年中最重要的事了，毕竟是应届生的第一份工作。第二件事是社交。一转眼，本科时候一起孤寡的同学都已经在莺莺燕燕了，而自己还是一个人在宿舍里发烂发臭...太失败了\n","slug":"2022-00","published":1,"layout":"post","photos":[],"link":"","_id":"clapldfcx000318mz6oi88rwv","content":"<p><strong>2022.10.30 - 2022.11.06</strong><br />\n看到很多大佬在写周报，把日子过得特别明白，所以从2022年11月的第一个星期开始，尝试记录一下。</p>\n<h2 id=\"紧迫感\"><a class=\"markdownIt-Anchor\" href=\"#紧迫感\"></a> 紧迫感</h2>\n<p>由于上周末的放浪，导致这周五和导师的周会的汇报内容毫无进展。到了周一着手开始做才发现，内容比想象中的工作量要大的多。接下来的三天，变成了廉价的数据工人，一个一个处理数据。还好紧赶慢赶在周会前，把schedule上的工作都做了差不多。也不记得什么时候开始（或许是大三或者大四），已经很久没有这种要做不完的紧迫感了。但这周明显感觉到了自己有所不同，心里紧张的同时，还多了一份从容吧（竟然还第一时间把原神更新的主线打完了…），不会因为这种紧迫打乱了自己的节奏。</p>\n<h2 id=\"拖延症\"><a class=\"markdownIt-Anchor\" href=\"#拖延症\"></a> 拖延症</h2>\n<p>在狂赶 DDL 的过程中，也一直在给自己定一系列的小目标，比如今晚收集完所有数据集就睡觉。然而，现实却是收集了一两份之后，就开始玩手机，然后就…在没完成小目标的情况下去睡觉了。之前也一直吐槽自己这个问题，从来没有按照导师的schedule 完成过，导致到 DDL 交出的结果往往差强人意。一定要改正！</p>\n<h2 id=\"太菜辣\"><a class=\"markdownIt-Anchor\" href=\"#太菜辣\"></a> 太菜辣</h2>\n<p>上周被开源社区的大佬疯狂 diss 代码，这周又被接着 diss。真的是自己太菜了，主要的问题可能有以下几点吧</p>\n<ol>\n<li>不同模块之间的依赖没有梳理清楚，高耦合</li>\n<li>要先对齐需求再去做，比如，想要的是一个通用的服务，那就不能把一些默认值给写死</li>\n<li>Polish! Polish Again! 写完的代码多想想能不能优化逻辑</li>\n</ol>\n<h2 id=\"焦虑\"><a class=\"markdownIt-Anchor\" href=\"#焦虑\"></a> 焦虑</h2>\n<p>作为一个摆烂 ISFP 人，还是感觉自己无时无刻不在焦虑。最近一直在为两件事焦虑，第一件是找工作。鉴于今年秋招的惨况，真的对明年的行情不敢抱有特别好的期望，所以还是想尽早开始做准备。之前计划每天一题，但由于上周赶 DDL 而搁置了。一定要捡回来，找工作应该是自己未来一年中最重要的事了，毕竟是应届生的第一份工作。第二件事是社交。一转眼，本科时候一起孤寡的同学都已经在莺莺燕燕了，而自己还是一个人在宿舍里发烂发臭…太失败了</p>\n","site":{"data":{}},"excerpt":"","more":"<p><strong>2022.10.30 - 2022.11.06</strong><br />\n看到很多大佬在写周报，把日子过得特别明白，所以从2022年11月的第一个星期开始，尝试记录一下。</p>\n<h2 id=\"紧迫感\"><a class=\"markdownIt-Anchor\" href=\"#紧迫感\"></a> 紧迫感</h2>\n<p>由于上周末的放浪，导致这周五和导师的周会的汇报内容毫无进展。到了周一着手开始做才发现，内容比想象中的工作量要大的多。接下来的三天，变成了廉价的数据工人，一个一个处理数据。还好紧赶慢赶在周会前，把schedule上的工作都做了差不多。也不记得什么时候开始（或许是大三或者大四），已经很久没有这种要做不完的紧迫感了。但这周明显感觉到了自己有所不同，心里紧张的同时，还多了一份从容吧（竟然还第一时间把原神更新的主线打完了…），不会因为这种紧迫打乱了自己的节奏。</p>\n<h2 id=\"拖延症\"><a class=\"markdownIt-Anchor\" href=\"#拖延症\"></a> 拖延症</h2>\n<p>在狂赶 DDL 的过程中，也一直在给自己定一系列的小目标，比如今晚收集完所有数据集就睡觉。然而，现实却是收集了一两份之后，就开始玩手机，然后就…在没完成小目标的情况下去睡觉了。之前也一直吐槽自己这个问题，从来没有按照导师的schedule 完成过，导致到 DDL 交出的结果往往差强人意。一定要改正！</p>\n<h2 id=\"太菜辣\"><a class=\"markdownIt-Anchor\" href=\"#太菜辣\"></a> 太菜辣</h2>\n<p>上周被开源社区的大佬疯狂 diss 代码，这周又被接着 diss。真的是自己太菜了，主要的问题可能有以下几点吧</p>\n<ol>\n<li>不同模块之间的依赖没有梳理清楚，高耦合</li>\n<li>要先对齐需求再去做，比如，想要的是一个通用的服务，那就不能把一些默认值给写死</li>\n<li>Polish! Polish Again! 写完的代码多想想能不能优化逻辑</li>\n</ol>\n<h2 id=\"焦虑\"><a class=\"markdownIt-Anchor\" href=\"#焦虑\"></a> 焦虑</h2>\n<p>作为一个摆烂 ISFP 人，还是感觉自己无时无刻不在焦虑。最近一直在为两件事焦虑，第一件是找工作。鉴于今年秋招的惨况，真的对明年的行情不敢抱有特别好的期望，所以还是想尽早开始做准备。之前计划每天一题，但由于上周赶 DDL 而搁置了。一定要捡回来，找工作应该是自己未来一年中最重要的事了，毕竟是应届生的第一份工作。第二件事是社交。一转眼，本科时候一起孤寡的同学都已经在莺莺燕燕了，而自己还是一个人在宿舍里发烂发臭…太失败了</p>\n"},{"title":"持续性摆烂","date":"2022-11-13T23:00:00.000Z","updated":"2022-11-13T23:00:00.000Z","comments":0,"_content":"**2022.11.07 - 2022.11.13**\n\n## 恍恍惚惚\n这周依旧非常焦虑，但感觉自己的焦虑有点太过了。目前的状态是只想找工作，但实际上在明年春招找工作之前，还有开题、论文一系列其他杂七杂八，但又不得不做的事情...在这种状态下，就感觉时间过得特别快，一天什么都没做就过去了。\n\n## 本末倒置\n这周一直在改论文的立意。其实感觉有些本末倒置了，先有了算法，然后再为算法找立意和场景。这就导致每次调研文献，找不到算法的背书，根本没人做过。只能怪之前都是拍脑袋想出来的算法...\n\n## 科研\n经过一番高强度的文献阅读，自己彻底放弃了对科研的最后一丝热情，以后安心做一名烂开发好了。主要的原因还是感觉自己在阅读文献，输入方面没什么问题，速度还可以，也可以很快的理解并且讲出来。但对于输出，提出创新，写作来说，真的愁死我了...憋了一个周末就做了两页 ppt...","source":"_posts/2022-01.md","raw":"---\ntitle: 持续性摆烂\ndate: 2022-11-13 23:00:00\nupdated: 2022-11-13 23:00:00\ncomments: false\ntags: \n  - weekly\n  - 2022-01\n---\n**2022.11.07 - 2022.11.13**\n\n## 恍恍惚惚\n这周依旧非常焦虑，但感觉自己的焦虑有点太过了。目前的状态是只想找工作，但实际上在明年春招找工作之前，还有开题、论文一系列其他杂七杂八，但又不得不做的事情...在这种状态下，就感觉时间过得特别快，一天什么都没做就过去了。\n\n## 本末倒置\n这周一直在改论文的立意。其实感觉有些本末倒置了，先有了算法，然后再为算法找立意和场景。这就导致每次调研文献，找不到算法的背书，根本没人做过。只能怪之前都是拍脑袋想出来的算法...\n\n## 科研\n经过一番高强度的文献阅读，自己彻底放弃了对科研的最后一丝热情，以后安心做一名烂开发好了。主要的原因还是感觉自己在阅读文献，输入方面没什么问题，速度还可以，也可以很快的理解并且讲出来。但对于输出，提出创新，写作来说，真的愁死我了...憋了一个周末就做了两页 ppt...","slug":"2022-01","published":1,"layout":"post","photos":[],"link":"","_id":"clapldfd1000518mze6l9181s","content":"<p><strong>2022.11.07 - 2022.11.13</strong></p>\n<h2 id=\"恍恍惚惚\"><a class=\"markdownIt-Anchor\" href=\"#恍恍惚惚\"></a> 恍恍惚惚</h2>\n<p>这周依旧非常焦虑，但感觉自己的焦虑有点太过了。目前的状态是只想找工作，但实际上在明年春招找工作之前，还有开题、论文一系列其他杂七杂八，但又不得不做的事情…在这种状态下，就感觉时间过得特别快，一天什么都没做就过去了。</p>\n<h2 id=\"本末倒置\"><a class=\"markdownIt-Anchor\" href=\"#本末倒置\"></a> 本末倒置</h2>\n<p>这周一直在改论文的立意。其实感觉有些本末倒置了，先有了算法，然后再为算法找立意和场景。这就导致每次调研文献，找不到算法的背书，根本没人做过。只能怪之前都是拍脑袋想出来的算法…</p>\n<h2 id=\"科研\"><a class=\"markdownIt-Anchor\" href=\"#科研\"></a> 科研</h2>\n<p>经过一番高强度的文献阅读，自己彻底放弃了对科研的最后一丝热情，以后安心做一名烂开发好了。主要的原因还是感觉自己在阅读文献，输入方面没什么问题，速度还可以，也可以很快的理解并且讲出来。但对于输出，提出创新，写作来说，真的愁死我了…憋了一个周末就做了两页 ppt…</p>\n","site":{"data":{}},"excerpt":"","more":"<p><strong>2022.11.07 - 2022.11.13</strong></p>\n<h2 id=\"恍恍惚惚\"><a class=\"markdownIt-Anchor\" href=\"#恍恍惚惚\"></a> 恍恍惚惚</h2>\n<p>这周依旧非常焦虑，但感觉自己的焦虑有点太过了。目前的状态是只想找工作，但实际上在明年春招找工作之前，还有开题、论文一系列其他杂七杂八，但又不得不做的事情…在这种状态下，就感觉时间过得特别快，一天什么都没做就过去了。</p>\n<h2 id=\"本末倒置\"><a class=\"markdownIt-Anchor\" href=\"#本末倒置\"></a> 本末倒置</h2>\n<p>这周一直在改论文的立意。其实感觉有些本末倒置了，先有了算法，然后再为算法找立意和场景。这就导致每次调研文献，找不到算法的背书，根本没人做过。只能怪之前都是拍脑袋想出来的算法…</p>\n<h2 id=\"科研\"><a class=\"markdownIt-Anchor\" href=\"#科研\"></a> 科研</h2>\n<p>经过一番高强度的文献阅读，自己彻底放弃了对科研的最后一丝热情，以后安心做一名烂开发好了。主要的原因还是感觉自己在阅读文献，输入方面没什么问题，速度还可以，也可以很快的理解并且讲出来。但对于输出，提出创新，写作来说，真的愁死我了…憋了一个周末就做了两页 ppt…</p>\n"},{"title":"开题滑铁卢","date":"2022-11-20T23:00:00.000Z","updated":"2022-11-20T23:00:00.000Z","comments":0,"_content":"\n**2022.11.14 - 2022.11.20**\n\n## 开题滑铁卢\n本周五终于完成了毕设的开题答辩。。做科研果然还是不太适合我。。虽然标题是滑铁卢，但其实从结果上来说不是特别差。由于是在设计学院读研，虽然实验室和专业方向都是计算机，但毕设依然需要紧扣设计问题。所以，接下来还需要改一下毕设题目的描述。\n\n\n## 为什么要工作\n去年的12月中旬，微软就开始了春招的提前批（万万没想到是巨硬这老贼最卷）。那现在离今年的提前批开始也就只剩下不到一个月的时间了，所以时间紧迫orz。这周找已经工作的同学review了下简历，感觉没有什么大的问题了，但有很多小的细节，我感觉还是有很多可以改进的地方，不改的话，一不小心就有可能变成面试里面的坑。\n\n另外，在网上冲浪的时候，在知乎上发现了一位做数据库的大佬，他写的一篇校招和一篇社招的文章，讲的都很好。我觉得非常清楚的分析了为什么要工作，以及什么样的工作适合他，很有参考价值。\n[硕士毕业半年的茫茫社招路](https://zhuanlan.zhihu.com/p/377154343)\n[19CS小硕校招面试心得、自学CS经验及找工作经验分享](https://zhuanlan.zhihu.com/p/108911948)\n\n## 专注度\n最近的状态一直不好，主要还是被论文烦的。实在不想做这个东西，但是又不得不做，所以一直处于一种焦虑的状态。这周把开题结束后，可以说是解决了一部分这种事情，希望下周可以好好的把论文写完。\n\n\n最后以乔布斯的一段话作为这次周报的结尾吧\n>工作将占据你生命中很大的一部分\n>\n>Your work is going to fill a large part of your life\n>只有相信自己所做的是伟大的工作，你才能获得快乐\n>\n>and the only way to be truly satisfied is to do what you believe is great work\n>而伟大的工作就源自你的爱\n>\n>And the only way to do great work is to love what you do\n>如果你还没有找到\n>\n>If you haven't found it yet\n>继续寻找，不要止步\n>\n>Keep looking and don't settle\n>全心全意地寻找\n>\n>As with all matters of the heart\n>当你遇到它时，你就会明白\n>\n>You'll know when you find it\n>就像那些美好的爱情\n>\n>And like any great relationship\n>随着岁月的流逝，愈加醇美\n>\n>it just gets better and better as the years roll on\n>所以，继续寻找，绝不止步！\n>\n>So keep looking, don't settle\n","source":"_posts/2022-02.md","raw":"---\ntitle: 开题滑铁卢\ndate: 2022-11-20 23:00:00\nupdated: 2022-11-20 23:00:00\ncomments: false\ntags: \n  - weekly\n  - 2022-02\n---\n\n**2022.11.14 - 2022.11.20**\n\n## 开题滑铁卢\n本周五终于完成了毕设的开题答辩。。做科研果然还是不太适合我。。虽然标题是滑铁卢，但其实从结果上来说不是特别差。由于是在设计学院读研，虽然实验室和专业方向都是计算机，但毕设依然需要紧扣设计问题。所以，接下来还需要改一下毕设题目的描述。\n\n\n## 为什么要工作\n去年的12月中旬，微软就开始了春招的提前批（万万没想到是巨硬这老贼最卷）。那现在离今年的提前批开始也就只剩下不到一个月的时间了，所以时间紧迫orz。这周找已经工作的同学review了下简历，感觉没有什么大的问题了，但有很多小的细节，我感觉还是有很多可以改进的地方，不改的话，一不小心就有可能变成面试里面的坑。\n\n另外，在网上冲浪的时候，在知乎上发现了一位做数据库的大佬，他写的一篇校招和一篇社招的文章，讲的都很好。我觉得非常清楚的分析了为什么要工作，以及什么样的工作适合他，很有参考价值。\n[硕士毕业半年的茫茫社招路](https://zhuanlan.zhihu.com/p/377154343)\n[19CS小硕校招面试心得、自学CS经验及找工作经验分享](https://zhuanlan.zhihu.com/p/108911948)\n\n## 专注度\n最近的状态一直不好，主要还是被论文烦的。实在不想做这个东西，但是又不得不做，所以一直处于一种焦虑的状态。这周把开题结束后，可以说是解决了一部分这种事情，希望下周可以好好的把论文写完。\n\n\n最后以乔布斯的一段话作为这次周报的结尾吧\n>工作将占据你生命中很大的一部分\n>\n>Your work is going to fill a large part of your life\n>只有相信自己所做的是伟大的工作，你才能获得快乐\n>\n>and the only way to be truly satisfied is to do what you believe is great work\n>而伟大的工作就源自你的爱\n>\n>And the only way to do great work is to love what you do\n>如果你还没有找到\n>\n>If you haven't found it yet\n>继续寻找，不要止步\n>\n>Keep looking and don't settle\n>全心全意地寻找\n>\n>As with all matters of the heart\n>当你遇到它时，你就会明白\n>\n>You'll know when you find it\n>就像那些美好的爱情\n>\n>And like any great relationship\n>随着岁月的流逝，愈加醇美\n>\n>it just gets better and better as the years roll on\n>所以，继续寻找，绝不止步！\n>\n>So keep looking, don't settle\n","slug":"2022-02","published":1,"layout":"post","photos":[],"link":"","_id":"clapldfd2000618mz6c7j01za","content":"<p><strong>2022.11.14 - 2022.11.20</strong></p>\n<h2 id=\"开题滑铁卢\"><a class=\"markdownIt-Anchor\" href=\"#开题滑铁卢\"></a> 开题滑铁卢</h2>\n<p>本周五终于完成了毕设的开题答辩。。做科研果然还是不太适合我。。虽然标题是滑铁卢，但其实从结果上来说不是特别差。由于是在设计学院读研，虽然实验室和专业方向都是计算机，但毕设依然需要紧扣设计问题。所以，接下来还需要改一下毕设题目的描述。</p>\n<h2 id=\"为什么要工作\"><a class=\"markdownIt-Anchor\" href=\"#为什么要工作\"></a> 为什么要工作</h2>\n<p>去年的12月中旬，微软就开始了春招的提前批（万万没想到是巨硬这老贼最卷）。那现在离今年的提前批开始也就只剩下不到一个月的时间了，所以时间紧迫orz。这周找已经工作的同学review了下简历，感觉没有什么大的问题了，但有很多小的细节，我感觉还是有很多可以改进的地方，不改的话，一不小心就有可能变成面试里面的坑。</p>\n<p>另外，在网上冲浪的时候，在知乎上发现了一位做数据库的大佬，他写的一篇校招和一篇社招的文章，讲的都很好。我觉得非常清楚的分析了为什么要工作，以及什么样的工作适合他，很有参考价值。<br />\n<a href=\"https://zhuanlan.zhihu.com/p/377154343\">硕士毕业半年的茫茫社招路</a><br />\n<a href=\"https://zhuanlan.zhihu.com/p/108911948\">19CS小硕校招面试心得、自学CS经验及找工作经验分享</a></p>\n<h2 id=\"专注度\"><a class=\"markdownIt-Anchor\" href=\"#专注度\"></a> 专注度</h2>\n<p>最近的状态一直不好，主要还是被论文烦的。实在不想做这个东西，但是又不得不做，所以一直处于一种焦虑的状态。这周把开题结束后，可以说是解决了一部分这种事情，希望下周可以好好的把论文写完。</p>\n<p>最后以乔布斯的一段话作为这次周报的结尾吧</p>\n<blockquote>\n<p>工作将占据你生命中很大的一部分</p>\n<p>Your work is going to fill a large part of your life<br />\n只有相信自己所做的是伟大的工作，你才能获得快乐</p>\n<p>and the only way to be truly satisfied is to do what you believe is great work<br />\n而伟大的工作就源自你的爱</p>\n<p>And the only way to do great work is to love what you do<br />\n如果你还没有找到</p>\n<p>If you haven’t found it yet<br />\n继续寻找，不要止步</p>\n<p>Keep looking and don’t settle<br />\n全心全意地寻找</p>\n<p>As with all matters of the heart<br />\n当你遇到它时，你就会明白</p>\n<p>You’ll know when you find it<br />\n就像那些美好的爱情</p>\n<p>And like any great relationship<br />\n随着岁月的流逝，愈加醇美</p>\n<p>it just gets better and better as the years roll on<br />\n所以，继续寻找，绝不止步！</p>\n<p>So keep looking, don’t settle</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p><strong>2022.11.14 - 2022.11.20</strong></p>\n<h2 id=\"开题滑铁卢\"><a class=\"markdownIt-Anchor\" href=\"#开题滑铁卢\"></a> 开题滑铁卢</h2>\n<p>本周五终于完成了毕设的开题答辩。。做科研果然还是不太适合我。。虽然标题是滑铁卢，但其实从结果上来说不是特别差。由于是在设计学院读研，虽然实验室和专业方向都是计算机，但毕设依然需要紧扣设计问题。所以，接下来还需要改一下毕设题目的描述。</p>\n<h2 id=\"为什么要工作\"><a class=\"markdownIt-Anchor\" href=\"#为什么要工作\"></a> 为什么要工作</h2>\n<p>去年的12月中旬，微软就开始了春招的提前批（万万没想到是巨硬这老贼最卷）。那现在离今年的提前批开始也就只剩下不到一个月的时间了，所以时间紧迫orz。这周找已经工作的同学review了下简历，感觉没有什么大的问题了，但有很多小的细节，我感觉还是有很多可以改进的地方，不改的话，一不小心就有可能变成面试里面的坑。</p>\n<p>另外，在网上冲浪的时候，在知乎上发现了一位做数据库的大佬，他写的一篇校招和一篇社招的文章，讲的都很好。我觉得非常清楚的分析了为什么要工作，以及什么样的工作适合他，很有参考价值。<br />\n<a href=\"https://zhuanlan.zhihu.com/p/377154343\">硕士毕业半年的茫茫社招路</a><br />\n<a href=\"https://zhuanlan.zhihu.com/p/108911948\">19CS小硕校招面试心得、自学CS经验及找工作经验分享</a></p>\n<h2 id=\"专注度\"><a class=\"markdownIt-Anchor\" href=\"#专注度\"></a> 专注度</h2>\n<p>最近的状态一直不好，主要还是被论文烦的。实在不想做这个东西，但是又不得不做，所以一直处于一种焦虑的状态。这周把开题结束后，可以说是解决了一部分这种事情，希望下周可以好好的把论文写完。</p>\n<p>最后以乔布斯的一段话作为这次周报的结尾吧</p>\n<blockquote>\n<p>工作将占据你生命中很大的一部分</p>\n<p>Your work is going to fill a large part of your life<br />\n只有相信自己所做的是伟大的工作，你才能获得快乐</p>\n<p>and the only way to be truly satisfied is to do what you believe is great work<br />\n而伟大的工作就源自你的爱</p>\n<p>And the only way to do great work is to love what you do<br />\n如果你还没有找到</p>\n<p>If you haven’t found it yet<br />\n继续寻找，不要止步</p>\n<p>Keep looking and don’t settle<br />\n全心全意地寻找</p>\n<p>As with all matters of the heart<br />\n当你遇到它时，你就会明白</p>\n<p>You’ll know when you find it<br />\n就像那些美好的爱情</p>\n<p>And like any great relationship<br />\n随着岁月的流逝，愈加醇美</p>\n<p>it just gets better and better as the years roll on<br />\n所以，继续寻找，绝不止步！</p>\n<p>So keep looking, don’t settle</p>\n</blockquote>\n"},{"title":"[BugFix] Spring Cloud IPv6端口问题排坑","date":"2021-03-07T23:59:22.000Z","updated":"2021-03-07T23:59:22.000Z","_content":"\n## 场景\n使用 Spring Cloud Eureka 搭建服务注册中心，使用 Zuul 搭建服务网关，一套比较传统的微服务架构。\n服务注册中心的地址为 http://localhost:8888，Zuul 网关地址为 http://localhost:8080， 另外搭建一个服务名为 metadata-service 的服务，地址为 http://localhost:8088。\n## 问题\n在 metadata-service 中提供一个测试的接口\n```Java\n@RestController\npublic class MetadataController {\n​\n    @GetMapping(value = \"/test\")\n    public int getTest() {\n        return 1;\n    }\n}\n```\n使用 Postman 进行测试，结果发现直接请求 http://localhost:8088/test 即 metadata-service 的地址，可以正常得到结果\n\n![](/asset/spring_ipv6/1.jpg)\n\n而通过网关，使用 Zuul 默认路由规则，调用服务，会出现 404 的错误\n\n![](/asset/spring_ipv6/2.jpg)\n\n## 分析\n首先，我们可以先通过 http://localhost:8888 查看服务是否注册到了服务注册中心\n\n![](/asset/spring_ipv6/3.jpg)\n\n可以看到没有任何问题。\n那么，我们再检查网关有没有获取到 metadata-service 的路由。可以通过 http://localhost:8080/actuator/routes 查看（actuator默认是关闭的，可以通过配置 management.endpoints.web.exposure.include=* 开启）。\n\n![](/asset/spring_ipv6/4.jpg)\n\n同样，我们可以看到没有任何问题。\n那么，就很奇怪了🤨，服务本身没有任何问题，直接调用也可以访问，而通过网关一转发，为什么就 404 了呢？在网上查了一下午，也没有找到有人遇到过类似的问题。。。😱\n问题的关键在我关闭服务后再次请求 http://localhost:8088/test 时终于找到了。正常情况下，关闭了服务后，应该没有返回的 response，但发出请求过后仍然是 404\n\n![](/asset/spring_ipv6/5.jpg)\n\n那么，就很明显了，有另一个进程也在监听 8088 端口 ！！！\n但还是很奇怪，那为什么服务启动的时候没有报端口被占用的错误呢？？？\n重新启动服务，使用 lsof -i tcp:8088 （Mac OS）查看端口占用情况\n\n![](/asset/spring_ipv6/6.jpg)\n\n果然有两个进程同时在监听，而一个是 IPv4，一个是 IPv6的。\n首先，根据这篇文章 https://blog.csdn.net/jiyiqinlovexx/article/details/50959351 的解释，多个进程是完全可以同时监听同一个端口的。\n而从 Java 7 开始，默认使用 IPv6 而不是 IPv4 （https://stackoverflow.com/questions/35470838/localhost-vs-127-0-0-1-in-spring-framework），所以对于 Spring 的 localhost 来说，其实真正使用的 IP 地址是 ::1，而不是 127.0.0.1 。使用 Postman 进行测试，可以发现 http://[::1]:8088/test 得到正常结果，而 http://127.0.0.1:8088/test 则为 404 。这就完美地解释了开启服务与停止服务，返回结果不同的问题，Spring 服务所对应的正是那个 IPv6 的进程。\n那么，为什么网关转发就到了 IPv4 呢？我们再来看一下服务注册中心里的信息\n\n![](/asset/spring_ipv6/7.jpg)\n\n可以看到其实 Eureka 保存的是每个服务的 IP 地址是本机的 IPv4 的内网地址，而不是保存域名，这就是问题的关键。我们可以使用 Postman 发送请求  http://localhost:8080/metadata-service/test 后，使用命令 lsof -i tcp:8088 进行验证。\n\n![](/asset/spring_ipv6/8.jpg)\n\n可以看到的确是向内网 IP 地址，而不是向 localhost 转发请求。\n## 解决方案\n至此，问题的原因已经完全清楚了，果然程序都是 debug de 出来的。\n最简单的方法也很清楚了，换个端口号就 OK 了。\n\n如果本文有错误或者理解不对的地方，欢迎指正！！！😆\n\n那么，占了 8088 端口的 IPv4 进程是哪个程序呢？🤨\n\n\n\n\n\n\n\n。。。。Hadoop 出来挨打！！！😭😭😭","source":"_posts/Spring IPv6.md","raw":"---\ntitle: \"[BugFix] Spring Cloud IPv6端口问题排坑\"\ndate: 2021-03-07 23:59:22\nupdated: 2021-03-07 23:59:22\n---\n\n## 场景\n使用 Spring Cloud Eureka 搭建服务注册中心，使用 Zuul 搭建服务网关，一套比较传统的微服务架构。\n服务注册中心的地址为 http://localhost:8888，Zuul 网关地址为 http://localhost:8080， 另外搭建一个服务名为 metadata-service 的服务，地址为 http://localhost:8088。\n## 问题\n在 metadata-service 中提供一个测试的接口\n```Java\n@RestController\npublic class MetadataController {\n​\n    @GetMapping(value = \"/test\")\n    public int getTest() {\n        return 1;\n    }\n}\n```\n使用 Postman 进行测试，结果发现直接请求 http://localhost:8088/test 即 metadata-service 的地址，可以正常得到结果\n\n![](/asset/spring_ipv6/1.jpg)\n\n而通过网关，使用 Zuul 默认路由规则，调用服务，会出现 404 的错误\n\n![](/asset/spring_ipv6/2.jpg)\n\n## 分析\n首先，我们可以先通过 http://localhost:8888 查看服务是否注册到了服务注册中心\n\n![](/asset/spring_ipv6/3.jpg)\n\n可以看到没有任何问题。\n那么，我们再检查网关有没有获取到 metadata-service 的路由。可以通过 http://localhost:8080/actuator/routes 查看（actuator默认是关闭的，可以通过配置 management.endpoints.web.exposure.include=* 开启）。\n\n![](/asset/spring_ipv6/4.jpg)\n\n同样，我们可以看到没有任何问题。\n那么，就很奇怪了🤨，服务本身没有任何问题，直接调用也可以访问，而通过网关一转发，为什么就 404 了呢？在网上查了一下午，也没有找到有人遇到过类似的问题。。。😱\n问题的关键在我关闭服务后再次请求 http://localhost:8088/test 时终于找到了。正常情况下，关闭了服务后，应该没有返回的 response，但发出请求过后仍然是 404\n\n![](/asset/spring_ipv6/5.jpg)\n\n那么，就很明显了，有另一个进程也在监听 8088 端口 ！！！\n但还是很奇怪，那为什么服务启动的时候没有报端口被占用的错误呢？？？\n重新启动服务，使用 lsof -i tcp:8088 （Mac OS）查看端口占用情况\n\n![](/asset/spring_ipv6/6.jpg)\n\n果然有两个进程同时在监听，而一个是 IPv4，一个是 IPv6的。\n首先，根据这篇文章 https://blog.csdn.net/jiyiqinlovexx/article/details/50959351 的解释，多个进程是完全可以同时监听同一个端口的。\n而从 Java 7 开始，默认使用 IPv6 而不是 IPv4 （https://stackoverflow.com/questions/35470838/localhost-vs-127-0-0-1-in-spring-framework），所以对于 Spring 的 localhost 来说，其实真正使用的 IP 地址是 ::1，而不是 127.0.0.1 。使用 Postman 进行测试，可以发现 http://[::1]:8088/test 得到正常结果，而 http://127.0.0.1:8088/test 则为 404 。这就完美地解释了开启服务与停止服务，返回结果不同的问题，Spring 服务所对应的正是那个 IPv6 的进程。\n那么，为什么网关转发就到了 IPv4 呢？我们再来看一下服务注册中心里的信息\n\n![](/asset/spring_ipv6/7.jpg)\n\n可以看到其实 Eureka 保存的是每个服务的 IP 地址是本机的 IPv4 的内网地址，而不是保存域名，这就是问题的关键。我们可以使用 Postman 发送请求  http://localhost:8080/metadata-service/test 后，使用命令 lsof -i tcp:8088 进行验证。\n\n![](/asset/spring_ipv6/8.jpg)\n\n可以看到的确是向内网 IP 地址，而不是向 localhost 转发请求。\n## 解决方案\n至此，问题的原因已经完全清楚了，果然程序都是 debug de 出来的。\n最简单的方法也很清楚了，换个端口号就 OK 了。\n\n如果本文有错误或者理解不对的地方，欢迎指正！！！😆\n\n那么，占了 8088 端口的 IPv4 进程是哪个程序呢？🤨\n\n\n\n\n\n\n\n。。。。Hadoop 出来挨打！！！😭😭😭","slug":"Spring IPv6","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clapldfd3000718mz0achedwn","content":"<h2 id=\"场景\"><a class=\"markdownIt-Anchor\" href=\"#场景\"></a> 场景</h2>\n<p>使用 Spring Cloud Eureka 搭建服务注册中心，使用 Zuul 搭建服务网关，一套比较传统的微服务架构。<br />\n服务注册中心的地址为 <a href=\"http://localhost:8888\">http://localhost:8888</a>，Zuul 网关地址为 <a href=\"http://localhost:8080\">http://localhost:8080</a>， 另外搭建一个服务名为 metadata-service 的服务，地址为 <a href=\"http://localhost:8088\">http://localhost:8088</a>。</p>\n<h2 id=\"问题\"><a class=\"markdownIt-Anchor\" href=\"#问题\"></a> 问题</h2>\n<p>在 metadata-service 中提供一个测试的接口</p>\n<pre class=\"highlight\"><code class=\"Java\"><span class=\"hljs-meta\">@RestController</span>\n<span class=\"hljs-keyword\">public</span> <span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MetadataController</span> &#123;\n​\n    <span class=\"hljs-meta\">@GetMapping(value = &quot;/test&quot;)</span>\n    <span class=\"hljs-keyword\">public</span> <span class=\"hljs-type\">int</span> <span class=\"hljs-title function_\">getTest</span><span class=\"hljs-params\">()</span> &#123;\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-number\">1</span>;\n    &#125;\n&#125;\n</code></pre>\n<p>使用 Postman 进行测试，结果发现直接请求 <a href=\"http://localhost:8088/test\">http://localhost:8088/test</a> 即 metadata-service 的地址，可以正常得到结果</p>\n<p><img src=\"/asset/spring_ipv6/1.jpg\" alt=\"\" /></p>\n<p>而通过网关，使用 Zuul 默认路由规则，调用服务，会出现 404 的错误</p>\n<p><img src=\"/asset/spring_ipv6/2.jpg\" alt=\"\" /></p>\n<h2 id=\"分析\"><a class=\"markdownIt-Anchor\" href=\"#分析\"></a> 分析</h2>\n<p>首先，我们可以先通过 <a href=\"http://localhost:8888\">http://localhost:8888</a> 查看服务是否注册到了服务注册中心</p>\n<p><img src=\"/asset/spring_ipv6/3.jpg\" alt=\"\" /></p>\n<p>可以看到没有任何问题。<br />\n那么，我们再检查网关有没有获取到 metadata-service 的路由。可以通过 <a href=\"http://localhost:8080/actuator/routes\">http://localhost:8080/actuator/routes</a> 查看（actuator默认是关闭的，可以通过配置 management.endpoints.web.exposure.include=* 开启）。</p>\n<p><img src=\"/asset/spring_ipv6/4.jpg\" alt=\"\" /></p>\n<p>同样，我们可以看到没有任何问题。<br />\n那么，就很奇怪了🤨，服务本身没有任何问题，直接调用也可以访问，而通过网关一转发，为什么就 404 了呢？在网上查了一下午，也没有找到有人遇到过类似的问题。。。😱<br />\n问题的关键在我关闭服务后再次请求 <a href=\"http://localhost:8088/test\">http://localhost:8088/test</a> 时终于找到了。正常情况下，关闭了服务后，应该没有返回的 response，但发出请求过后仍然是 404</p>\n<p><img src=\"/asset/spring_ipv6/5.jpg\" alt=\"\" /></p>\n<p>那么，就很明显了，有另一个进程也在监听 8088 端口 ！！！<br />\n但还是很奇怪，那为什么服务启动的时候没有报端口被占用的错误呢？？？<br />\n重新启动服务，使用 lsof -i tcp:8088 （Mac OS）查看端口占用情况</p>\n<p><img src=\"/asset/spring_ipv6/6.jpg\" alt=\"\" /></p>\n<p>果然有两个进程同时在监听，而一个是 IPv4，一个是 IPv6的。<br />\n首先，根据这篇文章 <a href=\"https://blog.csdn.net/jiyiqinlovexx/article/details/50959351\">https://blog.csdn.net/jiyiqinlovexx/article/details/50959351</a> 的解释，多个进程是完全可以同时监听同一个端口的。<br />\n而从 Java 7 开始，默认使用 IPv6 而不是 IPv4 （<a href=\"https://stackoverflow.com/questions/35470838/localhost-vs-127-0-0-1-in-spring-framework%EF%BC%89%EF%BC%8C%E6%89%80%E4%BB%A5%E5%AF%B9%E4%BA%8E\">https://stackoverflow.com/questions/35470838/localhost-vs-127-0-0-1-in-spring-framework），所以对于</a> Spring 的 localhost 来说，其实真正使用的 IP 地址是 ::1，而不是 127.0.0.1 。使用 Postman 进行测试，可以发现 http://[::1]:8088/test 得到正常结果，而 <a href=\"http://127.0.0.1:8088/test\">http://127.0.0.1:8088/test</a> 则为 404 。这就完美地解释了开启服务与停止服务，返回结果不同的问题，Spring 服务所对应的正是那个 IPv6 的进程。<br />\n那么，为什么网关转发就到了 IPv4 呢？我们再来看一下服务注册中心里的信息</p>\n<p><img src=\"/asset/spring_ipv6/7.jpg\" alt=\"\" /></p>\n<p>可以看到其实 Eureka 保存的是每个服务的 IP 地址是本机的 IPv4 的内网地址，而不是保存域名，这就是问题的关键。我们可以使用 Postman 发送请求  <a href=\"http://localhost:8080/metadata-service/test\">http://localhost:8080/metadata-service/test</a> 后，使用命令 lsof -i tcp:8088 进行验证。</p>\n<p><img src=\"/asset/spring_ipv6/8.jpg\" alt=\"\" /></p>\n<p>可以看到的确是向内网 IP 地址，而不是向 localhost 转发请求。</p>\n<h2 id=\"解决方案\"><a class=\"markdownIt-Anchor\" href=\"#解决方案\"></a> 解决方案</h2>\n<p>至此，问题的原因已经完全清楚了，果然程序都是 debug de 出来的。<br />\n最简单的方法也很清楚了，换个端口号就 OK 了。</p>\n<p>如果本文有错误或者理解不对的地方，欢迎指正！！！😆</p>\n<p>那么，占了 8088 端口的 IPv4 进程是哪个程序呢？🤨</p>\n<p>。。。。Hadoop 出来挨打！！！😭😭😭</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"场景\"><a class=\"markdownIt-Anchor\" href=\"#场景\"></a> 场景</h2>\n<p>使用 Spring Cloud Eureka 搭建服务注册中心，使用 Zuul 搭建服务网关，一套比较传统的微服务架构。<br />\n服务注册中心的地址为 <a href=\"http://localhost:8888\">http://localhost:8888</a>，Zuul 网关地址为 <a href=\"http://localhost:8080\">http://localhost:8080</a>， 另外搭建一个服务名为 metadata-service 的服务，地址为 <a href=\"http://localhost:8088\">http://localhost:8088</a>。</p>\n<h2 id=\"问题\"><a class=\"markdownIt-Anchor\" href=\"#问题\"></a> 问题</h2>\n<p>在 metadata-service 中提供一个测试的接口</p>\n<pre class=\"highlight\"><code class=\"Java\"><span class=\"hljs-meta\">@RestController</span>\n<span class=\"hljs-keyword\">public</span> <span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MetadataController</span> &#123;\n​\n    <span class=\"hljs-meta\">@GetMapping(value = &quot;/test&quot;)</span>\n    <span class=\"hljs-keyword\">public</span> <span class=\"hljs-type\">int</span> <span class=\"hljs-title function_\">getTest</span><span class=\"hljs-params\">()</span> &#123;\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-number\">1</span>;\n    &#125;\n&#125;\n</code></pre>\n<p>使用 Postman 进行测试，结果发现直接请求 <a href=\"http://localhost:8088/test\">http://localhost:8088/test</a> 即 metadata-service 的地址，可以正常得到结果</p>\n<p><img src=\"/asset/spring_ipv6/1.jpg\" alt=\"\" /></p>\n<p>而通过网关，使用 Zuul 默认路由规则，调用服务，会出现 404 的错误</p>\n<p><img src=\"/asset/spring_ipv6/2.jpg\" alt=\"\" /></p>\n<h2 id=\"分析\"><a class=\"markdownIt-Anchor\" href=\"#分析\"></a> 分析</h2>\n<p>首先，我们可以先通过 <a href=\"http://localhost:8888\">http://localhost:8888</a> 查看服务是否注册到了服务注册中心</p>\n<p><img src=\"/asset/spring_ipv6/3.jpg\" alt=\"\" /></p>\n<p>可以看到没有任何问题。<br />\n那么，我们再检查网关有没有获取到 metadata-service 的路由。可以通过 <a href=\"http://localhost:8080/actuator/routes\">http://localhost:8080/actuator/routes</a> 查看（actuator默认是关闭的，可以通过配置 management.endpoints.web.exposure.include=* 开启）。</p>\n<p><img src=\"/asset/spring_ipv6/4.jpg\" alt=\"\" /></p>\n<p>同样，我们可以看到没有任何问题。<br />\n那么，就很奇怪了🤨，服务本身没有任何问题，直接调用也可以访问，而通过网关一转发，为什么就 404 了呢？在网上查了一下午，也没有找到有人遇到过类似的问题。。。😱<br />\n问题的关键在我关闭服务后再次请求 <a href=\"http://localhost:8088/test\">http://localhost:8088/test</a> 时终于找到了。正常情况下，关闭了服务后，应该没有返回的 response，但发出请求过后仍然是 404</p>\n<p><img src=\"/asset/spring_ipv6/5.jpg\" alt=\"\" /></p>\n<p>那么，就很明显了，有另一个进程也在监听 8088 端口 ！！！<br />\n但还是很奇怪，那为什么服务启动的时候没有报端口被占用的错误呢？？？<br />\n重新启动服务，使用 lsof -i tcp:8088 （Mac OS）查看端口占用情况</p>\n<p><img src=\"/asset/spring_ipv6/6.jpg\" alt=\"\" /></p>\n<p>果然有两个进程同时在监听，而一个是 IPv4，一个是 IPv6的。<br />\n首先，根据这篇文章 <a href=\"https://blog.csdn.net/jiyiqinlovexx/article/details/50959351\">https://blog.csdn.net/jiyiqinlovexx/article/details/50959351</a> 的解释，多个进程是完全可以同时监听同一个端口的。<br />\n而从 Java 7 开始，默认使用 IPv6 而不是 IPv4 （<a href=\"https://stackoverflow.com/questions/35470838/localhost-vs-127-0-0-1-in-spring-framework%EF%BC%89%EF%BC%8C%E6%89%80%E4%BB%A5%E5%AF%B9%E4%BA%8E\">https://stackoverflow.com/questions/35470838/localhost-vs-127-0-0-1-in-spring-framework），所以对于</a> Spring 的 localhost 来说，其实真正使用的 IP 地址是 ::1，而不是 127.0.0.1 。使用 Postman 进行测试，可以发现 http://[::1]:8088/test 得到正常结果，而 <a href=\"http://127.0.0.1:8088/test\">http://127.0.0.1:8088/test</a> 则为 404 。这就完美地解释了开启服务与停止服务，返回结果不同的问题，Spring 服务所对应的正是那个 IPv6 的进程。<br />\n那么，为什么网关转发就到了 IPv4 呢？我们再来看一下服务注册中心里的信息</p>\n<p><img src=\"/asset/spring_ipv6/7.jpg\" alt=\"\" /></p>\n<p>可以看到其实 Eureka 保存的是每个服务的 IP 地址是本机的 IPv4 的内网地址，而不是保存域名，这就是问题的关键。我们可以使用 Postman 发送请求  <a href=\"http://localhost:8080/metadata-service/test\">http://localhost:8080/metadata-service/test</a> 后，使用命令 lsof -i tcp:8088 进行验证。</p>\n<p><img src=\"/asset/spring_ipv6/8.jpg\" alt=\"\" /></p>\n<p>可以看到的确是向内网 IP 地址，而不是向 localhost 转发请求。</p>\n<h2 id=\"解决方案\"><a class=\"markdownIt-Anchor\" href=\"#解决方案\"></a> 解决方案</h2>\n<p>至此，问题的原因已经完全清楚了，果然程序都是 debug de 出来的。<br />\n最简单的方法也很清楚了，换个端口号就 OK 了。</p>\n<p>如果本文有错误或者理解不对的地方，欢迎指正！！！😆</p>\n<p>那么，占了 8088 端口的 IPv4 进程是哪个程序呢？🤨</p>\n<p>。。。。Hadoop 出来挨打！！！😭😭😭</p>\n"},{"title":"Awesome Tech Post 第0期","date":"2022-07-15T22:58:41.000Z","updated":"2022-07-15T22:58:41.000Z","math":true,"_content":"\n![](/asset/AwesomeTechPost0/2022-06-30-00-00-42-image.png)\n\n每期 Awesome Tech Post 都会摘录推荐 5 篇优质技术博客，对这些文章的内容进行提炼总结。每期覆盖领域各不相同，可能从后端到可视化，从工程到算法等等，但每篇文章都会对领域内的某一问题进行深入分析或提出独到见解。欢迎大家私信推荐文章~\n\n## TL;DR\n\n本期内容：\n\n1. 如何对 Kubernetes Operator 进行分布式 Tracing `Cloud`\n\n2. Kubernetes 多集群管理与联邦 `Cloud`\n\n3. 强化学习中对无效动作的 mask `Reinforcement Learning`\n\n4. 一些深度学习中的采样方式和损失函数 `Deep Learning`\n\n5. 如何在一个全新的领域开展学习 `Soft Skill`\n\n**对于感兴趣的内容，可以阅读对应的博客原文查看更多详情~**\n\n## 0. How to Monitor Kubernetes Operators By Distributed-Tracing?\n\n原文链接：[https://yue9944882.github.io/posts/how-to-monitor-kubernetes-operator-by-distributed-tracing/](https://yue9944882.github.io/posts/how-to-monitor-kubernetes-operator-by-distributed-tracing/)\n\n在一个健全的系统中，应当对一条请求在完整生命周期中完成了哪些处理都进行监控追踪。随着现在大量的分布式应用和微服务的落地，一条请求可能跨越多个服务，甚至集群。对于这类请求的 tracing（追踪）问题就是 distributed tracing。Tracing 的整个流程可以被建模成一个树，其中每个节点是请求所经过的处理（根据监控的粒度，可以是一个服务，也可以是一个函数）。请求经过的每个处理被称作为 span。\n\n### 异步问题\n\n在微服务中，一条请求 R 到达微服务1后，一般通过 HTTP 或 RPC，进一步请求到微服务2进行处理，再到微服务N。最后，沿着这一条链，进行反向的返回 response。显然，这是一个同步的过程，我们可以很清晰的看出 R 的处理流程，R 的 tracing 结果就是这条转发树。\n\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/2022-07-12-22-28-28-image.png\" alt=\"\" data-align=\"center\" width=\"764\">\n</p>\n\n然而，与微服务不同，k8s operator 采用了完全不同的协作模式。K8s operator 不会与其他 operator 直接进行交互，而是生成一个 Event（例如，创建一个 Pod），k8s 会将这个 Event 放入到一个队列中。所有的 operator 都不断地轮询，从这个队列中获取符合自己过滤条件的 Event。显然，这是一个异步的过程。一异步，问题就麻烦了：\n\n1. Event 的生成/消费不是线性的\n   \n   Event 的生成和消费不是一一对应的。一方面，operator 可能将多个 Event 合并成一个任务，或者将一个 Event 分成多个任务。另一方面，一个任务由于重试策略可能会执行多次（在一个 operator 上产生多个 span）。\n\n2. Event 循环\n   \n   当一个 operator 根据 Event 改变 k8s 的资源后，又会产生一个新的 Event（k8s 中有资源被改变）。这就提出了一个问题，什么是这次 tracing 的结束？详细的讨论可以参考原文。\n\n### Operator 分类\n\n每个 operator 都可能监听一个主要的 resource 和多个次要的 resource，因此，可以对 operator 进行如下的分类：\n\n1. Type A：operator 只接收 Event。\n\n2. Type B：operator 接收 Event，对 k8s 外的系统进行操作。\n\n3. Type C：operator 只修改自己监听的资源。\n\n4. Type D：operator 只修改不被自己监听的资源。\n\n5. Type E：operator 修改任意的资源（Type C + Type D）。\n\n对于 Type A 和 Type B 来说，请求到他们这里就结束了，所以他们是 leaf span。\n\n对于 Type C 和 Type D 来说，由于不能确定有没有其他 operator 在监听同一个资源，所以无法判断其是否是 leaf span。对于 Type C 可以肯定的是，在 operator 完成最后一次 write 的时候，他仍会收到一个 Event（因为它所修改的资源正是自己监听的资源），并且会 drop 这个 Event（这个 Event 是由自身修改产生的，无意义）。因此，我们可以确定这个 operator 上多次 span 的 parent/child 关系。而对于 Type D，无法收到修改资源的最后一次 write 的 Event，所以，我们只能建立这个 operator 上多次 span 之间较弱的 link 关系。\n\n对于 Type E 来说，这是最复杂但又是最常见的类型。Type E 其实是 Type C 和 Type D 的组合，所以对于 Type E 操作的每个资源，我们可以按照资源的类型，将 Type E 当前的 span 暂时转换成 Type C 或 Type D 来处理。\n\n## 1. Kubernetes、集群联邦和资源分发\n\n原文链接：[https://draveness.me/kuberentes-federation/](https://draveness.me/kuberentes-federation/)\n\nKubernetes 目前最多可以支持管理 5000 个节点，对于超过 5000 个节点的集群管理，就需要寻找其他方法对多个 K8s 集群进行管理。多集群其实不是一个新的概念，在很久之前，就在业界看到过 Mesos + K8s 的多集群管理方法。但是，多集群中的每个集群都相对独立，彼此之间没有联系，每个服务都是独立的运行在一个集群里的。而集群联邦则是在此基础上增加了两个重要的功能：跨集群的服务发现和跨集群的调度，使得一个多应用服务可以运行在多个集群上，进一步提升了服务的稳定性和可用性。\n\n文章中，作者以两个比较出名的集群联邦项目为例，介绍了目前集群联邦的方案：\n\n1. **Kubefed** 会为每个原生资源（e.g. Deployment）生成对应的联邦资源（e.g. FederatedDeployment）作为管理。联邦资源中会包含 Template（资源信息）、Placement（所需部署的集群）和 Overrides（同步资源到集群时，需要覆写的属性）三个部分。在分发到下游集群时，Kubefed 再根据联邦资源生成具体的原生资源。\n\n2. **Karmada** 是 Kubefed 项目的延续，其中的概念也几乎全盘继承自 Kubefed。稍有不同的是，Karmada 保留了原生资源，并将 Kubefed 中联邦资源的 Placement 和 Override 抽离了出来，作为两个新的自定义资源 PropagationPolicy 和 OverriderPolicy。\n\n<figure>\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/0a98b72fdc1acab504726847f894f6f9df8a699e.png\" alt=\"16477814091159-kubefed-karmada-api.png\" data-align=\"center\" width=\"497\">\n</p>\n<figcaption align = \"center\">图片来自原文</figcaption>\n</figure>\n\n对于任务调度来说，文章中提到了“因为上下文的不足，集群联邦不需要也没有办法保证调度的全局最优解，而提供跨集群的部署和故障转移就已经可以满足常见的需求了”。\n\n## 2. A Closer Look at Invalid Action Masking in Policy Gradient Algorithms\n\n原文链接：[https://costa.sh/blog-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms.html](https://costa.sh/blog-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms.html)\n\n本篇文章是作者对所发表的同名论文 [https://arxiv.org/abs/2006.14171](https://arxiv.org/abs/2006.14171) 的介绍。对强化学习稍微有所了解的同学应该都知道 Policy Gradient，属于强化学习的两大分类之一。而 Invalid Action（无效动作）是强化学习中经常遇到的问题，例如，在训练模型走迷宫时，前方有障碍物，那么前进这一动作就是 invalid 的。那么，在训练时，需要对模型过滤掉这类动作，也就是 masking。而本篇文章就在尝试解释 Policy Gradient 算法中 invalid action masking 的工作原理。虽然 masking 在很多论文里都用到了，但都只是一句话带过（我之前读到的几篇甚至不会提到这些细节），没有对 masking 的原理进行深入探索。这也是文章作者的 motivition 之一。\n\n简单来说，invalid action masking 就是在模型根据概率采样动作时，采用一个 mask 将 invalid action 的概率置为 0。文章中作者将 invalid action masking 建模成以下的函数 $inv_s$：\n\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/2022-07-14-21-55-41-image.png\" alt=\"\" data-align=\"center\" width=\"300\">\n</p>\n\n$l(s)$ 是状态 $s$ 的 log 值。$inv_s$ 在两种情况下都是可微的，在常数 $M$ 时，梯度为0，因此，反向传播时不会更新模型有关 invalid action 相关的参数。\n\npolicy 在采样时的概率为：\n\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/2022-07-14-21-56-23-image.png\" alt=\"\" data-align=\"center\" width=\"235\">\n</p>\n\n文章中还通过量化的实验结果来验证 invalid action masking 的有效性，详情可以阅读原文。\n\n## 3. 深度学习新的采样方式和损失函数--论文笔记\n\n原文链接：[https://zhuanlan.zhihu.com/p/27748177](https://zhuanlan.zhihu.com/p/27748177)\n\n本篇文章是对论文《Sampling matters in deep embedding learning》[https://arxiv.org/pdf/1706.07567.pdf](https://arxiv.org/pdf/1706.07567.pdf) 的概述。论文主要解决的是 deep embedding learning 中的采样问题和损失函数问题。文章对论文的主要内容进行了很好的概述，这里就不再赘述了，就简单的罗列一些 insight 和文章中没解释清楚的部分：\n\n1. Triplet loss 优于 Contrastive loss 的原因有两点：\n   \n   1. constrative loss 假设所有样本都符合相同分布，而 triplet loss 没有这个假设。因此，可以适应各种空间形状，一定程度上能够抵御噪声的影响\n   \n   2. triplet loss 优化的目标是正负样本之间的相对距离差，即正样本之间的距离小于正负样本之间的距离。而 constrative loss 优化的目标是绝对距离，即所有的正样本之间的距离也要尽可能小。这是没有必要的。\n\n2. 假设负样本均匀分布，我们也均匀随机采样。那么，采样的负样本 pairwise distance 符合如下的分布：\n   \n   <p style=\"text-align: center\">\n   <img title=\"test\" src=\"/asset/AwesomeTechPost0/fdad5d725c64c0b589b8d107e5e228d285497474.png\" alt=\"\" data-align=\"center\" width=\"318\">\n   </p>\n   \n   换句话说，在高维空间里，采样得到的负样本 pairwise distance 基本上都是大于 $\\sqrt{2}$ 的。论文针对这个问题，提出的方法是 distance weighted sampling。以距离概率值的倒数 $q(d)^{-1}$ 作为样本采样的权重，这样在修正样本距离分布的 bias 的同时控制了 variance。\n\n3. Triplet loss 采用的是一种 hard negtive mining 的方法，也就是正负样本的区分是 hard 的。负样本的梯度通过如下的公式计算：\n\n   <p style=\"text-align: center\">\n   <img src=\"/asset/AwesomeTechPost0/f5026649684b309b85e2f5880e0a84bfdd7c01de.png\" title=\"\" alt=\"v2-966f53117397f560a6395fef136ca5f8_1440w.png\" data-align=\"center\">\n   </p>\n   \n   梯度的方向取决于 $h_{an}$ ，即 anchor 样本与负样本的向量差。那么，如果差向量的绝对值特别小，并且这个负样本是异常值，那对模型的梯度会造成很大的影响。\n   \n   <p style=\"text-align: center\">\n   <img src=\"/asset/AwesomeTechPost0/bd74f1ae5c04b74ee412affefedd54fceffb790d.png\" title=\"\" alt=\"v2-80e1c0da65e11100f88b6175857a79ba_1440w.png\" data-align=\"center\">\n   </p>\n   \n   上图中展示了随着 pairwise distance 的增加，各种 loss 中正负样本是如何变化的。蓝色实线是正样本，绿色虚线是负样本。对于图b，$D_{an}$ 越小，梯度值也越趋向于 0。根据 triplet loss 的计算公式，我们可以发现，这导致了模型趋向于这个点的梯度（正）会很大，但是远离这个点的梯度（负）很小。论文提出了 margin based loss 来解决这一问题，即图 d 中的 loss。在 $D_{an}$ 很小的时候，仍然保证了负样本的梯度为一个常数，这有点类似 ReLU 的思想。\n\n## 4. 如何在一个全新的领域展开学习\n\n原文链接：[https://ichn.xyz/blog/how-to-start-learning-in-a-new-area](https://ichn.xyz/blog/how-to-start-learning-in-a-new-area)\n\n学习计算机最重要的点在于关注能力的成长。而所有能力中最重要的，莫过于学习的能力，学习能力是培养其他能力的元能力。文章作者在接触了大量计算机细分领域后，总结出了几点特定的套路：\n\n1. 明确动机\n   \n   要有明确且实际的需求。一是可以解决兴趣使然导致的选择困难，二是在学习的过程中，感受到切实的正反馈。\n\n2. 背景调查\n   \n   一旦明确了学习的动机和目标，应该更系统性地、刻意地对这个领域展开背景调查。\n   \n   > 知道即将学习的知识可以解决什么样的问题，这种解决手段和其他方式相比的优劣，这个领域和其他领域、特别是自己已经熟悉的领域的关系是怎样的，这个领域的发展历史和发展脉络是怎样的，有哪些独特且重要的概念？\n   > \n   > 背景调查获取的信息通常是宏观或者碎片化的，这并不是真正的学习。但这个过程可以提高你对这个领域的熟悉程度，在你的话语体系和思考方式中加入这个领域的成分，并提高你对这个领域的品味与认知。\n\n3. 资源汇集与整理\n   \n   了解这个领域有哪些重要的资料，更重要的是会拥有判断这个方向的学习资料的优劣的能力。\n\n4. 制定计划，然后无情地执行\n   \n   如果执行学习计划中会有枯燥的感觉，就需要回顾自己的动机、目标，并稍稍跳出来重新审视一下学习计划。\n   \n   如果确认学习路径的正确性，就应该专注，而不是继续在这个领域中漫无目的的探索，这样才能进入深水区。\n","source":"_posts/awesome-tech-post-0.md","raw":"---\ntitle: Awesome Tech Post 第0期\ndate: 2022-07-15 22:58:41\nupdated: 2022-07-15 22:58:41\nmath: true\n---\n\n![](/asset/AwesomeTechPost0/2022-06-30-00-00-42-image.png)\n\n每期 Awesome Tech Post 都会摘录推荐 5 篇优质技术博客，对这些文章的内容进行提炼总结。每期覆盖领域各不相同，可能从后端到可视化，从工程到算法等等，但每篇文章都会对领域内的某一问题进行深入分析或提出独到见解。欢迎大家私信推荐文章~\n\n## TL;DR\n\n本期内容：\n\n1. 如何对 Kubernetes Operator 进行分布式 Tracing `Cloud`\n\n2. Kubernetes 多集群管理与联邦 `Cloud`\n\n3. 强化学习中对无效动作的 mask `Reinforcement Learning`\n\n4. 一些深度学习中的采样方式和损失函数 `Deep Learning`\n\n5. 如何在一个全新的领域开展学习 `Soft Skill`\n\n**对于感兴趣的内容，可以阅读对应的博客原文查看更多详情~**\n\n## 0. How to Monitor Kubernetes Operators By Distributed-Tracing?\n\n原文链接：[https://yue9944882.github.io/posts/how-to-monitor-kubernetes-operator-by-distributed-tracing/](https://yue9944882.github.io/posts/how-to-monitor-kubernetes-operator-by-distributed-tracing/)\n\n在一个健全的系统中，应当对一条请求在完整生命周期中完成了哪些处理都进行监控追踪。随着现在大量的分布式应用和微服务的落地，一条请求可能跨越多个服务，甚至集群。对于这类请求的 tracing（追踪）问题就是 distributed tracing。Tracing 的整个流程可以被建模成一个树，其中每个节点是请求所经过的处理（根据监控的粒度，可以是一个服务，也可以是一个函数）。请求经过的每个处理被称作为 span。\n\n### 异步问题\n\n在微服务中，一条请求 R 到达微服务1后，一般通过 HTTP 或 RPC，进一步请求到微服务2进行处理，再到微服务N。最后，沿着这一条链，进行反向的返回 response。显然，这是一个同步的过程，我们可以很清晰的看出 R 的处理流程，R 的 tracing 结果就是这条转发树。\n\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/2022-07-12-22-28-28-image.png\" alt=\"\" data-align=\"center\" width=\"764\">\n</p>\n\n然而，与微服务不同，k8s operator 采用了完全不同的协作模式。K8s operator 不会与其他 operator 直接进行交互，而是生成一个 Event（例如，创建一个 Pod），k8s 会将这个 Event 放入到一个队列中。所有的 operator 都不断地轮询，从这个队列中获取符合自己过滤条件的 Event。显然，这是一个异步的过程。一异步，问题就麻烦了：\n\n1. Event 的生成/消费不是线性的\n   \n   Event 的生成和消费不是一一对应的。一方面，operator 可能将多个 Event 合并成一个任务，或者将一个 Event 分成多个任务。另一方面，一个任务由于重试策略可能会执行多次（在一个 operator 上产生多个 span）。\n\n2. Event 循环\n   \n   当一个 operator 根据 Event 改变 k8s 的资源后，又会产生一个新的 Event（k8s 中有资源被改变）。这就提出了一个问题，什么是这次 tracing 的结束？详细的讨论可以参考原文。\n\n### Operator 分类\n\n每个 operator 都可能监听一个主要的 resource 和多个次要的 resource，因此，可以对 operator 进行如下的分类：\n\n1. Type A：operator 只接收 Event。\n\n2. Type B：operator 接收 Event，对 k8s 外的系统进行操作。\n\n3. Type C：operator 只修改自己监听的资源。\n\n4. Type D：operator 只修改不被自己监听的资源。\n\n5. Type E：operator 修改任意的资源（Type C + Type D）。\n\n对于 Type A 和 Type B 来说，请求到他们这里就结束了，所以他们是 leaf span。\n\n对于 Type C 和 Type D 来说，由于不能确定有没有其他 operator 在监听同一个资源，所以无法判断其是否是 leaf span。对于 Type C 可以肯定的是，在 operator 完成最后一次 write 的时候，他仍会收到一个 Event（因为它所修改的资源正是自己监听的资源），并且会 drop 这个 Event（这个 Event 是由自身修改产生的，无意义）。因此，我们可以确定这个 operator 上多次 span 的 parent/child 关系。而对于 Type D，无法收到修改资源的最后一次 write 的 Event，所以，我们只能建立这个 operator 上多次 span 之间较弱的 link 关系。\n\n对于 Type E 来说，这是最复杂但又是最常见的类型。Type E 其实是 Type C 和 Type D 的组合，所以对于 Type E 操作的每个资源，我们可以按照资源的类型，将 Type E 当前的 span 暂时转换成 Type C 或 Type D 来处理。\n\n## 1. Kubernetes、集群联邦和资源分发\n\n原文链接：[https://draveness.me/kuberentes-federation/](https://draveness.me/kuberentes-federation/)\n\nKubernetes 目前最多可以支持管理 5000 个节点，对于超过 5000 个节点的集群管理，就需要寻找其他方法对多个 K8s 集群进行管理。多集群其实不是一个新的概念，在很久之前，就在业界看到过 Mesos + K8s 的多集群管理方法。但是，多集群中的每个集群都相对独立，彼此之间没有联系，每个服务都是独立的运行在一个集群里的。而集群联邦则是在此基础上增加了两个重要的功能：跨集群的服务发现和跨集群的调度，使得一个多应用服务可以运行在多个集群上，进一步提升了服务的稳定性和可用性。\n\n文章中，作者以两个比较出名的集群联邦项目为例，介绍了目前集群联邦的方案：\n\n1. **Kubefed** 会为每个原生资源（e.g. Deployment）生成对应的联邦资源（e.g. FederatedDeployment）作为管理。联邦资源中会包含 Template（资源信息）、Placement（所需部署的集群）和 Overrides（同步资源到集群时，需要覆写的属性）三个部分。在分发到下游集群时，Kubefed 再根据联邦资源生成具体的原生资源。\n\n2. **Karmada** 是 Kubefed 项目的延续，其中的概念也几乎全盘继承自 Kubefed。稍有不同的是，Karmada 保留了原生资源，并将 Kubefed 中联邦资源的 Placement 和 Override 抽离了出来，作为两个新的自定义资源 PropagationPolicy 和 OverriderPolicy。\n\n<figure>\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/0a98b72fdc1acab504726847f894f6f9df8a699e.png\" alt=\"16477814091159-kubefed-karmada-api.png\" data-align=\"center\" width=\"497\">\n</p>\n<figcaption align = \"center\">图片来自原文</figcaption>\n</figure>\n\n对于任务调度来说，文章中提到了“因为上下文的不足，集群联邦不需要也没有办法保证调度的全局最优解，而提供跨集群的部署和故障转移就已经可以满足常见的需求了”。\n\n## 2. A Closer Look at Invalid Action Masking in Policy Gradient Algorithms\n\n原文链接：[https://costa.sh/blog-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms.html](https://costa.sh/blog-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms.html)\n\n本篇文章是作者对所发表的同名论文 [https://arxiv.org/abs/2006.14171](https://arxiv.org/abs/2006.14171) 的介绍。对强化学习稍微有所了解的同学应该都知道 Policy Gradient，属于强化学习的两大分类之一。而 Invalid Action（无效动作）是强化学习中经常遇到的问题，例如，在训练模型走迷宫时，前方有障碍物，那么前进这一动作就是 invalid 的。那么，在训练时，需要对模型过滤掉这类动作，也就是 masking。而本篇文章就在尝试解释 Policy Gradient 算法中 invalid action masking 的工作原理。虽然 masking 在很多论文里都用到了，但都只是一句话带过（我之前读到的几篇甚至不会提到这些细节），没有对 masking 的原理进行深入探索。这也是文章作者的 motivition 之一。\n\n简单来说，invalid action masking 就是在模型根据概率采样动作时，采用一个 mask 将 invalid action 的概率置为 0。文章中作者将 invalid action masking 建模成以下的函数 $inv_s$：\n\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/2022-07-14-21-55-41-image.png\" alt=\"\" data-align=\"center\" width=\"300\">\n</p>\n\n$l(s)$ 是状态 $s$ 的 log 值。$inv_s$ 在两种情况下都是可微的，在常数 $M$ 时，梯度为0，因此，反向传播时不会更新模型有关 invalid action 相关的参数。\n\npolicy 在采样时的概率为：\n\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/2022-07-14-21-56-23-image.png\" alt=\"\" data-align=\"center\" width=\"235\">\n</p>\n\n文章中还通过量化的实验结果来验证 invalid action masking 的有效性，详情可以阅读原文。\n\n## 3. 深度学习新的采样方式和损失函数--论文笔记\n\n原文链接：[https://zhuanlan.zhihu.com/p/27748177](https://zhuanlan.zhihu.com/p/27748177)\n\n本篇文章是对论文《Sampling matters in deep embedding learning》[https://arxiv.org/pdf/1706.07567.pdf](https://arxiv.org/pdf/1706.07567.pdf) 的概述。论文主要解决的是 deep embedding learning 中的采样问题和损失函数问题。文章对论文的主要内容进行了很好的概述，这里就不再赘述了，就简单的罗列一些 insight 和文章中没解释清楚的部分：\n\n1. Triplet loss 优于 Contrastive loss 的原因有两点：\n   \n   1. constrative loss 假设所有样本都符合相同分布，而 triplet loss 没有这个假设。因此，可以适应各种空间形状，一定程度上能够抵御噪声的影响\n   \n   2. triplet loss 优化的目标是正负样本之间的相对距离差，即正样本之间的距离小于正负样本之间的距离。而 constrative loss 优化的目标是绝对距离，即所有的正样本之间的距离也要尽可能小。这是没有必要的。\n\n2. 假设负样本均匀分布，我们也均匀随机采样。那么，采样的负样本 pairwise distance 符合如下的分布：\n   \n   <p style=\"text-align: center\">\n   <img title=\"test\" src=\"/asset/AwesomeTechPost0/fdad5d725c64c0b589b8d107e5e228d285497474.png\" alt=\"\" data-align=\"center\" width=\"318\">\n   </p>\n   \n   换句话说，在高维空间里，采样得到的负样本 pairwise distance 基本上都是大于 $\\sqrt{2}$ 的。论文针对这个问题，提出的方法是 distance weighted sampling。以距离概率值的倒数 $q(d)^{-1}$ 作为样本采样的权重，这样在修正样本距离分布的 bias 的同时控制了 variance。\n\n3. Triplet loss 采用的是一种 hard negtive mining 的方法，也就是正负样本的区分是 hard 的。负样本的梯度通过如下的公式计算：\n\n   <p style=\"text-align: center\">\n   <img src=\"/asset/AwesomeTechPost0/f5026649684b309b85e2f5880e0a84bfdd7c01de.png\" title=\"\" alt=\"v2-966f53117397f560a6395fef136ca5f8_1440w.png\" data-align=\"center\">\n   </p>\n   \n   梯度的方向取决于 $h_{an}$ ，即 anchor 样本与负样本的向量差。那么，如果差向量的绝对值特别小，并且这个负样本是异常值，那对模型的梯度会造成很大的影响。\n   \n   <p style=\"text-align: center\">\n   <img src=\"/asset/AwesomeTechPost0/bd74f1ae5c04b74ee412affefedd54fceffb790d.png\" title=\"\" alt=\"v2-80e1c0da65e11100f88b6175857a79ba_1440w.png\" data-align=\"center\">\n   </p>\n   \n   上图中展示了随着 pairwise distance 的增加，各种 loss 中正负样本是如何变化的。蓝色实线是正样本，绿色虚线是负样本。对于图b，$D_{an}$ 越小，梯度值也越趋向于 0。根据 triplet loss 的计算公式，我们可以发现，这导致了模型趋向于这个点的梯度（正）会很大，但是远离这个点的梯度（负）很小。论文提出了 margin based loss 来解决这一问题，即图 d 中的 loss。在 $D_{an}$ 很小的时候，仍然保证了负样本的梯度为一个常数，这有点类似 ReLU 的思想。\n\n## 4. 如何在一个全新的领域展开学习\n\n原文链接：[https://ichn.xyz/blog/how-to-start-learning-in-a-new-area](https://ichn.xyz/blog/how-to-start-learning-in-a-new-area)\n\n学习计算机最重要的点在于关注能力的成长。而所有能力中最重要的，莫过于学习的能力，学习能力是培养其他能力的元能力。文章作者在接触了大量计算机细分领域后，总结出了几点特定的套路：\n\n1. 明确动机\n   \n   要有明确且实际的需求。一是可以解决兴趣使然导致的选择困难，二是在学习的过程中，感受到切实的正反馈。\n\n2. 背景调查\n   \n   一旦明确了学习的动机和目标，应该更系统性地、刻意地对这个领域展开背景调查。\n   \n   > 知道即将学习的知识可以解决什么样的问题，这种解决手段和其他方式相比的优劣，这个领域和其他领域、特别是自己已经熟悉的领域的关系是怎样的，这个领域的发展历史和发展脉络是怎样的，有哪些独特且重要的概念？\n   > \n   > 背景调查获取的信息通常是宏观或者碎片化的，这并不是真正的学习。但这个过程可以提高你对这个领域的熟悉程度，在你的话语体系和思考方式中加入这个领域的成分，并提高你对这个领域的品味与认知。\n\n3. 资源汇集与整理\n   \n   了解这个领域有哪些重要的资料，更重要的是会拥有判断这个方向的学习资料的优劣的能力。\n\n4. 制定计划，然后无情地执行\n   \n   如果执行学习计划中会有枯燥的感觉，就需要回顾自己的动机、目标，并稍稍跳出来重新审视一下学习计划。\n   \n   如果确认学习路径的正确性，就应该专注，而不是继续在这个领域中漫无目的的探索，这样才能进入深水区。\n","slug":"awesome-tech-post-0","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clapldfd5000918mze4wmhrr4","content":"<p><img src=\"/asset/AwesomeTechPost0/2022-06-30-00-00-42-image.png\" alt=\"\" /></p>\n<p>每期 Awesome Tech Post 都会摘录推荐 5 篇优质技术博客，对这些文章的内容进行提炼总结。每期覆盖领域各不相同，可能从后端到可视化，从工程到算法等等，但每篇文章都会对领域内的某一问题进行深入分析或提出独到见解。欢迎大家私信推荐文章~</p>\n<h2 id=\"tldr\"><a class=\"markdownIt-Anchor\" href=\"#tldr\"></a> TL;DR</h2>\n<p>本期内容：</p>\n<ol>\n<li>\n<p>如何对 Kubernetes Operator 进行分布式 Tracing <code>Cloud</code></p>\n</li>\n<li>\n<p>Kubernetes 多集群管理与联邦 <code>Cloud</code></p>\n</li>\n<li>\n<p>强化学习中对无效动作的 mask <code>Reinforcement Learning</code></p>\n</li>\n<li>\n<p>一些深度学习中的采样方式和损失函数 <code>Deep Learning</code></p>\n</li>\n<li>\n<p>如何在一个全新的领域开展学习 <code>Soft Skill</code></p>\n</li>\n</ol>\n<p><strong>对于感兴趣的内容，可以阅读对应的博客原文查看更多详情~</strong></p>\n<h2 id=\"0-how-to-monitor-kubernetes-operators-by-distributed-tracing\"><a class=\"markdownIt-Anchor\" href=\"#0-how-to-monitor-kubernetes-operators-by-distributed-tracing\"></a> 0. How to Monitor Kubernetes Operators By Distributed-Tracing?</h2>\n<p>原文链接：<a href=\"https://yue9944882.github.io/posts/how-to-monitor-kubernetes-operator-by-distributed-tracing/\">https://yue9944882.github.io/posts/how-to-monitor-kubernetes-operator-by-distributed-tracing/</a></p>\n<p>在一个健全的系统中，应当对一条请求在完整生命周期中完成了哪些处理都进行监控追踪。随着现在大量的分布式应用和微服务的落地，一条请求可能跨越多个服务，甚至集群。对于这类请求的 tracing（追踪）问题就是 distributed tracing。Tracing 的整个流程可以被建模成一个树，其中每个节点是请求所经过的处理（根据监控的粒度，可以是一个服务，也可以是一个函数）。请求经过的每个处理被称作为 span。</p>\n<h3 id=\"异步问题\"><a class=\"markdownIt-Anchor\" href=\"#异步问题\"></a> 异步问题</h3>\n<p>在微服务中，一条请求 R 到达微服务1后，一般通过 HTTP 或 RPC，进一步请求到微服务2进行处理，再到微服务N。最后，沿着这一条链，进行反向的返回 response。显然，这是一个同步的过程，我们可以很清晰的看出 R 的处理流程，R 的 tracing 结果就是这条转发树。</p>\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/2022-07-12-22-28-28-image.png\" alt=\"\" data-align=\"center\" width=\"764\">\n</p>\n<p>然而，与微服务不同，k8s operator 采用了完全不同的协作模式。K8s operator 不会与其他 operator 直接进行交互，而是生成一个 Event（例如，创建一个 Pod），k8s 会将这个 Event 放入到一个队列中。所有的 operator 都不断地轮询，从这个队列中获取符合自己过滤条件的 Event。显然，这是一个异步的过程。一异步，问题就麻烦了：</p>\n<ol>\n<li>\n<p>Event 的生成/消费不是线性的</p>\n<p>Event 的生成和消费不是一一对应的。一方面，operator 可能将多个 Event 合并成一个任务，或者将一个 Event 分成多个任务。另一方面，一个任务由于重试策略可能会执行多次（在一个 operator 上产生多个 span）。</p>\n</li>\n<li>\n<p>Event 循环</p>\n<p>当一个 operator 根据 Event 改变 k8s 的资源后，又会产生一个新的 Event（k8s 中有资源被改变）。这就提出了一个问题，什么是这次 tracing 的结束？详细的讨论可以参考原文。</p>\n</li>\n</ol>\n<h3 id=\"operator-分类\"><a class=\"markdownIt-Anchor\" href=\"#operator-分类\"></a> Operator 分类</h3>\n<p>每个 operator 都可能监听一个主要的 resource 和多个次要的 resource，因此，可以对 operator 进行如下的分类：</p>\n<ol>\n<li>\n<p>Type A：operator 只接收 Event。</p>\n</li>\n<li>\n<p>Type B：operator 接收 Event，对 k8s 外的系统进行操作。</p>\n</li>\n<li>\n<p>Type C：operator 只修改自己监听的资源。</p>\n</li>\n<li>\n<p>Type D：operator 只修改不被自己监听的资源。</p>\n</li>\n<li>\n<p>Type E：operator 修改任意的资源（Type C + Type D）。</p>\n</li>\n</ol>\n<p>对于 Type A 和 Type B 来说，请求到他们这里就结束了，所以他们是 leaf span。</p>\n<p>对于 Type C 和 Type D 来说，由于不能确定有没有其他 operator 在监听同一个资源，所以无法判断其是否是 leaf span。对于 Type C 可以肯定的是，在 operator 完成最后一次 write 的时候，他仍会收到一个 Event（因为它所修改的资源正是自己监听的资源），并且会 drop 这个 Event（这个 Event 是由自身修改产生的，无意义）。因此，我们可以确定这个 operator 上多次 span 的 parent/child 关系。而对于 Type D，无法收到修改资源的最后一次 write 的 Event，所以，我们只能建立这个 operator 上多次 span 之间较弱的 link 关系。</p>\n<p>对于 Type E 来说，这是最复杂但又是最常见的类型。Type E 其实是 Type C 和 Type D 的组合，所以对于 Type E 操作的每个资源，我们可以按照资源的类型，将 Type E 当前的 span 暂时转换成 Type C 或 Type D 来处理。</p>\n<h2 id=\"1-kubernetes-集群联邦和资源分发\"><a class=\"markdownIt-Anchor\" href=\"#1-kubernetes-集群联邦和资源分发\"></a> 1. Kubernetes、集群联邦和资源分发</h2>\n<p>原文链接：<a href=\"https://draveness.me/kuberentes-federation/\">https://draveness.me/kuberentes-federation/</a></p>\n<p>Kubernetes 目前最多可以支持管理 5000 个节点，对于超过 5000 个节点的集群管理，就需要寻找其他方法对多个 K8s 集群进行管理。多集群其实不是一个新的概念，在很久之前，就在业界看到过 Mesos + K8s 的多集群管理方法。但是，多集群中的每个集群都相对独立，彼此之间没有联系，每个服务都是独立的运行在一个集群里的。而集群联邦则是在此基础上增加了两个重要的功能：跨集群的服务发现和跨集群的调度，使得一个多应用服务可以运行在多个集群上，进一步提升了服务的稳定性和可用性。</p>\n<p>文章中，作者以两个比较出名的集群联邦项目为例，介绍了目前集群联邦的方案：</p>\n<ol>\n<li>\n<p><strong>Kubefed</strong> 会为每个原生资源（e.g. Deployment）生成对应的联邦资源（e.g. FederatedDeployment）作为管理。联邦资源中会包含 Template（资源信息）、Placement（所需部署的集群）和 Overrides（同步资源到集群时，需要覆写的属性）三个部分。在分发到下游集群时，Kubefed 再根据联邦资源生成具体的原生资源。</p>\n</li>\n<li>\n<p><strong>Karmada</strong> 是 Kubefed 项目的延续，其中的概念也几乎全盘继承自 Kubefed。稍有不同的是，Karmada 保留了原生资源，并将 Kubefed 中联邦资源的 Placement 和 Override 抽离了出来，作为两个新的自定义资源 PropagationPolicy 和 OverriderPolicy。</p>\n</li>\n</ol>\n<figure>\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/0a98b72fdc1acab504726847f894f6f9df8a699e.png\" alt=\"16477814091159-kubefed-karmada-api.png\" data-align=\"center\" width=\"497\">\n</p>\n<figcaption align = \"center\">图片来自原文</figcaption>\n</figure>\n<p>对于任务调度来说，文章中提到了“因为上下文的不足，集群联邦不需要也没有办法保证调度的全局最优解，而提供跨集群的部署和故障转移就已经可以满足常见的需求了”。</p>\n<h2 id=\"2-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms\"><a class=\"markdownIt-Anchor\" href=\"#2-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms\"></a> 2. A Closer Look at Invalid Action Masking in Policy Gradient Algorithms</h2>\n<p>原文链接：<a href=\"https://costa.sh/blog-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms.html\">https://costa.sh/blog-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms.html</a></p>\n<p>本篇文章是作者对所发表的同名论文 <a href=\"https://arxiv.org/abs/2006.14171\">https://arxiv.org/abs/2006.14171</a> 的介绍。对强化学习稍微有所了解的同学应该都知道 Policy Gradient，属于强化学习的两大分类之一。而 Invalid Action（无效动作）是强化学习中经常遇到的问题，例如，在训练模型走迷宫时，前方有障碍物，那么前进这一动作就是 invalid 的。那么，在训练时，需要对模型过滤掉这类动作，也就是 masking。而本篇文章就在尝试解释 Policy Gradient 算法中 invalid action masking 的工作原理。虽然 masking 在很多论文里都用到了，但都只是一句话带过（我之前读到的几篇甚至不会提到这些细节），没有对 masking 的原理进行深入探索。这也是文章作者的 motivition 之一。</p>\n<p>简单来说，invalid action masking 就是在模型根据概率采样动作时，采用一个 mask 将 invalid action 的概率置为 0。文章中作者将 invalid action masking 建模成以下的函数 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi><mi>n</mi><msub><mi>v</mi><mi>s</mi></msub></mrow><annotation encoding=\"application/x-tex\">inv_s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.80952em;vertical-align:-0.15em;\"></span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">n</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">s</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>：</p>\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/2022-07-14-21-55-41-image.png\" alt=\"\" data-align=\"center\" width=\"300\">\n</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>l</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">l(s)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span></span></span></span> 是状态 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span> 的 log 值。<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi><mi>n</mi><msub><mi>v</mi><mi>s</mi></msub></mrow><annotation encoding=\"application/x-tex\">inv_s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.80952em;vertical-align:-0.15em;\"></span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">n</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">s</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 在两种情况下都是可微的，在常数 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span> 时，梯度为0，因此，反向传播时不会更新模型有关 invalid action 相关的参数。</p>\n<p>policy 在采样时的概率为：</p>\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/2022-07-14-21-56-23-image.png\" alt=\"\" data-align=\"center\" width=\"235\">\n</p>\n<p>文章中还通过量化的实验结果来验证 invalid action masking 的有效性，详情可以阅读原文。</p>\n<h2 id=\"3-深度学习新的采样方式和损失函数论文笔记\"><a class=\"markdownIt-Anchor\" href=\"#3-深度学习新的采样方式和损失函数论文笔记\"></a> 3. 深度学习新的采样方式和损失函数–论文笔记</h2>\n<p>原文链接：<a href=\"https://zhuanlan.zhihu.com/p/27748177\">https://zhuanlan.zhihu.com/p/27748177</a></p>\n<p>本篇文章是对论文《Sampling matters in deep embedding learning》<a href=\"https://arxiv.org/pdf/1706.07567.pdf\">https://arxiv.org/pdf/1706.07567.pdf</a> 的概述。论文主要解决的是 deep embedding learning 中的采样问题和损失函数问题。文章对论文的主要内容进行了很好的概述，这里就不再赘述了，就简单的罗列一些 insight 和文章中没解释清楚的部分：</p>\n<ol>\n<li>\n<p>Triplet loss 优于 Contrastive loss 的原因有两点：</p>\n<ol>\n<li>\n<p>constrative loss 假设所有样本都符合相同分布，而 triplet loss 没有这个假设。因此，可以适应各种空间形状，一定程度上能够抵御噪声的影响</p>\n</li>\n<li>\n<p>triplet loss 优化的目标是正负样本之间的相对距离差，即正样本之间的距离小于正负样本之间的距离。而 constrative loss 优化的目标是绝对距离，即所有的正样本之间的距离也要尽可能小。这是没有必要的。</p>\n</li>\n</ol>\n</li>\n<li>\n<p>假设负样本均匀分布，我们也均匀随机采样。那么，采样的负样本 pairwise distance 符合如下的分布：</p>\n<p style=\"text-align: center\">\n<img title=\"test\" src=\"/asset/AwesomeTechPost0/fdad5d725c64c0b589b8d107e5e228d285497474.png\" alt=\"\" data-align=\"center\" width=\"318\">\n</p>\n<p>换句话说，在高维空间里，采样得到的负样本 pairwise distance 基本上都是大于 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msqrt><mn>2</mn></msqrt></mrow><annotation encoding=\"application/x-tex\">\\sqrt{2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.04em;vertical-align:-0.13278em;\"></span><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.90722em;\"><span class=\"svg-align\" style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\" style=\"padding-left:0.833em;\"><span class=\"mord\">2</span></span></span><span style=\"top:-2.86722em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.13278em;\"><span></span></span></span></span></span></span></span></span> 的。论文针对这个问题，提出的方法是 distance weighted sampling。以距离概率值的倒数 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>q</mi><mo stretchy=\"false\">(</mo><mi>d</mi><msup><mo stretchy=\"false\">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">q(d)^{-1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.064108em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">d</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span></span></span></span> 作为样本采样的权重，这样在修正样本距离分布的 bias 的同时控制了 variance。</p>\n</li>\n<li>\n<p>Triplet loss 采用的是一种 hard negtive mining 的方法，也就是正负样本的区分是 hard 的。负样本的梯度通过如下的公式计算：</p>\n<p style=\"text-align: center\">\n<img src=\"/asset/AwesomeTechPost0/f5026649684b309b85e2f5880e0a84bfdd7c01de.png\" title=\"\" alt=\"v2-966f53117397f560a6395fef136ca5f8_1440w.png\" data-align=\"center\">\n</p>\n<p>梯度的方向取决于 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>h</mi><mrow><mi>a</mi><mi>n</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">h_{an}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">h</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> ，即 anchor 样本与负样本的向量差。那么，如果差向量的绝对值特别小，并且这个负样本是异常值，那对模型的梯度会造成很大的影响。</p>\n<p style=\"text-align: center\">\n<img src=\"/asset/AwesomeTechPost0/bd74f1ae5c04b74ee412affefedd54fceffb790d.png\" title=\"\" alt=\"v2-80e1c0da65e11100f88b6175857a79ba_1440w.png\" data-align=\"center\">\n</p>\n<p>上图中展示了随着 pairwise distance 的增加，各种 loss 中正负样本是如何变化的。蓝色实线是正样本，绿色虚线是负样本。对于图b，<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>D</mi><mrow><mi>a</mi><mi>n</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">D_{an}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 越小，梯度值也越趋向于 0。根据 triplet loss 的计算公式，我们可以发现，这导致了模型趋向于这个点的梯度（正）会很大，但是远离这个点的梯度（负）很小。论文提出了 margin based loss 来解决这一问题，即图 d 中的 loss。在 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>D</mi><mrow><mi>a</mi><mi>n</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">D_{an}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 很小的时候，仍然保证了负样本的梯度为一个常数，这有点类似 ReLU 的思想。</p>\n</li>\n</ol>\n<h2 id=\"4-如何在一个全新的领域展开学习\"><a class=\"markdownIt-Anchor\" href=\"#4-如何在一个全新的领域展开学习\"></a> 4. 如何在一个全新的领域展开学习</h2>\n<p>原文链接：<a href=\"https://ichn.xyz/blog/how-to-start-learning-in-a-new-area\">https://ichn.xyz/blog/how-to-start-learning-in-a-new-area</a></p>\n<p>学习计算机最重要的点在于关注能力的成长。而所有能力中最重要的，莫过于学习的能力，学习能力是培养其他能力的元能力。文章作者在接触了大量计算机细分领域后，总结出了几点特定的套路：</p>\n<ol>\n<li>\n<p>明确动机</p>\n<p>要有明确且实际的需求。一是可以解决兴趣使然导致的选择困难，二是在学习的过程中，感受到切实的正反馈。</p>\n</li>\n<li>\n<p>背景调查</p>\n<p>一旦明确了学习的动机和目标，应该更系统性地、刻意地对这个领域展开背景调查。</p>\n<blockquote>\n<p>知道即将学习的知识可以解决什么样的问题，这种解决手段和其他方式相比的优劣，这个领域和其他领域、特别是自己已经熟悉的领域的关系是怎样的，这个领域的发展历史和发展脉络是怎样的，有哪些独特且重要的概念？</p>\n<p>背景调查获取的信息通常是宏观或者碎片化的，这并不是真正的学习。但这个过程可以提高你对这个领域的熟悉程度，在你的话语体系和思考方式中加入这个领域的成分，并提高你对这个领域的品味与认知。</p>\n</blockquote>\n</li>\n<li>\n<p>资源汇集与整理</p>\n<p>了解这个领域有哪些重要的资料，更重要的是会拥有判断这个方向的学习资料的优劣的能力。</p>\n</li>\n<li>\n<p>制定计划，然后无情地执行</p>\n<p>如果执行学习计划中会有枯燥的感觉，就需要回顾自己的动机、目标，并稍稍跳出来重新审视一下学习计划。</p>\n<p>如果确认学习路径的正确性，就应该专注，而不是继续在这个领域中漫无目的的探索，这样才能进入深水区。</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"/asset/AwesomeTechPost0/2022-06-30-00-00-42-image.png\" alt=\"\" /></p>\n<p>每期 Awesome Tech Post 都会摘录推荐 5 篇优质技术博客，对这些文章的内容进行提炼总结。每期覆盖领域各不相同，可能从后端到可视化，从工程到算法等等，但每篇文章都会对领域内的某一问题进行深入分析或提出独到见解。欢迎大家私信推荐文章~</p>\n<h2 id=\"tldr\"><a class=\"markdownIt-Anchor\" href=\"#tldr\"></a> TL;DR</h2>\n<p>本期内容：</p>\n<ol>\n<li>\n<p>如何对 Kubernetes Operator 进行分布式 Tracing <code>Cloud</code></p>\n</li>\n<li>\n<p>Kubernetes 多集群管理与联邦 <code>Cloud</code></p>\n</li>\n<li>\n<p>强化学习中对无效动作的 mask <code>Reinforcement Learning</code></p>\n</li>\n<li>\n<p>一些深度学习中的采样方式和损失函数 <code>Deep Learning</code></p>\n</li>\n<li>\n<p>如何在一个全新的领域开展学习 <code>Soft Skill</code></p>\n</li>\n</ol>\n<p><strong>对于感兴趣的内容，可以阅读对应的博客原文查看更多详情~</strong></p>\n<h2 id=\"0-how-to-monitor-kubernetes-operators-by-distributed-tracing\"><a class=\"markdownIt-Anchor\" href=\"#0-how-to-monitor-kubernetes-operators-by-distributed-tracing\"></a> 0. How to Monitor Kubernetes Operators By Distributed-Tracing?</h2>\n<p>原文链接：<a href=\"https://yue9944882.github.io/posts/how-to-monitor-kubernetes-operator-by-distributed-tracing/\">https://yue9944882.github.io/posts/how-to-monitor-kubernetes-operator-by-distributed-tracing/</a></p>\n<p>在一个健全的系统中，应当对一条请求在完整生命周期中完成了哪些处理都进行监控追踪。随着现在大量的分布式应用和微服务的落地，一条请求可能跨越多个服务，甚至集群。对于这类请求的 tracing（追踪）问题就是 distributed tracing。Tracing 的整个流程可以被建模成一个树，其中每个节点是请求所经过的处理（根据监控的粒度，可以是一个服务，也可以是一个函数）。请求经过的每个处理被称作为 span。</p>\n<h3 id=\"异步问题\"><a class=\"markdownIt-Anchor\" href=\"#异步问题\"></a> 异步问题</h3>\n<p>在微服务中，一条请求 R 到达微服务1后，一般通过 HTTP 或 RPC，进一步请求到微服务2进行处理，再到微服务N。最后，沿着这一条链，进行反向的返回 response。显然，这是一个同步的过程，我们可以很清晰的看出 R 的处理流程，R 的 tracing 结果就是这条转发树。</p>\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/2022-07-12-22-28-28-image.png\" alt=\"\" data-align=\"center\" width=\"764\">\n</p>\n<p>然而，与微服务不同，k8s operator 采用了完全不同的协作模式。K8s operator 不会与其他 operator 直接进行交互，而是生成一个 Event（例如，创建一个 Pod），k8s 会将这个 Event 放入到一个队列中。所有的 operator 都不断地轮询，从这个队列中获取符合自己过滤条件的 Event。显然，这是一个异步的过程。一异步，问题就麻烦了：</p>\n<ol>\n<li>\n<p>Event 的生成/消费不是线性的</p>\n<p>Event 的生成和消费不是一一对应的。一方面，operator 可能将多个 Event 合并成一个任务，或者将一个 Event 分成多个任务。另一方面，一个任务由于重试策略可能会执行多次（在一个 operator 上产生多个 span）。</p>\n</li>\n<li>\n<p>Event 循环</p>\n<p>当一个 operator 根据 Event 改变 k8s 的资源后，又会产生一个新的 Event（k8s 中有资源被改变）。这就提出了一个问题，什么是这次 tracing 的结束？详细的讨论可以参考原文。</p>\n</li>\n</ol>\n<h3 id=\"operator-分类\"><a class=\"markdownIt-Anchor\" href=\"#operator-分类\"></a> Operator 分类</h3>\n<p>每个 operator 都可能监听一个主要的 resource 和多个次要的 resource，因此，可以对 operator 进行如下的分类：</p>\n<ol>\n<li>\n<p>Type A：operator 只接收 Event。</p>\n</li>\n<li>\n<p>Type B：operator 接收 Event，对 k8s 外的系统进行操作。</p>\n</li>\n<li>\n<p>Type C：operator 只修改自己监听的资源。</p>\n</li>\n<li>\n<p>Type D：operator 只修改不被自己监听的资源。</p>\n</li>\n<li>\n<p>Type E：operator 修改任意的资源（Type C + Type D）。</p>\n</li>\n</ol>\n<p>对于 Type A 和 Type B 来说，请求到他们这里就结束了，所以他们是 leaf span。</p>\n<p>对于 Type C 和 Type D 来说，由于不能确定有没有其他 operator 在监听同一个资源，所以无法判断其是否是 leaf span。对于 Type C 可以肯定的是，在 operator 完成最后一次 write 的时候，他仍会收到一个 Event（因为它所修改的资源正是自己监听的资源），并且会 drop 这个 Event（这个 Event 是由自身修改产生的，无意义）。因此，我们可以确定这个 operator 上多次 span 的 parent/child 关系。而对于 Type D，无法收到修改资源的最后一次 write 的 Event，所以，我们只能建立这个 operator 上多次 span 之间较弱的 link 关系。</p>\n<p>对于 Type E 来说，这是最复杂但又是最常见的类型。Type E 其实是 Type C 和 Type D 的组合，所以对于 Type E 操作的每个资源，我们可以按照资源的类型，将 Type E 当前的 span 暂时转换成 Type C 或 Type D 来处理。</p>\n<h2 id=\"1-kubernetes-集群联邦和资源分发\"><a class=\"markdownIt-Anchor\" href=\"#1-kubernetes-集群联邦和资源分发\"></a> 1. Kubernetes、集群联邦和资源分发</h2>\n<p>原文链接：<a href=\"https://draveness.me/kuberentes-federation/\">https://draveness.me/kuberentes-federation/</a></p>\n<p>Kubernetes 目前最多可以支持管理 5000 个节点，对于超过 5000 个节点的集群管理，就需要寻找其他方法对多个 K8s 集群进行管理。多集群其实不是一个新的概念，在很久之前，就在业界看到过 Mesos + K8s 的多集群管理方法。但是，多集群中的每个集群都相对独立，彼此之间没有联系，每个服务都是独立的运行在一个集群里的。而集群联邦则是在此基础上增加了两个重要的功能：跨集群的服务发现和跨集群的调度，使得一个多应用服务可以运行在多个集群上，进一步提升了服务的稳定性和可用性。</p>\n<p>文章中，作者以两个比较出名的集群联邦项目为例，介绍了目前集群联邦的方案：</p>\n<ol>\n<li>\n<p><strong>Kubefed</strong> 会为每个原生资源（e.g. Deployment）生成对应的联邦资源（e.g. FederatedDeployment）作为管理。联邦资源中会包含 Template（资源信息）、Placement（所需部署的集群）和 Overrides（同步资源到集群时，需要覆写的属性）三个部分。在分发到下游集群时，Kubefed 再根据联邦资源生成具体的原生资源。</p>\n</li>\n<li>\n<p><strong>Karmada</strong> 是 Kubefed 项目的延续，其中的概念也几乎全盘继承自 Kubefed。稍有不同的是，Karmada 保留了原生资源，并将 Kubefed 中联邦资源的 Placement 和 Override 抽离了出来，作为两个新的自定义资源 PropagationPolicy 和 OverriderPolicy。</p>\n</li>\n</ol>\n<figure>\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/0a98b72fdc1acab504726847f894f6f9df8a699e.png\" alt=\"16477814091159-kubefed-karmada-api.png\" data-align=\"center\" width=\"497\">\n</p>\n<figcaption align = \"center\">图片来自原文</figcaption>\n</figure>\n<p>对于任务调度来说，文章中提到了“因为上下文的不足，集群联邦不需要也没有办法保证调度的全局最优解，而提供跨集群的部署和故障转移就已经可以满足常见的需求了”。</p>\n<h2 id=\"2-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms\"><a class=\"markdownIt-Anchor\" href=\"#2-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms\"></a> 2. A Closer Look at Invalid Action Masking in Policy Gradient Algorithms</h2>\n<p>原文链接：<a href=\"https://costa.sh/blog-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms.html\">https://costa.sh/blog-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms.html</a></p>\n<p>本篇文章是作者对所发表的同名论文 <a href=\"https://arxiv.org/abs/2006.14171\">https://arxiv.org/abs/2006.14171</a> 的介绍。对强化学习稍微有所了解的同学应该都知道 Policy Gradient，属于强化学习的两大分类之一。而 Invalid Action（无效动作）是强化学习中经常遇到的问题，例如，在训练模型走迷宫时，前方有障碍物，那么前进这一动作就是 invalid 的。那么，在训练时，需要对模型过滤掉这类动作，也就是 masking。而本篇文章就在尝试解释 Policy Gradient 算法中 invalid action masking 的工作原理。虽然 masking 在很多论文里都用到了，但都只是一句话带过（我之前读到的几篇甚至不会提到这些细节），没有对 masking 的原理进行深入探索。这也是文章作者的 motivition 之一。</p>\n<p>简单来说，invalid action masking 就是在模型根据概率采样动作时，采用一个 mask 将 invalid action 的概率置为 0。文章中作者将 invalid action masking 建模成以下的函数 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi><mi>n</mi><msub><mi>v</mi><mi>s</mi></msub></mrow><annotation encoding=\"application/x-tex\">inv_s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.80952em;vertical-align:-0.15em;\"></span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">n</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">s</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>：</p>\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/2022-07-14-21-55-41-image.png\" alt=\"\" data-align=\"center\" width=\"300\">\n</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>l</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">l(s)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span></span></span></span> 是状态 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span> 的 log 值。<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi><mi>n</mi><msub><mi>v</mi><mi>s</mi></msub></mrow><annotation encoding=\"application/x-tex\">inv_s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.80952em;vertical-align:-0.15em;\"></span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">n</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">s</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 在两种情况下都是可微的，在常数 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span> 时，梯度为0，因此，反向传播时不会更新模型有关 invalid action 相关的参数。</p>\n<p>policy 在采样时的概率为：</p>\n<p style=\"text-align: center\">\n<img title=\"\" src=\"/asset/AwesomeTechPost0/2022-07-14-21-56-23-image.png\" alt=\"\" data-align=\"center\" width=\"235\">\n</p>\n<p>文章中还通过量化的实验结果来验证 invalid action masking 的有效性，详情可以阅读原文。</p>\n<h2 id=\"3-深度学习新的采样方式和损失函数论文笔记\"><a class=\"markdownIt-Anchor\" href=\"#3-深度学习新的采样方式和损失函数论文笔记\"></a> 3. 深度学习新的采样方式和损失函数–论文笔记</h2>\n<p>原文链接：<a href=\"https://zhuanlan.zhihu.com/p/27748177\">https://zhuanlan.zhihu.com/p/27748177</a></p>\n<p>本篇文章是对论文《Sampling matters in deep embedding learning》<a href=\"https://arxiv.org/pdf/1706.07567.pdf\">https://arxiv.org/pdf/1706.07567.pdf</a> 的概述。论文主要解决的是 deep embedding learning 中的采样问题和损失函数问题。文章对论文的主要内容进行了很好的概述，这里就不再赘述了，就简单的罗列一些 insight 和文章中没解释清楚的部分：</p>\n<ol>\n<li>\n<p>Triplet loss 优于 Contrastive loss 的原因有两点：</p>\n<ol>\n<li>\n<p>constrative loss 假设所有样本都符合相同分布，而 triplet loss 没有这个假设。因此，可以适应各种空间形状，一定程度上能够抵御噪声的影响</p>\n</li>\n<li>\n<p>triplet loss 优化的目标是正负样本之间的相对距离差，即正样本之间的距离小于正负样本之间的距离。而 constrative loss 优化的目标是绝对距离，即所有的正样本之间的距离也要尽可能小。这是没有必要的。</p>\n</li>\n</ol>\n</li>\n<li>\n<p>假设负样本均匀分布，我们也均匀随机采样。那么，采样的负样本 pairwise distance 符合如下的分布：</p>\n<p style=\"text-align: center\">\n<img title=\"test\" src=\"/asset/AwesomeTechPost0/fdad5d725c64c0b589b8d107e5e228d285497474.png\" alt=\"\" data-align=\"center\" width=\"318\">\n</p>\n<p>换句话说，在高维空间里，采样得到的负样本 pairwise distance 基本上都是大于 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msqrt><mn>2</mn></msqrt></mrow><annotation encoding=\"application/x-tex\">\\sqrt{2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.04em;vertical-align:-0.13278em;\"></span><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.90722em;\"><span class=\"svg-align\" style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\" style=\"padding-left:0.833em;\"><span class=\"mord\">2</span></span></span><span style=\"top:-2.86722em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.13278em;\"><span></span></span></span></span></span></span></span></span> 的。论文针对这个问题，提出的方法是 distance weighted sampling。以距离概率值的倒数 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>q</mi><mo stretchy=\"false\">(</mo><mi>d</mi><msup><mo stretchy=\"false\">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">q(d)^{-1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.064108em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">d</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span></span></span></span> 作为样本采样的权重，这样在修正样本距离分布的 bias 的同时控制了 variance。</p>\n</li>\n<li>\n<p>Triplet loss 采用的是一种 hard negtive mining 的方法，也就是正负样本的区分是 hard 的。负样本的梯度通过如下的公式计算：</p>\n<p style=\"text-align: center\">\n<img src=\"/asset/AwesomeTechPost0/f5026649684b309b85e2f5880e0a84bfdd7c01de.png\" title=\"\" alt=\"v2-966f53117397f560a6395fef136ca5f8_1440w.png\" data-align=\"center\">\n</p>\n<p>梯度的方向取决于 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>h</mi><mrow><mi>a</mi><mi>n</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">h_{an}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">h</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> ，即 anchor 样本与负样本的向量差。那么，如果差向量的绝对值特别小，并且这个负样本是异常值，那对模型的梯度会造成很大的影响。</p>\n<p style=\"text-align: center\">\n<img src=\"/asset/AwesomeTechPost0/bd74f1ae5c04b74ee412affefedd54fceffb790d.png\" title=\"\" alt=\"v2-80e1c0da65e11100f88b6175857a79ba_1440w.png\" data-align=\"center\">\n</p>\n<p>上图中展示了随着 pairwise distance 的增加，各种 loss 中正负样本是如何变化的。蓝色实线是正样本，绿色虚线是负样本。对于图b，<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>D</mi><mrow><mi>a</mi><mi>n</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">D_{an}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 越小，梯度值也越趋向于 0。根据 triplet loss 的计算公式，我们可以发现，这导致了模型趋向于这个点的梯度（正）会很大，但是远离这个点的梯度（负）很小。论文提出了 margin based loss 来解决这一问题，即图 d 中的 loss。在 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>D</mi><mrow><mi>a</mi><mi>n</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">D_{an}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 很小的时候，仍然保证了负样本的梯度为一个常数，这有点类似 ReLU 的思想。</p>\n</li>\n</ol>\n<h2 id=\"4-如何在一个全新的领域展开学习\"><a class=\"markdownIt-Anchor\" href=\"#4-如何在一个全新的领域展开学习\"></a> 4. 如何在一个全新的领域展开学习</h2>\n<p>原文链接：<a href=\"https://ichn.xyz/blog/how-to-start-learning-in-a-new-area\">https://ichn.xyz/blog/how-to-start-learning-in-a-new-area</a></p>\n<p>学习计算机最重要的点在于关注能力的成长。而所有能力中最重要的，莫过于学习的能力，学习能力是培养其他能力的元能力。文章作者在接触了大量计算机细分领域后，总结出了几点特定的套路：</p>\n<ol>\n<li>\n<p>明确动机</p>\n<p>要有明确且实际的需求。一是可以解决兴趣使然导致的选择困难，二是在学习的过程中，感受到切实的正反馈。</p>\n</li>\n<li>\n<p>背景调查</p>\n<p>一旦明确了学习的动机和目标，应该更系统性地、刻意地对这个领域展开背景调查。</p>\n<blockquote>\n<p>知道即将学习的知识可以解决什么样的问题，这种解决手段和其他方式相比的优劣，这个领域和其他领域、特别是自己已经熟悉的领域的关系是怎样的，这个领域的发展历史和发展脉络是怎样的，有哪些独特且重要的概念？</p>\n<p>背景调查获取的信息通常是宏观或者碎片化的，这并不是真正的学习。但这个过程可以提高你对这个领域的熟悉程度，在你的话语体系和思考方式中加入这个领域的成分，并提高你对这个领域的品味与认知。</p>\n</blockquote>\n</li>\n<li>\n<p>资源汇集与整理</p>\n<p>了解这个领域有哪些重要的资料，更重要的是会拥有判断这个方向的学习资料的优劣的能力。</p>\n</li>\n<li>\n<p>制定计划，然后无情地执行</p>\n<p>如果执行学习计划中会有枯燥的感觉，就需要回顾自己的动机、目标，并稍稍跳出来重新审视一下学习计划。</p>\n<p>如果确认学习路径的正确性，就应该专注，而不是继续在这个领域中漫无目的的探索，这样才能进入深水区。</p>\n</li>\n</ol>\n"},{"title":"Casbin is All You Need —— 访问控制框架 Casbin Ramp Up","date":"2022-06-22T01:05:51.000Z","updated":"2022-06-22T01:05:51.000Z","_content":"\n## TL;DR\n\n- 访问控制框架 Casbin 的原理以及其内部组件的结构\n- 以一个 RBAC 的简单例子介绍 Casbin 的用法\n\n## Casbin是什么？\n### 访问控制\n\n![](/asset/casbin-ramp-up/2022-06-23-17-50-00-image.png)\n\n访问控制，顾名思义，是指判断一条请求是否可以访问受保护的资源的技术。在上图的例子中，我们的后台中有两个资源，Resource1和Resource2。它们可以是服务器、账号、图片、视频等等等等。但是，它们的相同特性是不能被所有用户都访问。比如 Resource1 属于用户 Alice，那么只有 Alice 能够访问它，Bob 则不能。因此，我们就需要对访问请求进行过滤，判断其是否被允许到达目标资源。在上面的例子中，Alice 发起了两个访问请求，分别想要访问 Resource1 和 Resource2。访问控制层需要做的工作就是允许访问 Resource1 的请求通过，而阻拦想要访问 Resource2 的请求，因为 Resource2 属于 Bob，Alice 是无法访问的。\n\n在实际应用中，访问控制问题往往会随着业务而变得非常复杂。而 Casbin [<sup>1</sup>](#casbin) 就是一个强大的、高效的开源访问控制框架。Casbin 在 Github 上已获得超过 10k+ star，并且有着非常完整的生态。基于 Casbin 可以轻松的实现一系列访问控制模型，如 RBAC，ABAC等等。\n\n### 原理——PML\n\nCasbin 的底层原理基于其创建者罗杨博士所发表的一篇论文《PML: An Interpreter-Based Access Control Policy Language for Web Services》[<sup>2</sup>](#pml)\n\n#### 设计目标\n\n这篇论文主要关注于如何解决现实中云服务厂商有关权限校验所遇到的两个问题：\n\n1. 每个云服务厂商都有着自己的一套权限检验规则。这对于在多个云环境都进行部署的用户来说，造成了很大的迁移和维护成本。\n2. 同样的，维护自己的一套权限校验规则对于云服务厂商来说，也是一个挑战。如果云服务厂商缺乏在这方面的相关经验，就很有可能造成安全漏洞。\n\n既然文章的目标本质上是通过通用性来解决问题，作者也考虑了如何实现这个目标，提出了两个 independent 的设计要求：\n\n1. Access Control Model Independent：PML 既需要支持用户可以在多个云服务厂商中使用同一个模型，也需要支持用户在不改变校验代码的同时，可以切换不同的模型。\n2. Implementation Language Independent: PML 的设计不应该依赖于某种编程语言的特性。\n   因此，文中提出了一种新的权限校验语言——PML (PERM Modeling Language)，希望通过一种支持多种权限校验模型的配置语言来弥补这个 gap。\n\n#### 设计实现\n\n在介绍 PML 的设计之前，我们可以先大致了解一下访问控制问题中，所涉及到的一些概念。\n\n![1c2dea1652f67c0b7920b0471a4113afd8d9325a.png](/asset/casbin-ramp-up/overview.png)\n\n一般来说，访问控制会涉及到两个部分：\n\n1. **Model 访问控制模型**。常见的模型有 ACL（Access Control List 访问控制列表），RBAC（Role-Based Access Control 基于角色的访问控制），ABAC（Attribute-Based Access Control 基于属性的访问控制）。对于一个应用来说，Model 的选择是与应用的业务逻辑是密切相关的，因此也是相对静态的。一旦代码编译完成，这部分是不会随着应用的运行而产生变化的。\n2. **Policy 访问控制规则**。Policy 是和 Model 相对应的，每种不同的 Model，都会有不同格式的 Policy。而与 Model 完全相反的是，Policy 是相对动态的。在编写代码的过程中，我们只能去定义 Policy 的格式，而 Policy 的具体内容都是应用运行过程中添加或修改的。例如，有一个新用户注册了我们的应用。那么，我们就需要动态的为其添加一条 Policy。\n\n我们可以将这两部分理解为传统应用中的代码和数据。有了这两部分后，再加上用户特定的校验逻辑，那么就可以完成访问控制任务。\n\n##### PERM 模型\n\n当然，对于现实环境中复杂的情况，简单地将问题建模为这两部分肯定是不够的，因此，论文提出了一个新的元模型 PERM（Policy-Effect-Request-Matcher）。\n\n![6d8fa0a037c3ea185cfadb8d817c41cce0d66d78.png](/asset/casbin-ramp-up/pml.png)\n\nPERM 模型主要包含了 6 个主要的概念：\n\n1. **Request**：访问请求定义。用户真实的访问请求，通常会包含 sub（访问者），obj（被访问的资源）， act（访问时所进行的操作）或其他用户自定义的属性。\n2. **Policy**：访问控制规则定义。定义了需要对访问请求的哪些属性进行校验。\n3. **Policy Rule**：访问控制规则实例。\n4. **Matcher**：如何为一条 Request 匹配到其对应的 Policy Rule。\n5. **Effect**：当一条 Request 匹配到了一条或多条 Policy Rule，如何判断其是否应该被允许。\n6. **Stub Function**：在实际应用中，Request 实例 和 Policy Rule 的匹配往往无法通过简单的 == 等于来解决，例如通配符等等。所以 Stub Function 允许用户自定义一些复杂的匹配方法。\n\n这六个更加详细的建模了访问控制的问题。我们也可以对其简单的分一下类，Request，Policy，Matcher，Effect 和 Stub Function 都是静态的，属于 Model 的一部分。通过这个五项的组合，ACL等常见的模型以及一些用户自定义的规则，都可以很轻易的表示出来。在最后一部分中，会以 RBAC 为例，介绍如何通过 PML 实现这样一个模型。\n\n而 Policy Rule 就属于动态变化的内容。在实际实现中，往往也是像数据一样，存储在数据库当中的。\n\n## 结构\n\n与论文中的实现相比，目前 Casbin 的实现更加强大，支持了更多功能。所以，这里以 Casbin 主库（Go 版本），介绍 Casbin 是如何进行工作的。\n\n![6dbf7fb95022569c5ad99becdcd9090df8a99250.png](/asset/casbin-ramp-up/detail.png)\n\n1. 在应用启动时，Casbin 会读取用户已经定义好的 Model，其中会包含 Request, Policy, Matcher 和 Effector 四个部分的定义。同时，Casbin 会利用 Adapter，从数据源处读取 Policy 实例（也就是上文提到的 Policy Rule）。后文就将 Policy 实例简称为 Policy。\n\n2. 对于 Policy 的存储和读取，Casbin 将其解耦到了独立的 Adapter 模块。通过使用不同的 Adapter（File, MySQL等等），可以从不同的数据源中读取 Policy。对于 Policy 比较多的场景，将所有的 Policy 同时加载进内存，确实会导致一定的性能损失。所以，在加载时，部分 Adapter 也提供 `LoadFilteredPolicy` 的接口，通过只加载 Policy 的一部分子集，减少这部分带来的性能瓶颈。\n\n3. 在一条请求到来时，该请求首先会按照 Model 中的定义进行拆分。接下来，Matcher 会根据 Model 中定义的规则，与 Policy 进行匹配。除了支持 == 强匹配外，Matcher 还支持通过 Function 和 Role Manager 进行模糊匹配。Function 像用户提供了自定义匹配规则的接口。通过向 Matcher 传入自定义函数，Matcher 可以对 Request 与 Policy 之间进行一些复杂的匹配。\n   \n   对于 RBAC 等访问控制模型，除了单纯的用户与权限之间存在关系之外，用户与角色（Role）之间还存在着继承关系。Casbin 中采用了 Role Manager 来为一条 Request 的用户以及其对应角色（包含继承角色）寻找与其相关的 Policy。同时，Role Manager 也支持添加自定义的 Function，来对用户与角色之间进行复杂的匹配。\n\n4. 在实际应用中，一条 Request 可能会匹配到多条 Policy。得到所有的 Policy 后，需要进一步将多条 Policy 的结果进行聚合，得到最终是否允许 Request。Effector 根据 Model 中配置的规则，对所有 Matched Policy 的 effect 项进行进一步的 eval。\n\n5. 在很多场景下，访问控制服务会有多个实例。Casbin 支持对 Policy 进行增量更新，那么，就需要 Dispatcher 维护多个 Casbin 实例的 Policy 之间的一致性。Dispatcher 主要提供两部分的功能，一部分是 Casbin 的 API，另一部分是 Dispatcher 自身的 API，用来实现成员管理等一致性问题，可以通过 Raft 等共识算法实现。\n\n## Usage\n\n在了解了 Casbin 的原理和结构后，我们可以开始利用 Casbin 来进行一些实践。本章以 RBAC 模型为例，构建一个简单的访问控制示例。RBAC (Role-Based Access Control) 模型是基于角色的访问控制模型。在 RBAC 的模型中，用户和资源之间存在着角色（Role），用户可以属于一个或多个角色，角色拥有权限去访问资源。\n\n经过前两章的介绍，我们可以将访问控制分为三个部分：Static，Dynamic 和 User-specific Logic。在使用 Casbin 时，也可以这样进行划分。首先，我们先来定义一个静态的 RBAC Model（model.conf）。\n\n```c#\n[request_definition]\nr = sub, obj, act\n\n[policy_definition]\np = sub, obj, act\n\n[role_definition]\ng = _, _\n\n[policy_effect]\ne = some(where (p.eft == allow))\n\n[matchers]\nm = g(r.sub, p.sub) && r.obj == p.obj && r.act == p.act\n```\n\n在这个模型中，分别定义了五部分内容。\n\n1. **request_definition**，定义了 Request 的结构。这份示例中包含了访问者（sub），被访问者（obj）和操作（act）。\n2. **policy_definition**，定义了 Policy 的结构。通常，由于 Policy 和 Request 之间要进行匹配，所以两者的结构有一定的相似性。\n3. **role_definition**，定义了 Role Manager。`g` 定义了一套 RBAC 系统，换句话说是一组用户角色继承关系的集合。在实际使用中，更类似于一个函数，判断输入的参数是否在这个集合中存在继承关系。\n4. **policy_effect**，定义了如何对多个匹配到的 Policy 做合并。目前，Casbin 支持几个固定语法的合并模式，在官网 [<sup>3</sup>](#policy) 上有着详细的介绍。这些模式的含义也很好理解，模式的语法与自然语言或者 SQL 非常相近。例如，`some(where (p.eft == allow))` 表示的是当任意一个 Policy 的 effect 是 allow，那么合并的结果即为 allow。\n5. **matchers**，定义了如何匹配 Policy 和 Request。 定义公式的语法与常见语言中的布尔表达式相似，通过 `==` 可以将 Policy 和 Request 中的各项进行对比。\n\n#### Policy\n\n```\np, alice, data1, read\np, bob, data2, write\np, data2_admin, data2, read\np, data2_admin, data2, write\ng, alice, data2_admin\n```\n\n接下来，我们可以按照 Model 中定义的 Policy 结构来编写 Policy 实例。例如，第一项 `p, alice, data1, read` 与 `p = sub, obj, act` 相对应，`alice`，`data1`，`read` 与 `sub`，`obj`，`act` 相对应。另外一条比较特殊的实例是最后一项，`g, alice, data2_admin`，定义了用户 `alice` 继承了 `data2_admin` 这一角色。\n\n我们可以将上面的 Policy 保存在文件 policy.csv 中。但一般来说，Policy 储存在数据库等等一些更加 organized 的外部存储中。\n\n#### User Logic\n\nCasbin 几乎支持所有的常见的编程语言，用户使用的逻辑也基本相似，主要通过 Enforcer 类来进行操作。\n\n```go\ne, err := casbin.NewEnforcer(\"model.conf\", \"policy.csv\")\nresult1, _ := e.Enforce(\"alice\", \"data1\", \"read\")\nfmt.Println(result1)\nresult2, _ := e.Enforce(\"alice\", \"data1\", \"write\")\nfmt.Println(result2)\nresult3, _ := e.Enforce(\"alice\", \"data2\", \"read\")\nfmt.Println(result3)\n```\n\n通过 Enforce 方法，开发者输入 Request，就可以得到这条请求是否可以通过。在上述例子中有 3 个 test case，分别验证了合法请求匹配，非法请求匹配，集成角色请求匹配。第一个 test case 对应了 policy.csv 中的第一条 Policy，第二个 test case 则没有 test case。第三个 test case 通过 `g, alice, data2_admin` 将 alice 与 data2_admin 的 Policy 关联起来，然后通过第三条 Policy p, data2_admin, data2, read 验证其为合法请求。\n\n## Reference\n<div id=\"ref1\"/>\n- [1] https://github.com/casbin/casbin\n<div id=\"ref2\"/>\n- [2] https://arxiv.org/pdf/1903.09756.pdf\n<div id=\"ref3\"/>\n- [3] https://casbin.org/docs/en/syntax-for-models#policy-effect\n","source":"_posts/casbin-ramp-up.md","raw":"---\ntitle: \"Casbin is All You Need —— 访问控制框架 Casbin Ramp Up\"\ndate: 2022-06-22 01:05:51\nupdated: 2022-06-22 01:05:51\n---\n\n## TL;DR\n\n- 访问控制框架 Casbin 的原理以及其内部组件的结构\n- 以一个 RBAC 的简单例子介绍 Casbin 的用法\n\n## Casbin是什么？\n### 访问控制\n\n![](/asset/casbin-ramp-up/2022-06-23-17-50-00-image.png)\n\n访问控制，顾名思义，是指判断一条请求是否可以访问受保护的资源的技术。在上图的例子中，我们的后台中有两个资源，Resource1和Resource2。它们可以是服务器、账号、图片、视频等等等等。但是，它们的相同特性是不能被所有用户都访问。比如 Resource1 属于用户 Alice，那么只有 Alice 能够访问它，Bob 则不能。因此，我们就需要对访问请求进行过滤，判断其是否被允许到达目标资源。在上面的例子中，Alice 发起了两个访问请求，分别想要访问 Resource1 和 Resource2。访问控制层需要做的工作就是允许访问 Resource1 的请求通过，而阻拦想要访问 Resource2 的请求，因为 Resource2 属于 Bob，Alice 是无法访问的。\n\n在实际应用中，访问控制问题往往会随着业务而变得非常复杂。而 Casbin [<sup>1</sup>](#casbin) 就是一个强大的、高效的开源访问控制框架。Casbin 在 Github 上已获得超过 10k+ star，并且有着非常完整的生态。基于 Casbin 可以轻松的实现一系列访问控制模型，如 RBAC，ABAC等等。\n\n### 原理——PML\n\nCasbin 的底层原理基于其创建者罗杨博士所发表的一篇论文《PML: An Interpreter-Based Access Control Policy Language for Web Services》[<sup>2</sup>](#pml)\n\n#### 设计目标\n\n这篇论文主要关注于如何解决现实中云服务厂商有关权限校验所遇到的两个问题：\n\n1. 每个云服务厂商都有着自己的一套权限检验规则。这对于在多个云环境都进行部署的用户来说，造成了很大的迁移和维护成本。\n2. 同样的，维护自己的一套权限校验规则对于云服务厂商来说，也是一个挑战。如果云服务厂商缺乏在这方面的相关经验，就很有可能造成安全漏洞。\n\n既然文章的目标本质上是通过通用性来解决问题，作者也考虑了如何实现这个目标，提出了两个 independent 的设计要求：\n\n1. Access Control Model Independent：PML 既需要支持用户可以在多个云服务厂商中使用同一个模型，也需要支持用户在不改变校验代码的同时，可以切换不同的模型。\n2. Implementation Language Independent: PML 的设计不应该依赖于某种编程语言的特性。\n   因此，文中提出了一种新的权限校验语言——PML (PERM Modeling Language)，希望通过一种支持多种权限校验模型的配置语言来弥补这个 gap。\n\n#### 设计实现\n\n在介绍 PML 的设计之前，我们可以先大致了解一下访问控制问题中，所涉及到的一些概念。\n\n![1c2dea1652f67c0b7920b0471a4113afd8d9325a.png](/asset/casbin-ramp-up/overview.png)\n\n一般来说，访问控制会涉及到两个部分：\n\n1. **Model 访问控制模型**。常见的模型有 ACL（Access Control List 访问控制列表），RBAC（Role-Based Access Control 基于角色的访问控制），ABAC（Attribute-Based Access Control 基于属性的访问控制）。对于一个应用来说，Model 的选择是与应用的业务逻辑是密切相关的，因此也是相对静态的。一旦代码编译完成，这部分是不会随着应用的运行而产生变化的。\n2. **Policy 访问控制规则**。Policy 是和 Model 相对应的，每种不同的 Model，都会有不同格式的 Policy。而与 Model 完全相反的是，Policy 是相对动态的。在编写代码的过程中，我们只能去定义 Policy 的格式，而 Policy 的具体内容都是应用运行过程中添加或修改的。例如，有一个新用户注册了我们的应用。那么，我们就需要动态的为其添加一条 Policy。\n\n我们可以将这两部分理解为传统应用中的代码和数据。有了这两部分后，再加上用户特定的校验逻辑，那么就可以完成访问控制任务。\n\n##### PERM 模型\n\n当然，对于现实环境中复杂的情况，简单地将问题建模为这两部分肯定是不够的，因此，论文提出了一个新的元模型 PERM（Policy-Effect-Request-Matcher）。\n\n![6d8fa0a037c3ea185cfadb8d817c41cce0d66d78.png](/asset/casbin-ramp-up/pml.png)\n\nPERM 模型主要包含了 6 个主要的概念：\n\n1. **Request**：访问请求定义。用户真实的访问请求，通常会包含 sub（访问者），obj（被访问的资源）， act（访问时所进行的操作）或其他用户自定义的属性。\n2. **Policy**：访问控制规则定义。定义了需要对访问请求的哪些属性进行校验。\n3. **Policy Rule**：访问控制规则实例。\n4. **Matcher**：如何为一条 Request 匹配到其对应的 Policy Rule。\n5. **Effect**：当一条 Request 匹配到了一条或多条 Policy Rule，如何判断其是否应该被允许。\n6. **Stub Function**：在实际应用中，Request 实例 和 Policy Rule 的匹配往往无法通过简单的 == 等于来解决，例如通配符等等。所以 Stub Function 允许用户自定义一些复杂的匹配方法。\n\n这六个更加详细的建模了访问控制的问题。我们也可以对其简单的分一下类，Request，Policy，Matcher，Effect 和 Stub Function 都是静态的，属于 Model 的一部分。通过这个五项的组合，ACL等常见的模型以及一些用户自定义的规则，都可以很轻易的表示出来。在最后一部分中，会以 RBAC 为例，介绍如何通过 PML 实现这样一个模型。\n\n而 Policy Rule 就属于动态变化的内容。在实际实现中，往往也是像数据一样，存储在数据库当中的。\n\n## 结构\n\n与论文中的实现相比，目前 Casbin 的实现更加强大，支持了更多功能。所以，这里以 Casbin 主库（Go 版本），介绍 Casbin 是如何进行工作的。\n\n![6dbf7fb95022569c5ad99becdcd9090df8a99250.png](/asset/casbin-ramp-up/detail.png)\n\n1. 在应用启动时，Casbin 会读取用户已经定义好的 Model，其中会包含 Request, Policy, Matcher 和 Effector 四个部分的定义。同时，Casbin 会利用 Adapter，从数据源处读取 Policy 实例（也就是上文提到的 Policy Rule）。后文就将 Policy 实例简称为 Policy。\n\n2. 对于 Policy 的存储和读取，Casbin 将其解耦到了独立的 Adapter 模块。通过使用不同的 Adapter（File, MySQL等等），可以从不同的数据源中读取 Policy。对于 Policy 比较多的场景，将所有的 Policy 同时加载进内存，确实会导致一定的性能损失。所以，在加载时，部分 Adapter 也提供 `LoadFilteredPolicy` 的接口，通过只加载 Policy 的一部分子集，减少这部分带来的性能瓶颈。\n\n3. 在一条请求到来时，该请求首先会按照 Model 中的定义进行拆分。接下来，Matcher 会根据 Model 中定义的规则，与 Policy 进行匹配。除了支持 == 强匹配外，Matcher 还支持通过 Function 和 Role Manager 进行模糊匹配。Function 像用户提供了自定义匹配规则的接口。通过向 Matcher 传入自定义函数，Matcher 可以对 Request 与 Policy 之间进行一些复杂的匹配。\n   \n   对于 RBAC 等访问控制模型，除了单纯的用户与权限之间存在关系之外，用户与角色（Role）之间还存在着继承关系。Casbin 中采用了 Role Manager 来为一条 Request 的用户以及其对应角色（包含继承角色）寻找与其相关的 Policy。同时，Role Manager 也支持添加自定义的 Function，来对用户与角色之间进行复杂的匹配。\n\n4. 在实际应用中，一条 Request 可能会匹配到多条 Policy。得到所有的 Policy 后，需要进一步将多条 Policy 的结果进行聚合，得到最终是否允许 Request。Effector 根据 Model 中配置的规则，对所有 Matched Policy 的 effect 项进行进一步的 eval。\n\n5. 在很多场景下，访问控制服务会有多个实例。Casbin 支持对 Policy 进行增量更新，那么，就需要 Dispatcher 维护多个 Casbin 实例的 Policy 之间的一致性。Dispatcher 主要提供两部分的功能，一部分是 Casbin 的 API，另一部分是 Dispatcher 自身的 API，用来实现成员管理等一致性问题，可以通过 Raft 等共识算法实现。\n\n## Usage\n\n在了解了 Casbin 的原理和结构后，我们可以开始利用 Casbin 来进行一些实践。本章以 RBAC 模型为例，构建一个简单的访问控制示例。RBAC (Role-Based Access Control) 模型是基于角色的访问控制模型。在 RBAC 的模型中，用户和资源之间存在着角色（Role），用户可以属于一个或多个角色，角色拥有权限去访问资源。\n\n经过前两章的介绍，我们可以将访问控制分为三个部分：Static，Dynamic 和 User-specific Logic。在使用 Casbin 时，也可以这样进行划分。首先，我们先来定义一个静态的 RBAC Model（model.conf）。\n\n```c#\n[request_definition]\nr = sub, obj, act\n\n[policy_definition]\np = sub, obj, act\n\n[role_definition]\ng = _, _\n\n[policy_effect]\ne = some(where (p.eft == allow))\n\n[matchers]\nm = g(r.sub, p.sub) && r.obj == p.obj && r.act == p.act\n```\n\n在这个模型中，分别定义了五部分内容。\n\n1. **request_definition**，定义了 Request 的结构。这份示例中包含了访问者（sub），被访问者（obj）和操作（act）。\n2. **policy_definition**，定义了 Policy 的结构。通常，由于 Policy 和 Request 之间要进行匹配，所以两者的结构有一定的相似性。\n3. **role_definition**，定义了 Role Manager。`g` 定义了一套 RBAC 系统，换句话说是一组用户角色继承关系的集合。在实际使用中，更类似于一个函数，判断输入的参数是否在这个集合中存在继承关系。\n4. **policy_effect**，定义了如何对多个匹配到的 Policy 做合并。目前，Casbin 支持几个固定语法的合并模式，在官网 [<sup>3</sup>](#policy) 上有着详细的介绍。这些模式的含义也很好理解，模式的语法与自然语言或者 SQL 非常相近。例如，`some(where (p.eft == allow))` 表示的是当任意一个 Policy 的 effect 是 allow，那么合并的结果即为 allow。\n5. **matchers**，定义了如何匹配 Policy 和 Request。 定义公式的语法与常见语言中的布尔表达式相似，通过 `==` 可以将 Policy 和 Request 中的各项进行对比。\n\n#### Policy\n\n```\np, alice, data1, read\np, bob, data2, write\np, data2_admin, data2, read\np, data2_admin, data2, write\ng, alice, data2_admin\n```\n\n接下来，我们可以按照 Model 中定义的 Policy 结构来编写 Policy 实例。例如，第一项 `p, alice, data1, read` 与 `p = sub, obj, act` 相对应，`alice`，`data1`，`read` 与 `sub`，`obj`，`act` 相对应。另外一条比较特殊的实例是最后一项，`g, alice, data2_admin`，定义了用户 `alice` 继承了 `data2_admin` 这一角色。\n\n我们可以将上面的 Policy 保存在文件 policy.csv 中。但一般来说，Policy 储存在数据库等等一些更加 organized 的外部存储中。\n\n#### User Logic\n\nCasbin 几乎支持所有的常见的编程语言，用户使用的逻辑也基本相似，主要通过 Enforcer 类来进行操作。\n\n```go\ne, err := casbin.NewEnforcer(\"model.conf\", \"policy.csv\")\nresult1, _ := e.Enforce(\"alice\", \"data1\", \"read\")\nfmt.Println(result1)\nresult2, _ := e.Enforce(\"alice\", \"data1\", \"write\")\nfmt.Println(result2)\nresult3, _ := e.Enforce(\"alice\", \"data2\", \"read\")\nfmt.Println(result3)\n```\n\n通过 Enforce 方法，开发者输入 Request，就可以得到这条请求是否可以通过。在上述例子中有 3 个 test case，分别验证了合法请求匹配，非法请求匹配，集成角色请求匹配。第一个 test case 对应了 policy.csv 中的第一条 Policy，第二个 test case 则没有 test case。第三个 test case 通过 `g, alice, data2_admin` 将 alice 与 data2_admin 的 Policy 关联起来，然后通过第三条 Policy p, data2_admin, data2, read 验证其为合法请求。\n\n## Reference\n<div id=\"ref1\"/>\n- [1] https://github.com/casbin/casbin\n<div id=\"ref2\"/>\n- [2] https://arxiv.org/pdf/1903.09756.pdf\n<div id=\"ref3\"/>\n- [3] https://casbin.org/docs/en/syntax-for-models#policy-effect\n","slug":"casbin-ramp-up","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clapldfd6000a18mzarlxam2a","content":"<h2 id=\"tldr\"><a class=\"markdownIt-Anchor\" href=\"#tldr\"></a> TL;DR</h2>\n<ul>\n<li>访问控制框架 Casbin 的原理以及其内部组件的结构</li>\n<li>以一个 RBAC 的简单例子介绍 Casbin 的用法</li>\n</ul>\n<h2 id=\"casbin是什么\"><a class=\"markdownIt-Anchor\" href=\"#casbin是什么\"></a> Casbin是什么？</h2>\n<h3 id=\"访问控制\"><a class=\"markdownIt-Anchor\" href=\"#访问控制\"></a> 访问控制</h3>\n<p><img src=\"/asset/casbin-ramp-up/2022-06-23-17-50-00-image.png\" alt=\"\" /></p>\n<p>访问控制，顾名思义，是指判断一条请求是否可以访问受保护的资源的技术。在上图的例子中，我们的后台中有两个资源，Resource1和Resource2。它们可以是服务器、账号、图片、视频等等等等。但是，它们的相同特性是不能被所有用户都访问。比如 Resource1 属于用户 Alice，那么只有 Alice 能够访问它，Bob 则不能。因此，我们就需要对访问请求进行过滤，判断其是否被允许到达目标资源。在上面的例子中，Alice 发起了两个访问请求，分别想要访问 Resource1 和 Resource2。访问控制层需要做的工作就是允许访问 Resource1 的请求通过，而阻拦想要访问 Resource2 的请求，因为 Resource2 属于 Bob，Alice 是无法访问的。</p>\n<p>在实际应用中，访问控制问题往往会随着业务而变得非常复杂。而 Casbin <a href=\"#casbin\"><sup>1</sup></a> 就是一个强大的、高效的开源访问控制框架。Casbin 在 Github 上已获得超过 10k+ star，并且有着非常完整的生态。基于 Casbin 可以轻松的实现一系列访问控制模型，如 RBAC，ABAC等等。</p>\n<h3 id=\"原理pml\"><a class=\"markdownIt-Anchor\" href=\"#原理pml\"></a> 原理——PML</h3>\n<p>Casbin 的底层原理基于其创建者罗杨博士所发表的一篇论文《PML: An Interpreter-Based Access Control Policy Language for Web Services》<a href=\"#pml\"><sup>2</sup></a></p>\n<h4 id=\"设计目标\"><a class=\"markdownIt-Anchor\" href=\"#设计目标\"></a> 设计目标</h4>\n<p>这篇论文主要关注于如何解决现实中云服务厂商有关权限校验所遇到的两个问题：</p>\n<ol>\n<li>每个云服务厂商都有着自己的一套权限检验规则。这对于在多个云环境都进行部署的用户来说，造成了很大的迁移和维护成本。</li>\n<li>同样的，维护自己的一套权限校验规则对于云服务厂商来说，也是一个挑战。如果云服务厂商缺乏在这方面的相关经验，就很有可能造成安全漏洞。</li>\n</ol>\n<p>既然文章的目标本质上是通过通用性来解决问题，作者也考虑了如何实现这个目标，提出了两个 independent 的设计要求：</p>\n<ol>\n<li>Access Control Model Independent：PML 既需要支持用户可以在多个云服务厂商中使用同一个模型，也需要支持用户在不改变校验代码的同时，可以切换不同的模型。</li>\n<li>Implementation Language Independent: PML 的设计不应该依赖于某种编程语言的特性。<br />\n因此，文中提出了一种新的权限校验语言——PML (PERM Modeling Language)，希望通过一种支持多种权限校验模型的配置语言来弥补这个 gap。</li>\n</ol>\n<h4 id=\"设计实现\"><a class=\"markdownIt-Anchor\" href=\"#设计实现\"></a> 设计实现</h4>\n<p>在介绍 PML 的设计之前，我们可以先大致了解一下访问控制问题中，所涉及到的一些概念。</p>\n<p><img src=\"/asset/casbin-ramp-up/overview.png\" alt=\"1c2dea1652f67c0b7920b0471a4113afd8d9325a.png\" /></p>\n<p>一般来说，访问控制会涉及到两个部分：</p>\n<ol>\n<li><strong>Model 访问控制模型</strong>。常见的模型有 ACL（Access Control List 访问控制列表），RBAC（Role-Based Access Control 基于角色的访问控制），ABAC（Attribute-Based Access Control 基于属性的访问控制）。对于一个应用来说，Model 的选择是与应用的业务逻辑是密切相关的，因此也是相对静态的。一旦代码编译完成，这部分是不会随着应用的运行而产生变化的。</li>\n<li><strong>Policy 访问控制规则</strong>。Policy 是和 Model 相对应的，每种不同的 Model，都会有不同格式的 Policy。而与 Model 完全相反的是，Policy 是相对动态的。在编写代码的过程中，我们只能去定义 Policy 的格式，而 Policy 的具体内容都是应用运行过程中添加或修改的。例如，有一个新用户注册了我们的应用。那么，我们就需要动态的为其添加一条 Policy。</li>\n</ol>\n<p>我们可以将这两部分理解为传统应用中的代码和数据。有了这两部分后，再加上用户特定的校验逻辑，那么就可以完成访问控制任务。</p>\n<h5 id=\"perm-模型\"><a class=\"markdownIt-Anchor\" href=\"#perm-模型\"></a> PERM 模型</h5>\n<p>当然，对于现实环境中复杂的情况，简单地将问题建模为这两部分肯定是不够的，因此，论文提出了一个新的元模型 PERM（Policy-Effect-Request-Matcher）。</p>\n<p><img src=\"/asset/casbin-ramp-up/pml.png\" alt=\"6d8fa0a037c3ea185cfadb8d817c41cce0d66d78.png\" /></p>\n<p>PERM 模型主要包含了 6 个主要的概念：</p>\n<ol>\n<li><strong>Request</strong>：访问请求定义。用户真实的访问请求，通常会包含 sub（访问者），obj（被访问的资源）， act（访问时所进行的操作）或其他用户自定义的属性。</li>\n<li><strong>Policy</strong>：访问控制规则定义。定义了需要对访问请求的哪些属性进行校验。</li>\n<li><strong>Policy Rule</strong>：访问控制规则实例。</li>\n<li><strong>Matcher</strong>：如何为一条 Request 匹配到其对应的 Policy Rule。</li>\n<li><strong>Effect</strong>：当一条 Request 匹配到了一条或多条 Policy Rule，如何判断其是否应该被允许。</li>\n<li><strong>Stub Function</strong>：在实际应用中，Request 实例 和 Policy Rule 的匹配往往无法通过简单的 == 等于来解决，例如通配符等等。所以 Stub Function 允许用户自定义一些复杂的匹配方法。</li>\n</ol>\n<p>这六个更加详细的建模了访问控制的问题。我们也可以对其简单的分一下类，Request，Policy，Matcher，Effect 和 Stub Function 都是静态的，属于 Model 的一部分。通过这个五项的组合，ACL等常见的模型以及一些用户自定义的规则，都可以很轻易的表示出来。在最后一部分中，会以 RBAC 为例，介绍如何通过 PML 实现这样一个模型。</p>\n<p>而 Policy Rule 就属于动态变化的内容。在实际实现中，往往也是像数据一样，存储在数据库当中的。</p>\n<h2 id=\"结构\"><a class=\"markdownIt-Anchor\" href=\"#结构\"></a> 结构</h2>\n<p>与论文中的实现相比，目前 Casbin 的实现更加强大，支持了更多功能。所以，这里以 Casbin 主库（Go 版本），介绍 Casbin 是如何进行工作的。</p>\n<p><img src=\"/asset/casbin-ramp-up/detail.png\" alt=\"6dbf7fb95022569c5ad99becdcd9090df8a99250.png\" /></p>\n<ol>\n<li>\n<p>在应用启动时，Casbin 会读取用户已经定义好的 Model，其中会包含 Request, Policy, Matcher 和 Effector 四个部分的定义。同时，Casbin 会利用 Adapter，从数据源处读取 Policy 实例（也就是上文提到的 Policy Rule）。后文就将 Policy 实例简称为 Policy。</p>\n</li>\n<li>\n<p>对于 Policy 的存储和读取，Casbin 将其解耦到了独立的 Adapter 模块。通过使用不同的 Adapter（File, MySQL等等），可以从不同的数据源中读取 Policy。对于 Policy 比较多的场景，将所有的 Policy 同时加载进内存，确实会导致一定的性能损失。所以，在加载时，部分 Adapter 也提供 <code>LoadFilteredPolicy</code> 的接口，通过只加载 Policy 的一部分子集，减少这部分带来的性能瓶颈。</p>\n</li>\n<li>\n<p>在一条请求到来时，该请求首先会按照 Model 中的定义进行拆分。接下来，Matcher 会根据 Model 中定义的规则，与 Policy 进行匹配。除了支持 == 强匹配外，Matcher 还支持通过 Function 和 Role Manager 进行模糊匹配。Function 像用户提供了自定义匹配规则的接口。通过向 Matcher 传入自定义函数，Matcher 可以对 Request 与 Policy 之间进行一些复杂的匹配。</p>\n<p>对于 RBAC 等访问控制模型，除了单纯的用户与权限之间存在关系之外，用户与角色（Role）之间还存在着继承关系。Casbin 中采用了 Role Manager 来为一条 Request 的用户以及其对应角色（包含继承角色）寻找与其相关的 Policy。同时，Role Manager 也支持添加自定义的 Function，来对用户与角色之间进行复杂的匹配。</p>\n</li>\n<li>\n<p>在实际应用中，一条 Request 可能会匹配到多条 Policy。得到所有的 Policy 后，需要进一步将多条 Policy 的结果进行聚合，得到最终是否允许 Request。Effector 根据 Model 中配置的规则，对所有 Matched Policy 的 effect 项进行进一步的 eval。</p>\n</li>\n<li>\n<p>在很多场景下，访问控制服务会有多个实例。Casbin 支持对 Policy 进行增量更新，那么，就需要 Dispatcher 维护多个 Casbin 实例的 Policy 之间的一致性。Dispatcher 主要提供两部分的功能，一部分是 Casbin 的 API，另一部分是 Dispatcher 自身的 API，用来实现成员管理等一致性问题，可以通过 Raft 等共识算法实现。</p>\n</li>\n</ol>\n<h2 id=\"usage\"><a class=\"markdownIt-Anchor\" href=\"#usage\"></a> Usage</h2>\n<p>在了解了 Casbin 的原理和结构后，我们可以开始利用 Casbin 来进行一些实践。本章以 RBAC 模型为例，构建一个简单的访问控制示例。RBAC (Role-Based Access Control) 模型是基于角色的访问控制模型。在 RBAC 的模型中，用户和资源之间存在着角色（Role），用户可以属于一个或多个角色，角色拥有权限去访问资源。</p>\n<p>经过前两章的介绍，我们可以将访问控制分为三个部分：Static，Dynamic 和 User-specific Logic。在使用 Casbin 时，也可以这样进行划分。首先，我们先来定义一个静态的 RBAC Model（model.conf）。</p>\n<pre class=\"highlight\"><code class=\"c#\">[<span class=\"hljs-meta\">request_definition</span>]\nr = sub, obj, act\n\n[<span class=\"hljs-meta\">policy_definition</span>]\np = sub, obj, act\n\n[<span class=\"hljs-meta\">role_definition</span>]\ng = _, _\n\n[<span class=\"hljs-meta\">policy_effect</span>]\ne = some(<span class=\"hljs-keyword\">where</span> (p.eft == allow))\n\n[<span class=\"hljs-meta\">matchers</span>]\nm = g(r.sub, p.sub) &amp;&amp; r.obj == p.obj &amp;&amp; r.act == p.act\n</code></pre>\n<p>在这个模型中，分别定义了五部分内容。</p>\n<ol>\n<li><strong>request_definition</strong>，定义了 Request 的结构。这份示例中包含了访问者（sub），被访问者（obj）和操作（act）。</li>\n<li><strong>policy_definition</strong>，定义了 Policy 的结构。通常，由于 Policy 和 Request 之间要进行匹配，所以两者的结构有一定的相似性。</li>\n<li><strong>role_definition</strong>，定义了 Role Manager。<code>g</code> 定义了一套 RBAC 系统，换句话说是一组用户角色继承关系的集合。在实际使用中，更类似于一个函数，判断输入的参数是否在这个集合中存在继承关系。</li>\n<li><strong>policy_effect</strong>，定义了如何对多个匹配到的 Policy 做合并。目前，Casbin 支持几个固定语法的合并模式，在官网 <a href=\"#policy\"><sup>3</sup></a> 上有着详细的介绍。这些模式的含义也很好理解，模式的语法与自然语言或者 SQL 非常相近。例如，<code>some(where (p.eft == allow))</code> 表示的是当任意一个 Policy 的 effect 是 allow，那么合并的结果即为 allow。</li>\n<li><strong>matchers</strong>，定义了如何匹配 Policy 和 Request。 定义公式的语法与常见语言中的布尔表达式相似，通过 <code>==</code> 可以将 Policy 和 Request 中的各项进行对比。</li>\n</ol>\n<h4 id=\"policy\"><a class=\"markdownIt-Anchor\" href=\"#policy\"></a> Policy</h4>\n<pre class=\"highlight\"><code class=\"\">p, alice, data1, read\np, bob, data2, write\np, data2_admin, data2, read\np, data2_admin, data2, write\ng, alice, data2_admin\n</code></pre>\n<p>接下来，我们可以按照 Model 中定义的 Policy 结构来编写 Policy 实例。例如，第一项 <code>p, alice, data1, read</code> 与 <code>p = sub, obj, act</code> 相对应，<code>alice</code>，<code>data1</code>，<code>read</code> 与 <code>sub</code>，<code>obj</code>，<code>act</code> 相对应。另外一条比较特殊的实例是最后一项，<code>g, alice, data2_admin</code>，定义了用户 <code>alice</code> 继承了 <code>data2_admin</code> 这一角色。</p>\n<p>我们可以将上面的 Policy 保存在文件 policy.csv 中。但一般来说，Policy 储存在数据库等等一些更加 organized 的外部存储中。</p>\n<h4 id=\"user-logic\"><a class=\"markdownIt-Anchor\" href=\"#user-logic\"></a> User Logic</h4>\n<p>Casbin 几乎支持所有的常见的编程语言，用户使用的逻辑也基本相似，主要通过 Enforcer 类来进行操作。</p>\n<pre class=\"highlight\"><code class=\"go\">e, err := casbin.NewEnforcer(<span class=\"hljs-string\">&quot;model.conf&quot;</span>, <span class=\"hljs-string\">&quot;policy.csv&quot;</span>)\nresult1, _ := e.Enforce(<span class=\"hljs-string\">&quot;alice&quot;</span>, <span class=\"hljs-string\">&quot;data1&quot;</span>, <span class=\"hljs-string\">&quot;read&quot;</span>)\nfmt.Println(result1)\nresult2, _ := e.Enforce(<span class=\"hljs-string\">&quot;alice&quot;</span>, <span class=\"hljs-string\">&quot;data1&quot;</span>, <span class=\"hljs-string\">&quot;write&quot;</span>)\nfmt.Println(result2)\nresult3, _ := e.Enforce(<span class=\"hljs-string\">&quot;alice&quot;</span>, <span class=\"hljs-string\">&quot;data2&quot;</span>, <span class=\"hljs-string\">&quot;read&quot;</span>)\nfmt.Println(result3)\n</code></pre>\n<p>通过 Enforce 方法，开发者输入 Request，就可以得到这条请求是否可以通过。在上述例子中有 3 个 test case，分别验证了合法请求匹配，非法请求匹配，集成角色请求匹配。第一个 test case 对应了 policy.csv 中的第一条 Policy，第二个 test case 则没有 test case。第三个 test case 通过 <code>g, alice, data2_admin</code> 将 alice 与 data2_admin 的 Policy 关联起来，然后通过第三条 Policy p, data2_admin, data2, read 验证其为合法请求。</p>\n<h2 id=\"reference\"><a class=\"markdownIt-Anchor\" href=\"#reference\"></a> Reference</h2>\n<div id=\"ref1\"/>\n- [1] https://github.com/casbin/casbin\n<div id=\"ref2\"/>\n- [2] https://arxiv.org/pdf/1903.09756.pdf\n<div id=\"ref3\"/>\n- [3] https://casbin.org/docs/en/syntax-for-models#policy-effect\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"tldr\"><a class=\"markdownIt-Anchor\" href=\"#tldr\"></a> TL;DR</h2>\n<ul>\n<li>访问控制框架 Casbin 的原理以及其内部组件的结构</li>\n<li>以一个 RBAC 的简单例子介绍 Casbin 的用法</li>\n</ul>\n<h2 id=\"casbin是什么\"><a class=\"markdownIt-Anchor\" href=\"#casbin是什么\"></a> Casbin是什么？</h2>\n<h3 id=\"访问控制\"><a class=\"markdownIt-Anchor\" href=\"#访问控制\"></a> 访问控制</h3>\n<p><img src=\"/asset/casbin-ramp-up/2022-06-23-17-50-00-image.png\" alt=\"\" /></p>\n<p>访问控制，顾名思义，是指判断一条请求是否可以访问受保护的资源的技术。在上图的例子中，我们的后台中有两个资源，Resource1和Resource2。它们可以是服务器、账号、图片、视频等等等等。但是，它们的相同特性是不能被所有用户都访问。比如 Resource1 属于用户 Alice，那么只有 Alice 能够访问它，Bob 则不能。因此，我们就需要对访问请求进行过滤，判断其是否被允许到达目标资源。在上面的例子中，Alice 发起了两个访问请求，分别想要访问 Resource1 和 Resource2。访问控制层需要做的工作就是允许访问 Resource1 的请求通过，而阻拦想要访问 Resource2 的请求，因为 Resource2 属于 Bob，Alice 是无法访问的。</p>\n<p>在实际应用中，访问控制问题往往会随着业务而变得非常复杂。而 Casbin <a href=\"#casbin\"><sup>1</sup></a> 就是一个强大的、高效的开源访问控制框架。Casbin 在 Github 上已获得超过 10k+ star，并且有着非常完整的生态。基于 Casbin 可以轻松的实现一系列访问控制模型，如 RBAC，ABAC等等。</p>\n<h3 id=\"原理pml\"><a class=\"markdownIt-Anchor\" href=\"#原理pml\"></a> 原理——PML</h3>\n<p>Casbin 的底层原理基于其创建者罗杨博士所发表的一篇论文《PML: An Interpreter-Based Access Control Policy Language for Web Services》<a href=\"#pml\"><sup>2</sup></a></p>\n<h4 id=\"设计目标\"><a class=\"markdownIt-Anchor\" href=\"#设计目标\"></a> 设计目标</h4>\n<p>这篇论文主要关注于如何解决现实中云服务厂商有关权限校验所遇到的两个问题：</p>\n<ol>\n<li>每个云服务厂商都有着自己的一套权限检验规则。这对于在多个云环境都进行部署的用户来说，造成了很大的迁移和维护成本。</li>\n<li>同样的，维护自己的一套权限校验规则对于云服务厂商来说，也是一个挑战。如果云服务厂商缺乏在这方面的相关经验，就很有可能造成安全漏洞。</li>\n</ol>\n<p>既然文章的目标本质上是通过通用性来解决问题，作者也考虑了如何实现这个目标，提出了两个 independent 的设计要求：</p>\n<ol>\n<li>Access Control Model Independent：PML 既需要支持用户可以在多个云服务厂商中使用同一个模型，也需要支持用户在不改变校验代码的同时，可以切换不同的模型。</li>\n<li>Implementation Language Independent: PML 的设计不应该依赖于某种编程语言的特性。<br />\n因此，文中提出了一种新的权限校验语言——PML (PERM Modeling Language)，希望通过一种支持多种权限校验模型的配置语言来弥补这个 gap。</li>\n</ol>\n<h4 id=\"设计实现\"><a class=\"markdownIt-Anchor\" href=\"#设计实现\"></a> 设计实现</h4>\n<p>在介绍 PML 的设计之前，我们可以先大致了解一下访问控制问题中，所涉及到的一些概念。</p>\n<p><img src=\"/asset/casbin-ramp-up/overview.png\" alt=\"1c2dea1652f67c0b7920b0471a4113afd8d9325a.png\" /></p>\n<p>一般来说，访问控制会涉及到两个部分：</p>\n<ol>\n<li><strong>Model 访问控制模型</strong>。常见的模型有 ACL（Access Control List 访问控制列表），RBAC（Role-Based Access Control 基于角色的访问控制），ABAC（Attribute-Based Access Control 基于属性的访问控制）。对于一个应用来说，Model 的选择是与应用的业务逻辑是密切相关的，因此也是相对静态的。一旦代码编译完成，这部分是不会随着应用的运行而产生变化的。</li>\n<li><strong>Policy 访问控制规则</strong>。Policy 是和 Model 相对应的，每种不同的 Model，都会有不同格式的 Policy。而与 Model 完全相反的是，Policy 是相对动态的。在编写代码的过程中，我们只能去定义 Policy 的格式，而 Policy 的具体内容都是应用运行过程中添加或修改的。例如，有一个新用户注册了我们的应用。那么，我们就需要动态的为其添加一条 Policy。</li>\n</ol>\n<p>我们可以将这两部分理解为传统应用中的代码和数据。有了这两部分后，再加上用户特定的校验逻辑，那么就可以完成访问控制任务。</p>\n<h5 id=\"perm-模型\"><a class=\"markdownIt-Anchor\" href=\"#perm-模型\"></a> PERM 模型</h5>\n<p>当然，对于现实环境中复杂的情况，简单地将问题建模为这两部分肯定是不够的，因此，论文提出了一个新的元模型 PERM（Policy-Effect-Request-Matcher）。</p>\n<p><img src=\"/asset/casbin-ramp-up/pml.png\" alt=\"6d8fa0a037c3ea185cfadb8d817c41cce0d66d78.png\" /></p>\n<p>PERM 模型主要包含了 6 个主要的概念：</p>\n<ol>\n<li><strong>Request</strong>：访问请求定义。用户真实的访问请求，通常会包含 sub（访问者），obj（被访问的资源）， act（访问时所进行的操作）或其他用户自定义的属性。</li>\n<li><strong>Policy</strong>：访问控制规则定义。定义了需要对访问请求的哪些属性进行校验。</li>\n<li><strong>Policy Rule</strong>：访问控制规则实例。</li>\n<li><strong>Matcher</strong>：如何为一条 Request 匹配到其对应的 Policy Rule。</li>\n<li><strong>Effect</strong>：当一条 Request 匹配到了一条或多条 Policy Rule，如何判断其是否应该被允许。</li>\n<li><strong>Stub Function</strong>：在实际应用中，Request 实例 和 Policy Rule 的匹配往往无法通过简单的 == 等于来解决，例如通配符等等。所以 Stub Function 允许用户自定义一些复杂的匹配方法。</li>\n</ol>\n<p>这六个更加详细的建模了访问控制的问题。我们也可以对其简单的分一下类，Request，Policy，Matcher，Effect 和 Stub Function 都是静态的，属于 Model 的一部分。通过这个五项的组合，ACL等常见的模型以及一些用户自定义的规则，都可以很轻易的表示出来。在最后一部分中，会以 RBAC 为例，介绍如何通过 PML 实现这样一个模型。</p>\n<p>而 Policy Rule 就属于动态变化的内容。在实际实现中，往往也是像数据一样，存储在数据库当中的。</p>\n<h2 id=\"结构\"><a class=\"markdownIt-Anchor\" href=\"#结构\"></a> 结构</h2>\n<p>与论文中的实现相比，目前 Casbin 的实现更加强大，支持了更多功能。所以，这里以 Casbin 主库（Go 版本），介绍 Casbin 是如何进行工作的。</p>\n<p><img src=\"/asset/casbin-ramp-up/detail.png\" alt=\"6dbf7fb95022569c5ad99becdcd9090df8a99250.png\" /></p>\n<ol>\n<li>\n<p>在应用启动时，Casbin 会读取用户已经定义好的 Model，其中会包含 Request, Policy, Matcher 和 Effector 四个部分的定义。同时，Casbin 会利用 Adapter，从数据源处读取 Policy 实例（也就是上文提到的 Policy Rule）。后文就将 Policy 实例简称为 Policy。</p>\n</li>\n<li>\n<p>对于 Policy 的存储和读取，Casbin 将其解耦到了独立的 Adapter 模块。通过使用不同的 Adapter（File, MySQL等等），可以从不同的数据源中读取 Policy。对于 Policy 比较多的场景，将所有的 Policy 同时加载进内存，确实会导致一定的性能损失。所以，在加载时，部分 Adapter 也提供 <code>LoadFilteredPolicy</code> 的接口，通过只加载 Policy 的一部分子集，减少这部分带来的性能瓶颈。</p>\n</li>\n<li>\n<p>在一条请求到来时，该请求首先会按照 Model 中的定义进行拆分。接下来，Matcher 会根据 Model 中定义的规则，与 Policy 进行匹配。除了支持 == 强匹配外，Matcher 还支持通过 Function 和 Role Manager 进行模糊匹配。Function 像用户提供了自定义匹配规则的接口。通过向 Matcher 传入自定义函数，Matcher 可以对 Request 与 Policy 之间进行一些复杂的匹配。</p>\n<p>对于 RBAC 等访问控制模型，除了单纯的用户与权限之间存在关系之外，用户与角色（Role）之间还存在着继承关系。Casbin 中采用了 Role Manager 来为一条 Request 的用户以及其对应角色（包含继承角色）寻找与其相关的 Policy。同时，Role Manager 也支持添加自定义的 Function，来对用户与角色之间进行复杂的匹配。</p>\n</li>\n<li>\n<p>在实际应用中，一条 Request 可能会匹配到多条 Policy。得到所有的 Policy 后，需要进一步将多条 Policy 的结果进行聚合，得到最终是否允许 Request。Effector 根据 Model 中配置的规则，对所有 Matched Policy 的 effect 项进行进一步的 eval。</p>\n</li>\n<li>\n<p>在很多场景下，访问控制服务会有多个实例。Casbin 支持对 Policy 进行增量更新，那么，就需要 Dispatcher 维护多个 Casbin 实例的 Policy 之间的一致性。Dispatcher 主要提供两部分的功能，一部分是 Casbin 的 API，另一部分是 Dispatcher 自身的 API，用来实现成员管理等一致性问题，可以通过 Raft 等共识算法实现。</p>\n</li>\n</ol>\n<h2 id=\"usage\"><a class=\"markdownIt-Anchor\" href=\"#usage\"></a> Usage</h2>\n<p>在了解了 Casbin 的原理和结构后，我们可以开始利用 Casbin 来进行一些实践。本章以 RBAC 模型为例，构建一个简单的访问控制示例。RBAC (Role-Based Access Control) 模型是基于角色的访问控制模型。在 RBAC 的模型中，用户和资源之间存在着角色（Role），用户可以属于一个或多个角色，角色拥有权限去访问资源。</p>\n<p>经过前两章的介绍，我们可以将访问控制分为三个部分：Static，Dynamic 和 User-specific Logic。在使用 Casbin 时，也可以这样进行划分。首先，我们先来定义一个静态的 RBAC Model（model.conf）。</p>\n<pre class=\"highlight\"><code class=\"c#\">[<span class=\"hljs-meta\">request_definition</span>]\nr = sub, obj, act\n\n[<span class=\"hljs-meta\">policy_definition</span>]\np = sub, obj, act\n\n[<span class=\"hljs-meta\">role_definition</span>]\ng = _, _\n\n[<span class=\"hljs-meta\">policy_effect</span>]\ne = some(<span class=\"hljs-keyword\">where</span> (p.eft == allow))\n\n[<span class=\"hljs-meta\">matchers</span>]\nm = g(r.sub, p.sub) &amp;&amp; r.obj == p.obj &amp;&amp; r.act == p.act\n</code></pre>\n<p>在这个模型中，分别定义了五部分内容。</p>\n<ol>\n<li><strong>request_definition</strong>，定义了 Request 的结构。这份示例中包含了访问者（sub），被访问者（obj）和操作（act）。</li>\n<li><strong>policy_definition</strong>，定义了 Policy 的结构。通常，由于 Policy 和 Request 之间要进行匹配，所以两者的结构有一定的相似性。</li>\n<li><strong>role_definition</strong>，定义了 Role Manager。<code>g</code> 定义了一套 RBAC 系统，换句话说是一组用户角色继承关系的集合。在实际使用中，更类似于一个函数，判断输入的参数是否在这个集合中存在继承关系。</li>\n<li><strong>policy_effect</strong>，定义了如何对多个匹配到的 Policy 做合并。目前，Casbin 支持几个固定语法的合并模式，在官网 <a href=\"#policy\"><sup>3</sup></a> 上有着详细的介绍。这些模式的含义也很好理解，模式的语法与自然语言或者 SQL 非常相近。例如，<code>some(where (p.eft == allow))</code> 表示的是当任意一个 Policy 的 effect 是 allow，那么合并的结果即为 allow。</li>\n<li><strong>matchers</strong>，定义了如何匹配 Policy 和 Request。 定义公式的语法与常见语言中的布尔表达式相似，通过 <code>==</code> 可以将 Policy 和 Request 中的各项进行对比。</li>\n</ol>\n<h4 id=\"policy\"><a class=\"markdownIt-Anchor\" href=\"#policy\"></a> Policy</h4>\n<pre class=\"highlight\"><code class=\"\">p, alice, data1, read\np, bob, data2, write\np, data2_admin, data2, read\np, data2_admin, data2, write\ng, alice, data2_admin\n</code></pre>\n<p>接下来，我们可以按照 Model 中定义的 Policy 结构来编写 Policy 实例。例如，第一项 <code>p, alice, data1, read</code> 与 <code>p = sub, obj, act</code> 相对应，<code>alice</code>，<code>data1</code>，<code>read</code> 与 <code>sub</code>，<code>obj</code>，<code>act</code> 相对应。另外一条比较特殊的实例是最后一项，<code>g, alice, data2_admin</code>，定义了用户 <code>alice</code> 继承了 <code>data2_admin</code> 这一角色。</p>\n<p>我们可以将上面的 Policy 保存在文件 policy.csv 中。但一般来说，Policy 储存在数据库等等一些更加 organized 的外部存储中。</p>\n<h4 id=\"user-logic\"><a class=\"markdownIt-Anchor\" href=\"#user-logic\"></a> User Logic</h4>\n<p>Casbin 几乎支持所有的常见的编程语言，用户使用的逻辑也基本相似，主要通过 Enforcer 类来进行操作。</p>\n<pre class=\"highlight\"><code class=\"go\">e, err := casbin.NewEnforcer(<span class=\"hljs-string\">&quot;model.conf&quot;</span>, <span class=\"hljs-string\">&quot;policy.csv&quot;</span>)\nresult1, _ := e.Enforce(<span class=\"hljs-string\">&quot;alice&quot;</span>, <span class=\"hljs-string\">&quot;data1&quot;</span>, <span class=\"hljs-string\">&quot;read&quot;</span>)\nfmt.Println(result1)\nresult2, _ := e.Enforce(<span class=\"hljs-string\">&quot;alice&quot;</span>, <span class=\"hljs-string\">&quot;data1&quot;</span>, <span class=\"hljs-string\">&quot;write&quot;</span>)\nfmt.Println(result2)\nresult3, _ := e.Enforce(<span class=\"hljs-string\">&quot;alice&quot;</span>, <span class=\"hljs-string\">&quot;data2&quot;</span>, <span class=\"hljs-string\">&quot;read&quot;</span>)\nfmt.Println(result3)\n</code></pre>\n<p>通过 Enforce 方法，开发者输入 Request，就可以得到这条请求是否可以通过。在上述例子中有 3 个 test case，分别验证了合法请求匹配，非法请求匹配，集成角色请求匹配。第一个 test case 对应了 policy.csv 中的第一条 Policy，第二个 test case 则没有 test case。第三个 test case 通过 <code>g, alice, data2_admin</code> 将 alice 与 data2_admin 的 Policy 关联起来，然后通过第三条 Policy p, data2_admin, data2, read 验证其为合法请求。</p>\n<h2 id=\"reference\"><a class=\"markdownIt-Anchor\" href=\"#reference\"></a> Reference</h2>\n<div id=\"ref1\"/>\n- [1] https://github.com/casbin/casbin\n<div id=\"ref2\"/>\n- [2] https://arxiv.org/pdf/1903.09756.pdf\n<div id=\"ref3\"/>\n- [3] https://casbin.org/docs/en/syntax-for-models#policy-effect\n"},{"title":"[BugFix] Kubernetes Ingress Nginx DNS 报错日志 Bug Fix","date":"2021-03-13T12:27:49.000Z","updated":"2021-03-13T12:27:49.000Z","_content":"## 问题\n目前，当指定访问集群外部地址为 IP 时，ingress-nginx controller 的日志中存在大量的 DNS 报错的垃圾日志。虽然不影响正常运行（猜测可能会导致性能波动，对比见最后），但是查看 Nginx 日志 debug 时效率严重降低。\n\n![](/asset/ingress-nginx-bug-fix/error.png)\n\n## 原因\n详细的讨论见：\n[https://github.com/coredns/coredns/issues/2324](https://github.com/coredns/coredns/issues/2324)\n\n简略总结下，导致访问外部 IP，Nginx 报 DNS 解析错误的原因在于 Kubernetes 自身的bug，缺少了一个验证。\n\n出现问题的情况是通过 ExternalName 类型的 Service 访问外部服务的。定义的 yaml 类似于下面这种：\n```yaml\nkind: Service\napiVersion: v1\nmetadata:\n  name: demo2\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: xxx.xxx.xxx.xxx\n```\n对于 Kubernetes 的设计来说，ExternalName 就是一个域名。K8s 官方是这样介绍的 \n\n> ExternalName: Maps the service to the contents of the externalName field (e.g. foo.bar.example.com), by returning aCNAMErecord with its value. No proxying of any kind is set up.\n\n> Note:ExternalName accepts an IPv4 address string, but as a DNS names comprised of digits, not as an IP address. ExternalNames that resemble IPv4 addresses are not resolved by CoreDNS or ingress-nginx because ExternalName is intended to specify a canonical DNS name.\n\n从实现上来看，ExternalName 类型的 Service 其实就是在 CoreDNS 里的一条 CNAME 记录。 CNAME 是一条域名指向另一个域名的记录，在 K8s 中，这条 record 记载的就是 Service 名字指向 ExtenalName 的一个映射。\n\n但是，当 ExternalName 类型的 Service 中设定的是 IP 时，K8s 并没有对其进行判断，仍然允许其正常创建。\n\n同时，Nginx 本身存在着一个轮询机制，会不断的向 DNS 服务拉取记录进行缓存。\n[https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua](https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua)\n\n在每次拉取缓存时会发生以下的过程：\n1. Nginx 从 CoreDNS 拉取到了一个 CNAME 记录，例如：demo2 -> xxx.xxx.xxx.xxx\n2. 接着，Nginx 尝试解析 xxx.xxx.xxx.xxx 这个域名，CoreDNS 自然是对这个长成 IP 样子的域名解析不出来的，于是解析失败，导致报错\n\n------\n\n至于为什么 DNS 解析失败之后，Nginx 仍然能够成功转发请求，原因是 ingress-nginx controller 在实现上并没有对这个进行区分。\n\n首先，先简单介绍下 controller 的原理。Ingress-nginx controller 一直监听着 k8s 系统中的 ingress 资源。当有新的 ingress 创建时，controller 会开始更新 Nginx 的配置文件，向其中添加转发规则，并重启 Nginx。\n\n下面是 controller 解析指向 ExternalName 的 ingress，然后创建 upstream 的逻辑\n[https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52](https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52)\n\n![](/asset/ingress-nginx-bug-fix/nginx-code.png)\n\n可以看到，controller 是没有强制解析 ExternalName 成域名的，所以写进 nginx.conf 的 upstream 也是 ip 形式，这样 nginx 会自然地将 ExternalName 解析成 IP，从而可以正常工作。\n\n## 解决方法\n在上面提到的 Github Issue 的讨论中，有大佬已经给出了解决方法，就是通过 Service without selectors 的方式。\nhttps://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors\n\n常见的 K8s Service 都是通过标签选择器，选择一系列 Pod 作为后端，K8s endpoint controller 会自动根据 Service 的声明去为 Service 的每个端口创建一个 endpoint。Endpoint 是 K8s 中实际进行服务路由的资源。\n\n而创建 Service without selectors，就需要我们手动去创建一个与 Service 同名的 endpoint。这样就不需要指定 Service 为 ExternalName 的类型，CoreDNS 中就会将其视作一条 A 记录，而不是一条 CNAME 记录。Nginx 拉取 DNS 缓存时也不会把 IP 当做域名了。\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: demo2\n  namespace: default\nspec:\n  clusterIP: None\n  ports:\n  - name: grpc\n    port: 32443\n    protocol: TCP\n---\nkind: Endpoints\napiVersion: v1\nmetadata:\n  name: demo2\n  namespace: default\nsubsets:\n  - addresses:\n      - ip: xxx.xxx.xxx.xxx\n    ports:\n      - port: 32443\n        name: grpc\n        protocol: TCP\n```\n\n## 对比\n在两个对等的集群发生通信时，demo1 修复，demo2不修复，对比两侧的 CPU 使用情况\n\ndemo1：\n![](/asset/ingress-nginx-bug-fix/demo1.png)\n\ndemo2：\n![](/asset/ingress-nginx-bug-fix/demo2.png)\n\ndemo2 大约比 demo1 消耗 CPU 多 0.020 个核。虽然这个报错会稍微增加一点 CPU 的使用量，但并不多。\n","source":"_posts/ingress-nginx-bug-fix.md","raw":"---\ntitle: \"[BugFix] Kubernetes Ingress Nginx DNS 报错日志 Bug Fix\"\ndate: 2021-03-13 12:27:49\nupdated: 2021-03-13 12:27:49\n---\n## 问题\n目前，当指定访问集群外部地址为 IP 时，ingress-nginx controller 的日志中存在大量的 DNS 报错的垃圾日志。虽然不影响正常运行（猜测可能会导致性能波动，对比见最后），但是查看 Nginx 日志 debug 时效率严重降低。\n\n![](/asset/ingress-nginx-bug-fix/error.png)\n\n## 原因\n详细的讨论见：\n[https://github.com/coredns/coredns/issues/2324](https://github.com/coredns/coredns/issues/2324)\n\n简略总结下，导致访问外部 IP，Nginx 报 DNS 解析错误的原因在于 Kubernetes 自身的bug，缺少了一个验证。\n\n出现问题的情况是通过 ExternalName 类型的 Service 访问外部服务的。定义的 yaml 类似于下面这种：\n```yaml\nkind: Service\napiVersion: v1\nmetadata:\n  name: demo2\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: xxx.xxx.xxx.xxx\n```\n对于 Kubernetes 的设计来说，ExternalName 就是一个域名。K8s 官方是这样介绍的 \n\n> ExternalName: Maps the service to the contents of the externalName field (e.g. foo.bar.example.com), by returning aCNAMErecord with its value. No proxying of any kind is set up.\n\n> Note:ExternalName accepts an IPv4 address string, but as a DNS names comprised of digits, not as an IP address. ExternalNames that resemble IPv4 addresses are not resolved by CoreDNS or ingress-nginx because ExternalName is intended to specify a canonical DNS name.\n\n从实现上来看，ExternalName 类型的 Service 其实就是在 CoreDNS 里的一条 CNAME 记录。 CNAME 是一条域名指向另一个域名的记录，在 K8s 中，这条 record 记载的就是 Service 名字指向 ExtenalName 的一个映射。\n\n但是，当 ExternalName 类型的 Service 中设定的是 IP 时，K8s 并没有对其进行判断，仍然允许其正常创建。\n\n同时，Nginx 本身存在着一个轮询机制，会不断的向 DNS 服务拉取记录进行缓存。\n[https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua](https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua)\n\n在每次拉取缓存时会发生以下的过程：\n1. Nginx 从 CoreDNS 拉取到了一个 CNAME 记录，例如：demo2 -> xxx.xxx.xxx.xxx\n2. 接着，Nginx 尝试解析 xxx.xxx.xxx.xxx 这个域名，CoreDNS 自然是对这个长成 IP 样子的域名解析不出来的，于是解析失败，导致报错\n\n------\n\n至于为什么 DNS 解析失败之后，Nginx 仍然能够成功转发请求，原因是 ingress-nginx controller 在实现上并没有对这个进行区分。\n\n首先，先简单介绍下 controller 的原理。Ingress-nginx controller 一直监听着 k8s 系统中的 ingress 资源。当有新的 ingress 创建时，controller 会开始更新 Nginx 的配置文件，向其中添加转发规则，并重启 Nginx。\n\n下面是 controller 解析指向 ExternalName 的 ingress，然后创建 upstream 的逻辑\n[https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52](https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52)\n\n![](/asset/ingress-nginx-bug-fix/nginx-code.png)\n\n可以看到，controller 是没有强制解析 ExternalName 成域名的，所以写进 nginx.conf 的 upstream 也是 ip 形式，这样 nginx 会自然地将 ExternalName 解析成 IP，从而可以正常工作。\n\n## 解决方法\n在上面提到的 Github Issue 的讨论中，有大佬已经给出了解决方法，就是通过 Service without selectors 的方式。\nhttps://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors\n\n常见的 K8s Service 都是通过标签选择器，选择一系列 Pod 作为后端，K8s endpoint controller 会自动根据 Service 的声明去为 Service 的每个端口创建一个 endpoint。Endpoint 是 K8s 中实际进行服务路由的资源。\n\n而创建 Service without selectors，就需要我们手动去创建一个与 Service 同名的 endpoint。这样就不需要指定 Service 为 ExternalName 的类型，CoreDNS 中就会将其视作一条 A 记录，而不是一条 CNAME 记录。Nginx 拉取 DNS 缓存时也不会把 IP 当做域名了。\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: demo2\n  namespace: default\nspec:\n  clusterIP: None\n  ports:\n  - name: grpc\n    port: 32443\n    protocol: TCP\n---\nkind: Endpoints\napiVersion: v1\nmetadata:\n  name: demo2\n  namespace: default\nsubsets:\n  - addresses:\n      - ip: xxx.xxx.xxx.xxx\n    ports:\n      - port: 32443\n        name: grpc\n        protocol: TCP\n```\n\n## 对比\n在两个对等的集群发生通信时，demo1 修复，demo2不修复，对比两侧的 CPU 使用情况\n\ndemo1：\n![](/asset/ingress-nginx-bug-fix/demo1.png)\n\ndemo2：\n![](/asset/ingress-nginx-bug-fix/demo2.png)\n\ndemo2 大约比 demo1 消耗 CPU 多 0.020 个核。虽然这个报错会稍微增加一点 CPU 的使用量，但并不多。\n","slug":"ingress-nginx-bug-fix","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clapldfd8000c18mzedql2j54","content":"<h2 id=\"问题\"><a class=\"markdownIt-Anchor\" href=\"#问题\"></a> 问题</h2>\n<p>目前，当指定访问集群外部地址为 IP 时，ingress-nginx controller 的日志中存在大量的 DNS 报错的垃圾日志。虽然不影响正常运行（猜测可能会导致性能波动，对比见最后），但是查看 Nginx 日志 debug 时效率严重降低。</p>\n<p><img src=\"/asset/ingress-nginx-bug-fix/error.png\" alt=\"\" /></p>\n<h2 id=\"原因\"><a class=\"markdownIt-Anchor\" href=\"#原因\"></a> 原因</h2>\n<p>详细的讨论见：<br />\n<a href=\"https://github.com/coredns/coredns/issues/2324\">https://github.com/coredns/coredns/issues/2324</a></p>\n<p>简略总结下，导致访问外部 IP，Nginx 报 DNS 解析错误的原因在于 Kubernetes 自身的bug，缺少了一个验证。</p>\n<p>出现问题的情况是通过 ExternalName 类型的 Service 访问外部服务的。定义的 yaml 类似于下面这种：</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Service</span>\n<span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">v1</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">demo2</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">type:</span> <span class=\"hljs-string\">ExternalName</span>\n  <span class=\"hljs-attr\">externalName:</span> <span class=\"hljs-string\">xxx.xxx.xxx.xxx</span>\n</code></pre>\n<p>对于 Kubernetes 的设计来说，ExternalName 就是一个域名。K8s 官方是这样介绍的</p>\n<blockquote>\n<p>ExternalName: Maps the service to the contents of the externalName field (e.g. <a href=\"http://foo.bar.example.com\">foo.bar.example.com</a>), by returning aCNAMErecord with its value. No proxying of any kind is set up.</p>\n</blockquote>\n<blockquote>\n<p>Note:ExternalName accepts an IPv4 address string, but as a DNS names comprised of digits, not as an IP address. ExternalNames that resemble IPv4 addresses are not resolved by CoreDNS or ingress-nginx because ExternalName is intended to specify a canonical DNS name.</p>\n</blockquote>\n<p>从实现上来看，ExternalName 类型的 Service 其实就是在 CoreDNS 里的一条 CNAME 记录。 CNAME 是一条域名指向另一个域名的记录，在 K8s 中，这条 record 记载的就是 Service 名字指向 ExtenalName 的一个映射。</p>\n<p>但是，当 ExternalName 类型的 Service 中设定的是 IP 时，K8s 并没有对其进行判断，仍然允许其正常创建。</p>\n<p>同时，Nginx 本身存在着一个轮询机制，会不断的向 DNS 服务拉取记录进行缓存。<br />\n<a href=\"https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua\">https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua</a></p>\n<p>在每次拉取缓存时会发生以下的过程：</p>\n<ol>\n<li>Nginx 从 CoreDNS 拉取到了一个 CNAME 记录，例如：demo2 -&gt; <a href=\"http://xxx.xxx.xxx.xxx\">xxx.xxx.xxx.xxx</a></li>\n<li>接着，Nginx 尝试解析 <a href=\"http://xxx.xxx.xxx.xxx\">xxx.xxx.xxx.xxx</a> 这个域名，CoreDNS 自然是对这个长成 IP 样子的域名解析不出来的，于是解析失败，导致报错</li>\n</ol>\n<hr />\n<p>至于为什么 DNS 解析失败之后，Nginx 仍然能够成功转发请求，原因是 ingress-nginx controller 在实现上并没有对这个进行区分。</p>\n<p>首先，先简单介绍下 controller 的原理。Ingress-nginx controller 一直监听着 k8s 系统中的 ingress 资源。当有新的 ingress 创建时，controller 会开始更新 Nginx 的配置文件，向其中添加转发规则，并重启 Nginx。</p>\n<p>下面是 controller 解析指向 ExternalName 的 ingress，然后创建 upstream 的逻辑<br />\n<a href=\"https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52\">https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52</a></p>\n<p><img src=\"/asset/ingress-nginx-bug-fix/nginx-code.png\" alt=\"\" /></p>\n<p>可以看到，controller 是没有强制解析 ExternalName 成域名的，所以写进 nginx.conf 的 upstream 也是 ip 形式，这样 nginx 会自然地将 ExternalName 解析成 IP，从而可以正常工作。</p>\n<h2 id=\"解决方法\"><a class=\"markdownIt-Anchor\" href=\"#解决方法\"></a> 解决方法</h2>\n<p>在上面提到的 Github Issue 的讨论中，有大佬已经给出了解决方法，就是通过 Service without selectors 的方式。<br />\n<a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors\">https://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors</a></p>\n<p>常见的 K8s Service 都是通过标签选择器，选择一系列 Pod 作为后端，K8s endpoint controller 会自动根据 Service 的声明去为 Service 的每个端口创建一个 endpoint。Endpoint 是 K8s 中实际进行服务路由的资源。</p>\n<p>而创建 Service without selectors，就需要我们手动去创建一个与 Service 同名的 endpoint。这样就不需要指定 Service 为 ExternalName 的类型，CoreDNS 中就会将其视作一条 A 记录，而不是一条 CNAME 记录。Nginx 拉取 DNS 缓存时也不会把 IP 当做域名了。</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">v1</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Service</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">demo2</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">clusterIP:</span> <span class=\"hljs-string\">None</span>\n  <span class=\"hljs-attr\">ports:</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">grpc</span>\n    <span class=\"hljs-attr\">port:</span> <span class=\"hljs-number\">32443</span>\n    <span class=\"hljs-attr\">protocol:</span> <span class=\"hljs-string\">TCP</span>\n<span class=\"hljs-meta\">---</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Endpoints</span>\n<span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">v1</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">demo2</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">subsets:</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">addresses:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">ip:</span> <span class=\"hljs-string\">xxx.xxx.xxx.xxx</span>\n    <span class=\"hljs-attr\">ports:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">port:</span> <span class=\"hljs-number\">32443</span>\n        <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">grpc</span>\n        <span class=\"hljs-attr\">protocol:</span> <span class=\"hljs-string\">TCP</span>\n</code></pre>\n<h2 id=\"对比\"><a class=\"markdownIt-Anchor\" href=\"#对比\"></a> 对比</h2>\n<p>在两个对等的集群发生通信时，demo1 修复，demo2不修复，对比两侧的 CPU 使用情况</p>\n<p>demo1：<br />\n<img src=\"/asset/ingress-nginx-bug-fix/demo1.png\" alt=\"\" /></p>\n<p>demo2：<br />\n<img src=\"/asset/ingress-nginx-bug-fix/demo2.png\" alt=\"\" /></p>\n<p>demo2 大约比 demo1 消耗 CPU 多 0.020 个核。虽然这个报错会稍微增加一点 CPU 的使用量，但并不多。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"问题\"><a class=\"markdownIt-Anchor\" href=\"#问题\"></a> 问题</h2>\n<p>目前，当指定访问集群外部地址为 IP 时，ingress-nginx controller 的日志中存在大量的 DNS 报错的垃圾日志。虽然不影响正常运行（猜测可能会导致性能波动，对比见最后），但是查看 Nginx 日志 debug 时效率严重降低。</p>\n<p><img src=\"/asset/ingress-nginx-bug-fix/error.png\" alt=\"\" /></p>\n<h2 id=\"原因\"><a class=\"markdownIt-Anchor\" href=\"#原因\"></a> 原因</h2>\n<p>详细的讨论见：<br />\n<a href=\"https://github.com/coredns/coredns/issues/2324\">https://github.com/coredns/coredns/issues/2324</a></p>\n<p>简略总结下，导致访问外部 IP，Nginx 报 DNS 解析错误的原因在于 Kubernetes 自身的bug，缺少了一个验证。</p>\n<p>出现问题的情况是通过 ExternalName 类型的 Service 访问外部服务的。定义的 yaml 类似于下面这种：</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Service</span>\n<span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">v1</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">demo2</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">type:</span> <span class=\"hljs-string\">ExternalName</span>\n  <span class=\"hljs-attr\">externalName:</span> <span class=\"hljs-string\">xxx.xxx.xxx.xxx</span>\n</code></pre>\n<p>对于 Kubernetes 的设计来说，ExternalName 就是一个域名。K8s 官方是这样介绍的</p>\n<blockquote>\n<p>ExternalName: Maps the service to the contents of the externalName field (e.g. <a href=\"http://foo.bar.example.com\">foo.bar.example.com</a>), by returning aCNAMErecord with its value. No proxying of any kind is set up.</p>\n</blockquote>\n<blockquote>\n<p>Note:ExternalName accepts an IPv4 address string, but as a DNS names comprised of digits, not as an IP address. ExternalNames that resemble IPv4 addresses are not resolved by CoreDNS or ingress-nginx because ExternalName is intended to specify a canonical DNS name.</p>\n</blockquote>\n<p>从实现上来看，ExternalName 类型的 Service 其实就是在 CoreDNS 里的一条 CNAME 记录。 CNAME 是一条域名指向另一个域名的记录，在 K8s 中，这条 record 记载的就是 Service 名字指向 ExtenalName 的一个映射。</p>\n<p>但是，当 ExternalName 类型的 Service 中设定的是 IP 时，K8s 并没有对其进行判断，仍然允许其正常创建。</p>\n<p>同时，Nginx 本身存在着一个轮询机制，会不断的向 DNS 服务拉取记录进行缓存。<br />\n<a href=\"https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua\">https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua</a></p>\n<p>在每次拉取缓存时会发生以下的过程：</p>\n<ol>\n<li>Nginx 从 CoreDNS 拉取到了一个 CNAME 记录，例如：demo2 -&gt; <a href=\"http://xxx.xxx.xxx.xxx\">xxx.xxx.xxx.xxx</a></li>\n<li>接着，Nginx 尝试解析 <a href=\"http://xxx.xxx.xxx.xxx\">xxx.xxx.xxx.xxx</a> 这个域名，CoreDNS 自然是对这个长成 IP 样子的域名解析不出来的，于是解析失败，导致报错</li>\n</ol>\n<hr />\n<p>至于为什么 DNS 解析失败之后，Nginx 仍然能够成功转发请求，原因是 ingress-nginx controller 在实现上并没有对这个进行区分。</p>\n<p>首先，先简单介绍下 controller 的原理。Ingress-nginx controller 一直监听着 k8s 系统中的 ingress 资源。当有新的 ingress 创建时，controller 会开始更新 Nginx 的配置文件，向其中添加转发规则，并重启 Nginx。</p>\n<p>下面是 controller 解析指向 ExternalName 的 ingress，然后创建 upstream 的逻辑<br />\n<a href=\"https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52\">https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52</a></p>\n<p><img src=\"/asset/ingress-nginx-bug-fix/nginx-code.png\" alt=\"\" /></p>\n<p>可以看到，controller 是没有强制解析 ExternalName 成域名的，所以写进 nginx.conf 的 upstream 也是 ip 形式，这样 nginx 会自然地将 ExternalName 解析成 IP，从而可以正常工作。</p>\n<h2 id=\"解决方法\"><a class=\"markdownIt-Anchor\" href=\"#解决方法\"></a> 解决方法</h2>\n<p>在上面提到的 Github Issue 的讨论中，有大佬已经给出了解决方法，就是通过 Service without selectors 的方式。<br />\n<a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors\">https://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors</a></p>\n<p>常见的 K8s Service 都是通过标签选择器，选择一系列 Pod 作为后端，K8s endpoint controller 会自动根据 Service 的声明去为 Service 的每个端口创建一个 endpoint。Endpoint 是 K8s 中实际进行服务路由的资源。</p>\n<p>而创建 Service without selectors，就需要我们手动去创建一个与 Service 同名的 endpoint。这样就不需要指定 Service 为 ExternalName 的类型，CoreDNS 中就会将其视作一条 A 记录，而不是一条 CNAME 记录。Nginx 拉取 DNS 缓存时也不会把 IP 当做域名了。</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">v1</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Service</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">demo2</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">clusterIP:</span> <span class=\"hljs-string\">None</span>\n  <span class=\"hljs-attr\">ports:</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">grpc</span>\n    <span class=\"hljs-attr\">port:</span> <span class=\"hljs-number\">32443</span>\n    <span class=\"hljs-attr\">protocol:</span> <span class=\"hljs-string\">TCP</span>\n<span class=\"hljs-meta\">---</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Endpoints</span>\n<span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">v1</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">demo2</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">subsets:</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">addresses:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">ip:</span> <span class=\"hljs-string\">xxx.xxx.xxx.xxx</span>\n    <span class=\"hljs-attr\">ports:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">port:</span> <span class=\"hljs-number\">32443</span>\n        <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">grpc</span>\n        <span class=\"hljs-attr\">protocol:</span> <span class=\"hljs-string\">TCP</span>\n</code></pre>\n<h2 id=\"对比\"><a class=\"markdownIt-Anchor\" href=\"#对比\"></a> 对比</h2>\n<p>在两个对等的集群发生通信时，demo1 修复，demo2不修复，对比两侧的 CPU 使用情况</p>\n<p>demo1：<br />\n<img src=\"/asset/ingress-nginx-bug-fix/demo1.png\" alt=\"\" /></p>\n<p>demo2：<br />\n<img src=\"/asset/ingress-nginx-bug-fix/demo2.png\" alt=\"\" /></p>\n<p>demo2 大约比 demo1 消耗 CPU 多 0.020 个核。虽然这个报错会稍微增加一点 CPU 的使用量，但并不多。</p>\n"},{"title":"[Introduction] Kubernetes CronJob","date":"2021-03-08T00:06:14.000Z","updated":"2021-03-08T00:06:14.000Z","_content":"本文主要介绍下 K8s 的 CronJob，还有其中的一些小坑。\n\n## 概念\nCronjob 是 K8s 定时通过 cronjob controller 定时去创建 Job 实现的。\n创建 Cronjob 的一个例子：\n```yaml\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"*/1 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox\n            imagePullPolicy: IfNotPresent\n            args:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n```\n\n## Cronjob controller 工作原理\nstartingDeadlineSeconds 是一个很重要的参数，其配置了一个周期创建job时，多长时间算作失败。\n1. Controller 每10秒轮询一次 cronjob\n2. 对于每个 cronjob，计算从上次被调度 lastScheduleTime 到现在错过了多少次调度。如果大于100次，则将其状态置为 FailedNeedsStart\n3. 对于其他 cronjob 计算当前是否还在其 lastScheduleTime + startingDeadlineSeconds 内，如果在，则进行调度。如果不在，则发送一条 event\n\"Missed starting window for {cronjob name}. Missed scheduled time to start a job {scheduledTime}\"\n\n## Tips\n1. 时间\n因为 Cronjob 实际上是通过 controller 去管理的，所以其时间是 kube-controller-manager 的时间。\n2. 命名\nCronjob 的命名要遵循 DNS subdomain name，并且不能超过 52 个字符。因为 Cronjob Controller 会在其创建的 Job 后再拼接 11 个字符 （ K8s 对 Job 命名的限制是 63 个字符）\n3. 幂等性\n根据配置的重启和健康规则的不同，K8s 启动 Cronjob 时只能保证 about 一次，有时可能会启动多次，有时可能会没有启动，所以需要 cronjob 保证幂等性。\n以下是 cronjob 可能会发生的两种异常情况：\n  1. 触发多次\nconcurrencyPolicy 配置为 Allow，并且 startingDeadlineSeconds 不设置或者设置为很大。这样，controller 在多次轮询中可能都会查看到 cronjob 符合再次运行的条件，从而创建多个 job。\n  2. 触发0次\nconcurrencyPolicy 配置为 Forbid，这样 controller 就会等到上一次cronjob结束之后才会进行下一次调度。\n4. 自定义 Crontroller\n从 kubernetes 1.20 开始支持\n5. 删除运行成功的 Job\n配置 https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/\n  successfulJobsHistoryLimit: 0\n  failedJobsHistoryLimit: 0\n\n","source":"_posts/k8s-cronjob.md","raw":"---\ntitle: \"[Introduction] Kubernetes CronJob\"\ndate: 2021-03-08 00:06:14\nupdated: 2021-03-08 00:06:14\n---\n本文主要介绍下 K8s 的 CronJob，还有其中的一些小坑。\n\n## 概念\nCronjob 是 K8s 定时通过 cronjob controller 定时去创建 Job 实现的。\n创建 Cronjob 的一个例子：\n```yaml\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"*/1 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox\n            imagePullPolicy: IfNotPresent\n            args:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n```\n\n## Cronjob controller 工作原理\nstartingDeadlineSeconds 是一个很重要的参数，其配置了一个周期创建job时，多长时间算作失败。\n1. Controller 每10秒轮询一次 cronjob\n2. 对于每个 cronjob，计算从上次被调度 lastScheduleTime 到现在错过了多少次调度。如果大于100次，则将其状态置为 FailedNeedsStart\n3. 对于其他 cronjob 计算当前是否还在其 lastScheduleTime + startingDeadlineSeconds 内，如果在，则进行调度。如果不在，则发送一条 event\n\"Missed starting window for {cronjob name}. Missed scheduled time to start a job {scheduledTime}\"\n\n## Tips\n1. 时间\n因为 Cronjob 实际上是通过 controller 去管理的，所以其时间是 kube-controller-manager 的时间。\n2. 命名\nCronjob 的命名要遵循 DNS subdomain name，并且不能超过 52 个字符。因为 Cronjob Controller 会在其创建的 Job 后再拼接 11 个字符 （ K8s 对 Job 命名的限制是 63 个字符）\n3. 幂等性\n根据配置的重启和健康规则的不同，K8s 启动 Cronjob 时只能保证 about 一次，有时可能会启动多次，有时可能会没有启动，所以需要 cronjob 保证幂等性。\n以下是 cronjob 可能会发生的两种异常情况：\n  1. 触发多次\nconcurrencyPolicy 配置为 Allow，并且 startingDeadlineSeconds 不设置或者设置为很大。这样，controller 在多次轮询中可能都会查看到 cronjob 符合再次运行的条件，从而创建多个 job。\n  2. 触发0次\nconcurrencyPolicy 配置为 Forbid，这样 controller 就会等到上一次cronjob结束之后才会进行下一次调度。\n4. 自定义 Crontroller\n从 kubernetes 1.20 开始支持\n5. 删除运行成功的 Job\n配置 https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/\n  successfulJobsHistoryLimit: 0\n  failedJobsHistoryLimit: 0\n\n","slug":"k8s-cronjob","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clapldfda000e18mz1trme7ph","content":"<p>本文主要介绍下 K8s 的 CronJob，还有其中的一些小坑。</p>\n<h2 id=\"概念\"><a class=\"markdownIt-Anchor\" href=\"#概念\"></a> 概念</h2>\n<p>Cronjob 是 K8s 定时通过 cronjob controller 定时去创建 Job 实现的。<br />\n创建 Cronjob 的一个例子：</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">batch/v1beta1</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">CronJob</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">hello</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">schedule:</span> <span class=\"hljs-string\">&quot;*/1 * * * *&quot;</span>\n  <span class=\"hljs-attr\">jobTemplate:</span>\n    <span class=\"hljs-attr\">spec:</span>\n      <span class=\"hljs-attr\">template:</span>\n        <span class=\"hljs-attr\">spec:</span>\n          <span class=\"hljs-attr\">containers:</span>\n          <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">hello</span>\n            <span class=\"hljs-attr\">image:</span> <span class=\"hljs-string\">busybox</span>\n            <span class=\"hljs-attr\">imagePullPolicy:</span> <span class=\"hljs-string\">IfNotPresent</span>\n            <span class=\"hljs-attr\">args:</span>\n            <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">/bin/sh</span>\n            <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">-c</span>\n            <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">date;</span> <span class=\"hljs-string\">echo</span> <span class=\"hljs-string\">Hello</span> <span class=\"hljs-string\">from</span> <span class=\"hljs-string\">the</span> <span class=\"hljs-string\">Kubernetes</span> <span class=\"hljs-string\">cluster</span>\n          <span class=\"hljs-attr\">restartPolicy:</span> <span class=\"hljs-string\">OnFailure</span>\n</code></pre>\n<h2 id=\"cronjob-controller-工作原理\"><a class=\"markdownIt-Anchor\" href=\"#cronjob-controller-工作原理\"></a> Cronjob controller 工作原理</h2>\n<p>startingDeadlineSeconds 是一个很重要的参数，其配置了一个周期创建job时，多长时间算作失败。</p>\n<ol>\n<li>Controller 每10秒轮询一次 cronjob</li>\n<li>对于每个 cronjob，计算从上次被调度 lastScheduleTime 到现在错过了多少次调度。如果大于100次，则将其状态置为 FailedNeedsStart</li>\n<li>对于其他 cronjob 计算当前是否还在其 lastScheduleTime + startingDeadlineSeconds 内，如果在，则进行调度。如果不在，则发送一条 event<br />\n“Missed starting window for {cronjob name}. Missed scheduled time to start a job {scheduledTime}”</li>\n</ol>\n<h2 id=\"tips\"><a class=\"markdownIt-Anchor\" href=\"#tips\"></a> Tips</h2>\n<ol>\n<li>时间<br />\n因为 Cronjob 实际上是通过 controller 去管理的，所以其时间是 kube-controller-manager 的时间。</li>\n<li>命名<br />\nCronjob 的命名要遵循 DNS subdomain name，并且不能超过 52 个字符。因为 Cronjob Controller 会在其创建的 Job 后再拼接 11 个字符 （ K8s 对 Job 命名的限制是 63 个字符）</li>\n<li>幂等性<br />\n根据配置的重启和健康规则的不同，K8s 启动 Cronjob 时只能保证 about 一次，有时可能会启动多次，有时可能会没有启动，所以需要 cronjob 保证幂等性。<br />\n以下是 cronjob 可能会发生的两种异常情况：</li>\n<li>触发多次<br />\nconcurrencyPolicy 配置为 Allow，并且 startingDeadlineSeconds 不设置或者设置为很大。这样，controller 在多次轮询中可能都会查看到 cronjob 符合再次运行的条件，从而创建多个 job。</li>\n<li>触发0次<br />\nconcurrencyPolicy 配置为 Forbid，这样 controller 就会等到上一次cronjob结束之后才会进行下一次调度。</li>\n<li>自定义 Crontroller<br />\n从 kubernetes 1.20 开始支持</li>\n<li>删除运行成功的 Job<br />\n配置 <a href=\"https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/\">https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/</a><br />\nsuccessfulJobsHistoryLimit: 0<br />\nfailedJobsHistoryLimit: 0</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>本文主要介绍下 K8s 的 CronJob，还有其中的一些小坑。</p>\n<h2 id=\"概念\"><a class=\"markdownIt-Anchor\" href=\"#概念\"></a> 概念</h2>\n<p>Cronjob 是 K8s 定时通过 cronjob controller 定时去创建 Job 实现的。<br />\n创建 Cronjob 的一个例子：</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">batch/v1beta1</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">CronJob</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">hello</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">schedule:</span> <span class=\"hljs-string\">&quot;*/1 * * * *&quot;</span>\n  <span class=\"hljs-attr\">jobTemplate:</span>\n    <span class=\"hljs-attr\">spec:</span>\n      <span class=\"hljs-attr\">template:</span>\n        <span class=\"hljs-attr\">spec:</span>\n          <span class=\"hljs-attr\">containers:</span>\n          <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">hello</span>\n            <span class=\"hljs-attr\">image:</span> <span class=\"hljs-string\">busybox</span>\n            <span class=\"hljs-attr\">imagePullPolicy:</span> <span class=\"hljs-string\">IfNotPresent</span>\n            <span class=\"hljs-attr\">args:</span>\n            <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">/bin/sh</span>\n            <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">-c</span>\n            <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">date;</span> <span class=\"hljs-string\">echo</span> <span class=\"hljs-string\">Hello</span> <span class=\"hljs-string\">from</span> <span class=\"hljs-string\">the</span> <span class=\"hljs-string\">Kubernetes</span> <span class=\"hljs-string\">cluster</span>\n          <span class=\"hljs-attr\">restartPolicy:</span> <span class=\"hljs-string\">OnFailure</span>\n</code></pre>\n<h2 id=\"cronjob-controller-工作原理\"><a class=\"markdownIt-Anchor\" href=\"#cronjob-controller-工作原理\"></a> Cronjob controller 工作原理</h2>\n<p>startingDeadlineSeconds 是一个很重要的参数，其配置了一个周期创建job时，多长时间算作失败。</p>\n<ol>\n<li>Controller 每10秒轮询一次 cronjob</li>\n<li>对于每个 cronjob，计算从上次被调度 lastScheduleTime 到现在错过了多少次调度。如果大于100次，则将其状态置为 FailedNeedsStart</li>\n<li>对于其他 cronjob 计算当前是否还在其 lastScheduleTime + startingDeadlineSeconds 内，如果在，则进行调度。如果不在，则发送一条 event<br />\n“Missed starting window for {cronjob name}. Missed scheduled time to start a job {scheduledTime}”</li>\n</ol>\n<h2 id=\"tips\"><a class=\"markdownIt-Anchor\" href=\"#tips\"></a> Tips</h2>\n<ol>\n<li>时间<br />\n因为 Cronjob 实际上是通过 controller 去管理的，所以其时间是 kube-controller-manager 的时间。</li>\n<li>命名<br />\nCronjob 的命名要遵循 DNS subdomain name，并且不能超过 52 个字符。因为 Cronjob Controller 会在其创建的 Job 后再拼接 11 个字符 （ K8s 对 Job 命名的限制是 63 个字符）</li>\n<li>幂等性<br />\n根据配置的重启和健康规则的不同，K8s 启动 Cronjob 时只能保证 about 一次，有时可能会启动多次，有时可能会没有启动，所以需要 cronjob 保证幂等性。<br />\n以下是 cronjob 可能会发生的两种异常情况：</li>\n<li>触发多次<br />\nconcurrencyPolicy 配置为 Allow，并且 startingDeadlineSeconds 不设置或者设置为很大。这样，controller 在多次轮询中可能都会查看到 cronjob 符合再次运行的条件，从而创建多个 job。</li>\n<li>触发0次<br />\nconcurrencyPolicy 配置为 Forbid，这样 controller 就会等到上一次cronjob结束之后才会进行下一次调度。</li>\n<li>自定义 Crontroller<br />\n从 kubernetes 1.20 开始支持</li>\n<li>删除运行成功的 Job<br />\n配置 <a href=\"https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/\">https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/</a><br />\nsuccessfulJobsHistoryLimit: 0<br />\nfailedJobsHistoryLimit: 0</li>\n</ol>\n"},{"title":"[Introduction] Kubernetes Ramp Up","date":"2021-03-15T15:39:36.000Z","updated":"2021-03-15T15:39:36.000Z","_content":"## 目标\n- 介绍 K8s，Docker 概念以及原理\n- 从 0 开始部署一个简单完整的服务\n\n## Docker是什么？\nDocker是由Google推出的Go语言进行开发实现，基于Linux内核的 <font color=red>namespace</font>，对<font color=red>进程</font>进行封装<font color=red>隔离</font>，属于操作系统层面的容器化技术。\n\n![](/asset/k8s-ramp-up/1.png)\n\n### 三大核心概念\n镜像（Image）\n\n容器（Container）\n\n仓库（Repository）\n\n从代码的角度来看，镜像就像一个类；容器是对象实例，运行时在系统中会有许多容器；仓库主要用于存储和维护这些镜像。\n\n### 为什么使用 Docker？\n- 配置环境\n开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性\n- 应用隔离\n机器上可能同时运行多个服务。如果服务之间没有隔离，一个服务出现异常，往往可能会导致其他服务也挂掉。同时，不同服务所依赖的环境也可能发生冲突。\n\n### 原理\n首先，要了解一下进程的命名空间。Linux 系统中的所有进程按照惯例是通过PID标识的，这意味着内核必须管理一个全局的PID列表。而且，所有调用者通过uname系统调用返回的系统相关信息（包括系统名称和有关内核的一些信息）都是相同的。\n\nLinux 的命名空间从内核层面上进行了虚拟化，对所有的全局资源进行一个抽象。本质上，建立了系统的不同视图。每一项全局资源都必须包装到命名空间的数据结构中，只有资源和包含资源的命名空间构成的二元组仍然是全局唯一的。不仅仅是 PID，Linux 通过同样的方法对其他资源也做了虚拟化处理。命名空间共有以下6种：\n\n![](/asset/k8s-ramp-up/2.png)\n\n借助 Linux 的命名空间，Docker 对进程进行隔离，可以从进程树的角度理解。\n\n![](/asset/k8s-ramp-up/3.png)\n\n每次在执行 `docker start` 或 `docker run` 的时候，其实是由 docker 的 daemon 进程 docker containerd，调用 Linux 系统调用 `clone()` 去创建新的进程。而创建进程的过程中就为新创建的进程分配了新的 Linux 命名空间。可以简单阅读一下 docker 的开源代码\n\n<pre><code class=\"go\">// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L17\n// 创建容器的函数，其中又调用了设置\nfunc (daemon *Daemon) ContainerStart(name string, hostConfig *containertypes.HostConfig, checkpoint string, checkpointDir string) error\n\n// https://github.com/moby/moby/blob/470ae8422fc6f1845288eb7572253b08f1e6edf8/daemon/oci_linux.go#L212\n// 设置 Namespace\nfunc setNamespace(s *specs.Spec, ns specs.LinuxNamespace) {\n   for i, n := range s.Linux.Namespaces {\n      if n.Type == ns.Type {\n         s.Linux.Namespaces[i] = ns\n         return\n      }\n   }\n   s.Linux.Namespaces = append(s.Linux.Namespaces, ns)\n}\n\n// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L198\n// 创建新的进程\npid, err := daemon.containerd.Start(context.Background(), \n                                    container.ID, \n                                    checkpointDir,    \n                                    container.StreamConfig.Stdin() != nil | | container.Config.Tty, \n                                    container.InitializeStdio)\n</code></pre>\n\n### 如何安装？\n[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)\n\n## Kubernetes是什么？\nKubernetes 是 Google 于 2014 年基于其内部 Brog 系统开源的一个容器编排管理系统，可使用声明式的配置（以 yaml 文件的形式）自动地执行容器化应用程序的管理，包括部署、伸缩、负载均衡、回滚等。\n\n为什么叫 K8s？因为 K<font color=red>ubernete</font>s，中间是8个字母。\n\nkubernetes 提供的功能：\n- 自动发布与伸缩：可以通过声明式的配置文件定义想要部署的容器\n- 滚动升级与灰度发布：采用逐步替换的策略实现滚动升级\n- 服务发现与负载均衡：Kubernetes 通过 DNS 名称或 IP 地址暴露容器的访问方式，并且可在同一容器组内实现负载分发与均衡\n- 存储编排：Kubernetes 可以自动挂载指定的存储系统，如 local storage/nfs / 云存储等\n- 故障恢复：Kubernetes 自动重启已经停机的容器，替换不满足健康检查的容器\n- 密钥与配置管理：Kubernetes 可以存储与管理敏感信息，如 Docker Registry 的登录凭证，密码，ssh 密钥等\n\n### 为什么使用 K8s？\n大型单体应用被逐渐拆分成小的、可独立运行的组件。随着部署组件的增多和数据中心的增长，配置、管理和运维变得很困难。(微服务）\n\nK8s 的定义就是容器编排和管理引擎，解决了这些问题。\n\n### 如何安装？\n由难到易(๑•̀ㅂ•́)و✧\n- Kubeadm: https://kubernetes.io/docs/reference/setup-tools/kubeadm/\n- MiniKube: Local kubernetes https://minikube.sigs.k8s.io/docs/start/\n- Kind: Kubernetes in Docker https://github.com/kubernetes-sigs/kind\n- Docker-desktop（仅限 Mac）: 一键开启\n![](/asset/k8s-ramp-up/4.png)\n\n\n其他版本的类 K8s 系统：\n- K3s: https://github.com/k3s-io/k3s\n- K0s: https://github.com/k0sproject/k0s\n\n## Kubernetes 架构\n\n![](/asset/k8s-ramp-up/5.png)\n\n### master\n\nMaster 负责管理服务来对整个系统进行管理与控制，包括\n- apiserver：作为整个系统的对外接口，提供一套 Restful API 供客户端调用，任何的资源请求 / 调用操作都是通过 kube-apiserver 提供的接口进行, 如 kubectl、kubernetes dashboard 等管理工具就是通过 apiserver 来实现对集群的管理\n- kube-scheduler：资源调度器，负责将容器组分配到哪些节点上\n- kube-controller-manager：管理控制器，集群中处理常规任务的后台线程，包括节点控制器（负责监听节点停机的事件并作出对应响应）、endpoint-controller（刷新服务与容器组的关联信息）、replication-controller（维护容器组的副本数为指定的数值）、Service Account & Token 控制器（负责为新的命名空间创建默认的 Service Account 以及 API Access Token）\n- etcd：数据存储，存储集群所有的配置信息\n- coredns：实现集群内部通过服务名称进行容器组访问的功能\n\n### worker\n\nWorker 负载执行 Master 分配的任务，包括\n- kubelet：是工作节点上执行操作的代理程序，负责容器的生命周期管理，定期执行容器健康检查，并上报容器的运行状态\n- kube-proxy：是一个具有负载均衡能力的简单的网络访问代理，负责将访问某个服务的请求分配到工作节点的具体某个容器上（kube-proxy 也运行于 master node 上）\n- Docker Daemon：Kubernetes 其实不局限于 Docker（即将取消），它支持任何实现了 Kubernetes 容器引擎接口的容器引擎，如 containerd、rktlet\n\n### 网络通信\n网络通信组件只需要符合 CNI （Container Network Interface）接口规范，主要作用在于给各个容器分配集群内 IP，使得其内网 IP 能够集群内唯一，并且可以互相访问，目前常用的有 Flannel，Calico等网络组件。\n\n简单介绍下比较常用的 Flannel 的原理。Flannel 运行在第3层网络层，基于 IPv4，创建一个大型内部网络，跨越集群中每个节点。每个节点组成一个子网，每个容器在内网中有唯一的IP。\n\n首先，Flannel 会为每台节点分配一个子网段。Flanneld 在 Docker 容器启动时修改其启动参数，将其 IP 限制在当前的子网段内，具体 IP 的分配仍是由 docker 进行。Flannel通过Etcd服务维护了一张节点间的路由表，详细记录了各节点子网网段，保证不同节点的子网网段不会重复。\n\n数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，flanneld服务监听在网卡的另外一端。\n\n源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。\n\n![](/asset/k8s-ramp-up/6.png)\n\n## 快速上手 K8s 概念\n\n一些推荐的 K8s 概念介绍：\n- 微软的 50天 K8s 教程中（https://azure.microsoft.com/en-us/resources/kubernetes-learning-path/）通过动物园的形式介绍了一些 K8s 概念 http://aka.ms/k8s/LearnwithPhippy\n- 综述PPT：https://www2.slideshare.net/BobKillen/kubernetes-a-comprehensive-overview-updated?from_action=save\n\nK8s 中的概念极多，比较零碎，这里通过一个简单的小例子，尽可能覆盖多的 K8s 概念。\n\n## 概览\n\n例子使用一个开源的 fortune-teller 镜像（`quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1`） ，每次请求容器内的服务，服务会返回一句名言。希望在 MacOS 的环境下，展示一个应用在 K8s 中运行的全流程。\n\n准备环境\n为了不影响大家本地的环境，这里使用 Kind 创建出一个独立的 K8s 集群，方便统一版本并且可以在完成快速清理掉。(Docker 双重隔离）\n\n1. 安装 Kind 以及 gRpc 测试工具\n```bash\nbrew install kind\nbrew install grpcurl\n```\n\n2. 拉取镜像\n```bash\ndocker pull kindest/node:v1.16.15\ndocker pull quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n```\n\n3. 创建 K8s 集群，因为 Kind 是在 Docker 容器里面创建的 K8s，所以宿主机访问，需要把端口暴露出来。Kind 会默认把 K8s apiserver 的端口暴露出来，用来给 kubectl 命令使用。但为了之后的测试，我们提前把几个端口在创建的时候就暴露出来。\n\nKind 同样支持通过yaml 的形式创建集群\n```yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 32080\n    hostPort: 32080\n  - containerPort: 32443\n    hostPort: 32443\n```\n\n```bash\nkind create cluster --name=fortune-teller --image=kindest/node:v1.16.15 --config kind-config.yaml\n```\n\n### 运行 Docker 版本\n1. 启动容器\n```bash\ndocker run -p 50051:50051 quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n```\n\n`-p` 将容器内的 50051 端口映射到宿主机的 50051 端口\n\n2. 测试应用是否正常运行，第一次运行时可能需要给 grpcurl 开启权限\n```bash\ngrpcurl -plaintext 127.0.0.1:50051 build.stack.fortune.FortuneTeller/Predict\n```\n\n应用在收到请求以后，会返回一句名言\n\n![](/asset/k8s-ramp-up/7.png)\n\n### 将应用从 Docker 迁移到 K8s 中\n\n与 Docker 中容器概念相对应的，K8s 中也有着容器的概念。对于虚拟化的容器来说，最佳实践是一个容器一个应用，但当一个服务需要多个应用组合完成时，简单的将多个应用部署到一个容器内，就破坏了应用之间的隔离性，所以 K8s 对于容器进行了一层封装，形成了 Pod 的概念。\n\n#### Pod\n\nPod 是 Kubernetes 创建或部署的最小基本单元。一个 Pod 封装一个或多个应用容器、存储资源、一个独立的网络 IP 以及管理控制容器运行方式的策略选项。Pod 中的每个容器共享网络命名空间（包括 IP 与端口），Pod 内的容器可以使用 localhost 相互通信。Pod 可以指定一组共享存储卷 Volumes，Pod 中所有容器都可以访问共享的 Volumes。 \n\n通过 Pod，用户就可以非常方便地控制容器之间的隔离性。\n\n有了 Pod 作为基础以后，K8s 就要实现它最重要的功能，对容器的编排管理。当服务需要扩容时，K8s 需要能够快速复制 Pod，当 Pod 挂掉了，K8s 需要能够自动重启。所以 K8s 由此衍生出了 ReplicaSet 的概念。\n\n#### ReplicaSet\n\nReplicaSet 确保在任何时候都有按配置的 Pod 副本数在运行，通过标签选择器的方式对 Pod 进行筛选和管理。在旧的版本中还有一个 ReplicaController 的概念，RC 与 RS 两者功能完全相同，区别仅仅在于 RS 对于 Pod 的标签选择器更加强大。\n\n开头提到了 K8s 使用声明式的配置自动去管理容器，而 ReplicaSet 的内容却太过具体，涉及到了 Pod 的具体维护细节。所以 K8s 在 ReplicaSet 之上又衍生出声明式配置容器的概念，Deployment。\n\n#### Deployment\n\nDeployment 为 Pod 与 ReplicaSet 提供了声明式的定义，描述你想要的目标状态是什么，Deployment controller 就会帮你将 Pod 与 ReplicaSet 的实际状态改变到你想要的目标状态。\n\n以 fortune-teller 为例子，可以编写一份下面这样的 Deployment 配置文件\n```yaml\napiVersion: apps/v1 # k8s api版本\nkind: Deployment # 资源类型\nmetadata:\n  name: fortune-teller-app # deployment 名字\n  namespace: default\nspec:\n  replicas: 1 # Pod 副本数量\n  selector:\n    matchLabels:\n      k8s-app: fortune-teller-app # 管理标签中包含 k8s-app: fortune-teller-app 的 Pod\n  template: # Pod 模板\n    metadata:\n      labels:\n        k8s-app: fortune-teller-app # Pod 标签\n    spec: # Pod 配置\n      containers:\n      - image: quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n        imagePullPolicy: IfNotPresent\n        name: fortune-teller-app\n        ports:\n        - containerPort: 50051\n          name: grpc\n          protocol: TCP\n```\n\n将上面的内容保存到一份 yaml 文件中，执行以下命令，让 K8s 执行 yaml\n```bash\nkubectl apply -f deployment.yaml\n```\n\n通过以下命令，我们就可以看到刚刚创建的 deployment\n```bash\nkubectl get deployement\n```\n\n![](/asset/k8s-ramp-up/8.png)\n\n此时，K8s 已经自动根据 deployment 中配置的 Pod 模板和配置，创建了 Pod。通过以下命令，我们就可以看到 K8s 自动创建的 Pod\n```bash\nkubectl get pods\n```\n![](/asset/k8s-ramp-up/9.png)\n\n因为 K8s 采用声明式的配置去管理 Pod，所以我们可以动态地去修改 deployment 的配置，K8s 会自动根据新的配置去管理 Pod。\n```bash\nkubectl edit deployment fortune-teller-app\n```\n\n我们把配置文件中的副本数量修改为 2\n\n![](/asset/k8s-ramp-up/10.png)\n\n保存退出后，我们再次执行 kubectl get pods ，我们就可以看到 K8s 根据新的配置，创建了一个新的 Pod\n\n![](/asset/k8s-ramp-up/11.png)\n\n现在我们就有了两个 fortune-teller 的服务。在真实环境中，Pod 的调度由 K8s 进行管理，某个时刻服务可能在 Node1 上，而另一时刻服务可能就被调度到了 Node2 上。所以，访问具体 Pod 是一种不稳定的服务访问方法，而且目前大多数的后端服务都是无状态的服务，直接访问 Pod 也导致不能进行负载均衡。所以，K8s 在此基础上衍生出 Service 的概念。\n\n#### Service\n\nService 可以看做一组提供相同服务的 Pod 的对外访问接口。Kubernetes 提供三种类型的 Service：\n- NodePort： 集群外部可以通过 Node IP 与 Node Port 来访问具体某个 Pod，每台机器上都会暴露同样的端口\n- ClusterIP：指通过集群的内部 IP 暴露服务，服务只能够在集群内部可以访问，这也是默认的 ServiceType\n- ExternalName：不指向 Pod，指向外部服务\nService 和 Deployment 是一对比较容易混淆的概念，两者都是对一组 Pod 进行管理，但它们两者之间的关系可以用下面这张图来概括\n\n![](/asset/k8s-ramp-up/12.png)\n\nService 是面向服务调用者，也就是外部访问 K8s。而 Deployment 是面向 K8s 底层引擎的，面向内部管理者。\n\nService 的配置文件格式与 Deployment 很类似\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: fortune-teller-service\n  namespace: default\nspec:\n  ports:\n  - name: grpc\n    port: 50051\n    protocol: TCP\n    targetPort: 50051\n  selector:\n    k8s-app: fortune-teller-app\n  type: ClusterIP\n```\n\n同样的，我们通过 kubectl apply -f service.yaml 命令，可以创建 Service。通过 kubectl get service 可以查看到刚刚创建的 Service。\n\n![](/asset/k8s-ramp-up/13.png)\n\n接下来，我们登陆到一个 Pod 里去测试一下是否可以访问服务。\nNetshoot 镜像中包含了一些网络测试的工具，我们可以直接进入一个 netshoot 容器内测试。采用 deployment 的方式创建 Pod\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netshoot\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: netshoot\n  template:\n    metadata:\n      labels:\n        k8s-app: netshoot\n    spec:\n      containers:\n      - args:\n        - 1000d\n        command:\n        - /bin/sleep\n        image: nicolaka/netshoot\n        name: netshoot\n```\n\n与 Docker 的命令类似，使用命令 `kubectl cp grpcurl_1.6.0_linux_x86_64.tar.gz <pod name>:/` 复制工具到容器内。\n\n复制成功后，使用命令 kubectl exec -it <pod name> bash 可以进入到容器内。\n\n解压\n```bash\ncd /\ntar -zxf grpcurl_1.6.0_linux_x86_64.tar.gz\n```\n\n然后我们可以测试是否可以从 K8s 集群内访问 Service。对于 K8s 集群内部的服务，K8s 有自己的 DNS 组件，所以可以直接通过服务名访问。\n```bash\n./grpcurl -plaintext fortune-teller-service:50051 build.stack.fortune.FortuneTeller/Predict\n```\n\n![](/asset/k8s-ramp-up/14.png)\n\n验证服务可以从集群内访问之后，我们就需要解决如何从集群外访问服务的问题，毕竟大多数服务是面向 K8s 集群外的用户的。其实目前我们已经了解了一种解决方案，就是使用 Nodeport 类型的 Service。但采用这种方法有几个缺点：\n1. 每个端口只能是一种服务\n2. 端口范围只能是 30000-32767\n3. 如果节点 的 IP 地址发生变化，调用方需要能够察觉。\n所以，K8s 为服务的外部访问路由提供了新的类型 Ingress。\n\n#### Ingress\nIngress 其实是一种类似于路由表一样的配置，实际的路由工作需要 Ingress Controller 执行。K8s 本身并没有提供 Ingress Controller，目前常用的是通过 Nginx 实现的版本 https://github.com/kubernetes/ingress-nginx 。可以使用上面压缩包中的 ingress-controller.yaml 安装\n\n![](/asset/k8s-ramp-up/15.png)\n\nIngree nginx controller 通过宿主机暴露给外部访问的端口是随机的，所以我们修改 yaml，改成我们在一开始创建集群时映射的端口。\n\n通过 `kubectl get svc -n ingress-nginx` 我们就可以看到，暴露的是两个随机分配的端口\n\n![](/asset/k8s-ramp-up/16.png)\n\n我们手动将其改成 32080 和 32443。\n\n![](/asset/k8s-ramp-up/17.png)\n\nIngress nginx controller 同样是通过标签选择器的方式管理 Ingress。\n\n下面是一个简单的 Ingress，将发往 Host fortune.bytedance.com 的请求路由到 Service fortune-teller-service。\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: GRPC\n  name: fortune-ingress\n  namespace: default\nspec:\n  rules:\n  - host: fortune.bytedance.com\n    http:\n      paths:\n      - backend:\n          serviceName: fortune-teller-service\n          servicePort: grpc\n```\n\n安装 Ingress\n```bash\nkubectl apply -f ingress.yaml\nkubectl get ingress\n```\n\n![](/asset/k8s-ramp-up/18.png)\n\nIngress nginx controller 对于 gRpc 默认只支持 SSL 的形式，而Fedlearner 中的 controller 做了一些定制化操作，使得通过 80 端口，只使用 HTTP2 也可以转发 gRpc。详情可以参考 https://github.com/bytedance/ingress-nginx/pull/2/files\n\n使用下面这个命令，我们可以测试一下从外部访问服务\n```bash\ngrpcurl -insecure -servername 'fortune.bytedance.com' 0.0.0.0:32443 build.stack.fortune.FortuneTeller/Predict\n```\n\n![](/asset/k8s-ramp-up/19.png)\n\n刚才也提到了，gRpc 通常是使用 SSL 进行加密的，SSL 的关键在于公钥，私钥以及证书的验证。通过文件系统的方式确实可以处理证书的问题，但 K8s 抽象出 Secret 这种资源，大大提高了对这类文件的管理和复用能力。\n\n#### Secret\n\nSecret 解决了密码、token、密钥等敏感数据的存储问题，主要分为三种类型：\n- Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的 / run/secrets/kubernetes.io/serviceaccount 目录中\n- Opaque ：Base64 编码格式的 Secret，用来存储密码、密钥等\n- kubernetes.io/dockerconfigjson ：用来存储 docker registry 的认证信息\n\n接下来，我们就来创建一个 Opaque 类型的 Secret，使得 ingress nginx controller 支持服务端的 SSL。\n由于篇幅有限，这里简单介绍下证书相关的概念：\n- CA：证书授权中心(certificate authority)，用来签发私钥，并验证公钥，私钥的合法性\n- 私钥，公钥：私钥用于加密，公钥用于解密\n\nSecret 支持不编写 yaml，直接从文件中创建 Secret\n```bash\nkubectl create secret generic fortune-teller-ssl-verify \\\n  --from-file=ca.crt=CA.pem \\\n  --from-file=tls.crt=server-public.pem \\\n  --from-file=tls.key=server-private.key\n```\n\n修改 Ingress，使其使用新创建的 Secret 提供服务侧的 SSL。\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: GRPC\n  name: fortune-ingress\n  namespace: default\nspec:\n  rules:\n  - host: fortune.test.com\n    http:\n      paths:\n      - backend:\n          serviceName: fortune-teller-service\n          servicePort: grpc\n  tls:\n  - hosts:\n    - fortune.test.com\n    secretName: fortune-teller-ssl-verify\n```\n\n因为我们使用的是自签名的证书，不被公共的 CA 所信任，所以在发送请求是需要手动指定自己所信任的 CA。\n\n```bash\ngrpcurl -cacert CA.pem \\\n  -servername 'fortune.test.com' \\\n  127.0.0.1:32443 \\\n  build.stack.fortune.FortuneTeller/Predict\n```\n\n![](/asset/k8s-ramp-up/20.png)\n\n## 参考\n- https://draveness.me/docker/\n- https://www.yuque.com/kshare/2020/fe3b6f86-3b9b-48da-9770-897d838cbf41?language=zh-cn#Namespace\n- https://network.51cto.com/art/201907/598970.htm\n- https://blog.csdn.net/gatieme/article/details/51383322\n\n","source":"_posts/k8s-ramp-up.md","raw":"---\ntitle: \"[Introduction] Kubernetes Ramp Up\"\ndate: 2021-03-15 15:39:36\nupdated: 2021-03-15 15:39:36\n---\n## 目标\n- 介绍 K8s，Docker 概念以及原理\n- 从 0 开始部署一个简单完整的服务\n\n## Docker是什么？\nDocker是由Google推出的Go语言进行开发实现，基于Linux内核的 <font color=red>namespace</font>，对<font color=red>进程</font>进行封装<font color=red>隔离</font>，属于操作系统层面的容器化技术。\n\n![](/asset/k8s-ramp-up/1.png)\n\n### 三大核心概念\n镜像（Image）\n\n容器（Container）\n\n仓库（Repository）\n\n从代码的角度来看，镜像就像一个类；容器是对象实例，运行时在系统中会有许多容器；仓库主要用于存储和维护这些镜像。\n\n### 为什么使用 Docker？\n- 配置环境\n开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性\n- 应用隔离\n机器上可能同时运行多个服务。如果服务之间没有隔离，一个服务出现异常，往往可能会导致其他服务也挂掉。同时，不同服务所依赖的环境也可能发生冲突。\n\n### 原理\n首先，要了解一下进程的命名空间。Linux 系统中的所有进程按照惯例是通过PID标识的，这意味着内核必须管理一个全局的PID列表。而且，所有调用者通过uname系统调用返回的系统相关信息（包括系统名称和有关内核的一些信息）都是相同的。\n\nLinux 的命名空间从内核层面上进行了虚拟化，对所有的全局资源进行一个抽象。本质上，建立了系统的不同视图。每一项全局资源都必须包装到命名空间的数据结构中，只有资源和包含资源的命名空间构成的二元组仍然是全局唯一的。不仅仅是 PID，Linux 通过同样的方法对其他资源也做了虚拟化处理。命名空间共有以下6种：\n\n![](/asset/k8s-ramp-up/2.png)\n\n借助 Linux 的命名空间，Docker 对进程进行隔离，可以从进程树的角度理解。\n\n![](/asset/k8s-ramp-up/3.png)\n\n每次在执行 `docker start` 或 `docker run` 的时候，其实是由 docker 的 daemon 进程 docker containerd，调用 Linux 系统调用 `clone()` 去创建新的进程。而创建进程的过程中就为新创建的进程分配了新的 Linux 命名空间。可以简单阅读一下 docker 的开源代码\n\n<pre><code class=\"go\">// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L17\n// 创建容器的函数，其中又调用了设置\nfunc (daemon *Daemon) ContainerStart(name string, hostConfig *containertypes.HostConfig, checkpoint string, checkpointDir string) error\n\n// https://github.com/moby/moby/blob/470ae8422fc6f1845288eb7572253b08f1e6edf8/daemon/oci_linux.go#L212\n// 设置 Namespace\nfunc setNamespace(s *specs.Spec, ns specs.LinuxNamespace) {\n   for i, n := range s.Linux.Namespaces {\n      if n.Type == ns.Type {\n         s.Linux.Namespaces[i] = ns\n         return\n      }\n   }\n   s.Linux.Namespaces = append(s.Linux.Namespaces, ns)\n}\n\n// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L198\n// 创建新的进程\npid, err := daemon.containerd.Start(context.Background(), \n                                    container.ID, \n                                    checkpointDir,    \n                                    container.StreamConfig.Stdin() != nil | | container.Config.Tty, \n                                    container.InitializeStdio)\n</code></pre>\n\n### 如何安装？\n[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)\n\n## Kubernetes是什么？\nKubernetes 是 Google 于 2014 年基于其内部 Brog 系统开源的一个容器编排管理系统，可使用声明式的配置（以 yaml 文件的形式）自动地执行容器化应用程序的管理，包括部署、伸缩、负载均衡、回滚等。\n\n为什么叫 K8s？因为 K<font color=red>ubernete</font>s，中间是8个字母。\n\nkubernetes 提供的功能：\n- 自动发布与伸缩：可以通过声明式的配置文件定义想要部署的容器\n- 滚动升级与灰度发布：采用逐步替换的策略实现滚动升级\n- 服务发现与负载均衡：Kubernetes 通过 DNS 名称或 IP 地址暴露容器的访问方式，并且可在同一容器组内实现负载分发与均衡\n- 存储编排：Kubernetes 可以自动挂载指定的存储系统，如 local storage/nfs / 云存储等\n- 故障恢复：Kubernetes 自动重启已经停机的容器，替换不满足健康检查的容器\n- 密钥与配置管理：Kubernetes 可以存储与管理敏感信息，如 Docker Registry 的登录凭证，密码，ssh 密钥等\n\n### 为什么使用 K8s？\n大型单体应用被逐渐拆分成小的、可独立运行的组件。随着部署组件的增多和数据中心的增长，配置、管理和运维变得很困难。(微服务）\n\nK8s 的定义就是容器编排和管理引擎，解决了这些问题。\n\n### 如何安装？\n由难到易(๑•̀ㅂ•́)و✧\n- Kubeadm: https://kubernetes.io/docs/reference/setup-tools/kubeadm/\n- MiniKube: Local kubernetes https://minikube.sigs.k8s.io/docs/start/\n- Kind: Kubernetes in Docker https://github.com/kubernetes-sigs/kind\n- Docker-desktop（仅限 Mac）: 一键开启\n![](/asset/k8s-ramp-up/4.png)\n\n\n其他版本的类 K8s 系统：\n- K3s: https://github.com/k3s-io/k3s\n- K0s: https://github.com/k0sproject/k0s\n\n## Kubernetes 架构\n\n![](/asset/k8s-ramp-up/5.png)\n\n### master\n\nMaster 负责管理服务来对整个系统进行管理与控制，包括\n- apiserver：作为整个系统的对外接口，提供一套 Restful API 供客户端调用，任何的资源请求 / 调用操作都是通过 kube-apiserver 提供的接口进行, 如 kubectl、kubernetes dashboard 等管理工具就是通过 apiserver 来实现对集群的管理\n- kube-scheduler：资源调度器，负责将容器组分配到哪些节点上\n- kube-controller-manager：管理控制器，集群中处理常规任务的后台线程，包括节点控制器（负责监听节点停机的事件并作出对应响应）、endpoint-controller（刷新服务与容器组的关联信息）、replication-controller（维护容器组的副本数为指定的数值）、Service Account & Token 控制器（负责为新的命名空间创建默认的 Service Account 以及 API Access Token）\n- etcd：数据存储，存储集群所有的配置信息\n- coredns：实现集群内部通过服务名称进行容器组访问的功能\n\n### worker\n\nWorker 负载执行 Master 分配的任务，包括\n- kubelet：是工作节点上执行操作的代理程序，负责容器的生命周期管理，定期执行容器健康检查，并上报容器的运行状态\n- kube-proxy：是一个具有负载均衡能力的简单的网络访问代理，负责将访问某个服务的请求分配到工作节点的具体某个容器上（kube-proxy 也运行于 master node 上）\n- Docker Daemon：Kubernetes 其实不局限于 Docker（即将取消），它支持任何实现了 Kubernetes 容器引擎接口的容器引擎，如 containerd、rktlet\n\n### 网络通信\n网络通信组件只需要符合 CNI （Container Network Interface）接口规范，主要作用在于给各个容器分配集群内 IP，使得其内网 IP 能够集群内唯一，并且可以互相访问，目前常用的有 Flannel，Calico等网络组件。\n\n简单介绍下比较常用的 Flannel 的原理。Flannel 运行在第3层网络层，基于 IPv4，创建一个大型内部网络，跨越集群中每个节点。每个节点组成一个子网，每个容器在内网中有唯一的IP。\n\n首先，Flannel 会为每台节点分配一个子网段。Flanneld 在 Docker 容器启动时修改其启动参数，将其 IP 限制在当前的子网段内，具体 IP 的分配仍是由 docker 进行。Flannel通过Etcd服务维护了一张节点间的路由表，详细记录了各节点子网网段，保证不同节点的子网网段不会重复。\n\n数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，flanneld服务监听在网卡的另外一端。\n\n源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。\n\n![](/asset/k8s-ramp-up/6.png)\n\n## 快速上手 K8s 概念\n\n一些推荐的 K8s 概念介绍：\n- 微软的 50天 K8s 教程中（https://azure.microsoft.com/en-us/resources/kubernetes-learning-path/）通过动物园的形式介绍了一些 K8s 概念 http://aka.ms/k8s/LearnwithPhippy\n- 综述PPT：https://www2.slideshare.net/BobKillen/kubernetes-a-comprehensive-overview-updated?from_action=save\n\nK8s 中的概念极多，比较零碎，这里通过一个简单的小例子，尽可能覆盖多的 K8s 概念。\n\n## 概览\n\n例子使用一个开源的 fortune-teller 镜像（`quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1`） ，每次请求容器内的服务，服务会返回一句名言。希望在 MacOS 的环境下，展示一个应用在 K8s 中运行的全流程。\n\n准备环境\n为了不影响大家本地的环境，这里使用 Kind 创建出一个独立的 K8s 集群，方便统一版本并且可以在完成快速清理掉。(Docker 双重隔离）\n\n1. 安装 Kind 以及 gRpc 测试工具\n```bash\nbrew install kind\nbrew install grpcurl\n```\n\n2. 拉取镜像\n```bash\ndocker pull kindest/node:v1.16.15\ndocker pull quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n```\n\n3. 创建 K8s 集群，因为 Kind 是在 Docker 容器里面创建的 K8s，所以宿主机访问，需要把端口暴露出来。Kind 会默认把 K8s apiserver 的端口暴露出来，用来给 kubectl 命令使用。但为了之后的测试，我们提前把几个端口在创建的时候就暴露出来。\n\nKind 同样支持通过yaml 的形式创建集群\n```yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 32080\n    hostPort: 32080\n  - containerPort: 32443\n    hostPort: 32443\n```\n\n```bash\nkind create cluster --name=fortune-teller --image=kindest/node:v1.16.15 --config kind-config.yaml\n```\n\n### 运行 Docker 版本\n1. 启动容器\n```bash\ndocker run -p 50051:50051 quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n```\n\n`-p` 将容器内的 50051 端口映射到宿主机的 50051 端口\n\n2. 测试应用是否正常运行，第一次运行时可能需要给 grpcurl 开启权限\n```bash\ngrpcurl -plaintext 127.0.0.1:50051 build.stack.fortune.FortuneTeller/Predict\n```\n\n应用在收到请求以后，会返回一句名言\n\n![](/asset/k8s-ramp-up/7.png)\n\n### 将应用从 Docker 迁移到 K8s 中\n\n与 Docker 中容器概念相对应的，K8s 中也有着容器的概念。对于虚拟化的容器来说，最佳实践是一个容器一个应用，但当一个服务需要多个应用组合完成时，简单的将多个应用部署到一个容器内，就破坏了应用之间的隔离性，所以 K8s 对于容器进行了一层封装，形成了 Pod 的概念。\n\n#### Pod\n\nPod 是 Kubernetes 创建或部署的最小基本单元。一个 Pod 封装一个或多个应用容器、存储资源、一个独立的网络 IP 以及管理控制容器运行方式的策略选项。Pod 中的每个容器共享网络命名空间（包括 IP 与端口），Pod 内的容器可以使用 localhost 相互通信。Pod 可以指定一组共享存储卷 Volumes，Pod 中所有容器都可以访问共享的 Volumes。 \n\n通过 Pod，用户就可以非常方便地控制容器之间的隔离性。\n\n有了 Pod 作为基础以后，K8s 就要实现它最重要的功能，对容器的编排管理。当服务需要扩容时，K8s 需要能够快速复制 Pod，当 Pod 挂掉了，K8s 需要能够自动重启。所以 K8s 由此衍生出了 ReplicaSet 的概念。\n\n#### ReplicaSet\n\nReplicaSet 确保在任何时候都有按配置的 Pod 副本数在运行，通过标签选择器的方式对 Pod 进行筛选和管理。在旧的版本中还有一个 ReplicaController 的概念，RC 与 RS 两者功能完全相同，区别仅仅在于 RS 对于 Pod 的标签选择器更加强大。\n\n开头提到了 K8s 使用声明式的配置自动去管理容器，而 ReplicaSet 的内容却太过具体，涉及到了 Pod 的具体维护细节。所以 K8s 在 ReplicaSet 之上又衍生出声明式配置容器的概念，Deployment。\n\n#### Deployment\n\nDeployment 为 Pod 与 ReplicaSet 提供了声明式的定义，描述你想要的目标状态是什么，Deployment controller 就会帮你将 Pod 与 ReplicaSet 的实际状态改变到你想要的目标状态。\n\n以 fortune-teller 为例子，可以编写一份下面这样的 Deployment 配置文件\n```yaml\napiVersion: apps/v1 # k8s api版本\nkind: Deployment # 资源类型\nmetadata:\n  name: fortune-teller-app # deployment 名字\n  namespace: default\nspec:\n  replicas: 1 # Pod 副本数量\n  selector:\n    matchLabels:\n      k8s-app: fortune-teller-app # 管理标签中包含 k8s-app: fortune-teller-app 的 Pod\n  template: # Pod 模板\n    metadata:\n      labels:\n        k8s-app: fortune-teller-app # Pod 标签\n    spec: # Pod 配置\n      containers:\n      - image: quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n        imagePullPolicy: IfNotPresent\n        name: fortune-teller-app\n        ports:\n        - containerPort: 50051\n          name: grpc\n          protocol: TCP\n```\n\n将上面的内容保存到一份 yaml 文件中，执行以下命令，让 K8s 执行 yaml\n```bash\nkubectl apply -f deployment.yaml\n```\n\n通过以下命令，我们就可以看到刚刚创建的 deployment\n```bash\nkubectl get deployement\n```\n\n![](/asset/k8s-ramp-up/8.png)\n\n此时，K8s 已经自动根据 deployment 中配置的 Pod 模板和配置，创建了 Pod。通过以下命令，我们就可以看到 K8s 自动创建的 Pod\n```bash\nkubectl get pods\n```\n![](/asset/k8s-ramp-up/9.png)\n\n因为 K8s 采用声明式的配置去管理 Pod，所以我们可以动态地去修改 deployment 的配置，K8s 会自动根据新的配置去管理 Pod。\n```bash\nkubectl edit deployment fortune-teller-app\n```\n\n我们把配置文件中的副本数量修改为 2\n\n![](/asset/k8s-ramp-up/10.png)\n\n保存退出后，我们再次执行 kubectl get pods ，我们就可以看到 K8s 根据新的配置，创建了一个新的 Pod\n\n![](/asset/k8s-ramp-up/11.png)\n\n现在我们就有了两个 fortune-teller 的服务。在真实环境中，Pod 的调度由 K8s 进行管理，某个时刻服务可能在 Node1 上，而另一时刻服务可能就被调度到了 Node2 上。所以，访问具体 Pod 是一种不稳定的服务访问方法，而且目前大多数的后端服务都是无状态的服务，直接访问 Pod 也导致不能进行负载均衡。所以，K8s 在此基础上衍生出 Service 的概念。\n\n#### Service\n\nService 可以看做一组提供相同服务的 Pod 的对外访问接口。Kubernetes 提供三种类型的 Service：\n- NodePort： 集群外部可以通过 Node IP 与 Node Port 来访问具体某个 Pod，每台机器上都会暴露同样的端口\n- ClusterIP：指通过集群的内部 IP 暴露服务，服务只能够在集群内部可以访问，这也是默认的 ServiceType\n- ExternalName：不指向 Pod，指向外部服务\nService 和 Deployment 是一对比较容易混淆的概念，两者都是对一组 Pod 进行管理，但它们两者之间的关系可以用下面这张图来概括\n\n![](/asset/k8s-ramp-up/12.png)\n\nService 是面向服务调用者，也就是外部访问 K8s。而 Deployment 是面向 K8s 底层引擎的，面向内部管理者。\n\nService 的配置文件格式与 Deployment 很类似\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: fortune-teller-service\n  namespace: default\nspec:\n  ports:\n  - name: grpc\n    port: 50051\n    protocol: TCP\n    targetPort: 50051\n  selector:\n    k8s-app: fortune-teller-app\n  type: ClusterIP\n```\n\n同样的，我们通过 kubectl apply -f service.yaml 命令，可以创建 Service。通过 kubectl get service 可以查看到刚刚创建的 Service。\n\n![](/asset/k8s-ramp-up/13.png)\n\n接下来，我们登陆到一个 Pod 里去测试一下是否可以访问服务。\nNetshoot 镜像中包含了一些网络测试的工具，我们可以直接进入一个 netshoot 容器内测试。采用 deployment 的方式创建 Pod\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netshoot\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: netshoot\n  template:\n    metadata:\n      labels:\n        k8s-app: netshoot\n    spec:\n      containers:\n      - args:\n        - 1000d\n        command:\n        - /bin/sleep\n        image: nicolaka/netshoot\n        name: netshoot\n```\n\n与 Docker 的命令类似，使用命令 `kubectl cp grpcurl_1.6.0_linux_x86_64.tar.gz <pod name>:/` 复制工具到容器内。\n\n复制成功后，使用命令 kubectl exec -it <pod name> bash 可以进入到容器内。\n\n解压\n```bash\ncd /\ntar -zxf grpcurl_1.6.0_linux_x86_64.tar.gz\n```\n\n然后我们可以测试是否可以从 K8s 集群内访问 Service。对于 K8s 集群内部的服务，K8s 有自己的 DNS 组件，所以可以直接通过服务名访问。\n```bash\n./grpcurl -plaintext fortune-teller-service:50051 build.stack.fortune.FortuneTeller/Predict\n```\n\n![](/asset/k8s-ramp-up/14.png)\n\n验证服务可以从集群内访问之后，我们就需要解决如何从集群外访问服务的问题，毕竟大多数服务是面向 K8s 集群外的用户的。其实目前我们已经了解了一种解决方案，就是使用 Nodeport 类型的 Service。但采用这种方法有几个缺点：\n1. 每个端口只能是一种服务\n2. 端口范围只能是 30000-32767\n3. 如果节点 的 IP 地址发生变化，调用方需要能够察觉。\n所以，K8s 为服务的外部访问路由提供了新的类型 Ingress。\n\n#### Ingress\nIngress 其实是一种类似于路由表一样的配置，实际的路由工作需要 Ingress Controller 执行。K8s 本身并没有提供 Ingress Controller，目前常用的是通过 Nginx 实现的版本 https://github.com/kubernetes/ingress-nginx 。可以使用上面压缩包中的 ingress-controller.yaml 安装\n\n![](/asset/k8s-ramp-up/15.png)\n\nIngree nginx controller 通过宿主机暴露给外部访问的端口是随机的，所以我们修改 yaml，改成我们在一开始创建集群时映射的端口。\n\n通过 `kubectl get svc -n ingress-nginx` 我们就可以看到，暴露的是两个随机分配的端口\n\n![](/asset/k8s-ramp-up/16.png)\n\n我们手动将其改成 32080 和 32443。\n\n![](/asset/k8s-ramp-up/17.png)\n\nIngress nginx controller 同样是通过标签选择器的方式管理 Ingress。\n\n下面是一个简单的 Ingress，将发往 Host fortune.bytedance.com 的请求路由到 Service fortune-teller-service。\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: GRPC\n  name: fortune-ingress\n  namespace: default\nspec:\n  rules:\n  - host: fortune.bytedance.com\n    http:\n      paths:\n      - backend:\n          serviceName: fortune-teller-service\n          servicePort: grpc\n```\n\n安装 Ingress\n```bash\nkubectl apply -f ingress.yaml\nkubectl get ingress\n```\n\n![](/asset/k8s-ramp-up/18.png)\n\nIngress nginx controller 对于 gRpc 默认只支持 SSL 的形式，而Fedlearner 中的 controller 做了一些定制化操作，使得通过 80 端口，只使用 HTTP2 也可以转发 gRpc。详情可以参考 https://github.com/bytedance/ingress-nginx/pull/2/files\n\n使用下面这个命令，我们可以测试一下从外部访问服务\n```bash\ngrpcurl -insecure -servername 'fortune.bytedance.com' 0.0.0.0:32443 build.stack.fortune.FortuneTeller/Predict\n```\n\n![](/asset/k8s-ramp-up/19.png)\n\n刚才也提到了，gRpc 通常是使用 SSL 进行加密的，SSL 的关键在于公钥，私钥以及证书的验证。通过文件系统的方式确实可以处理证书的问题，但 K8s 抽象出 Secret 这种资源，大大提高了对这类文件的管理和复用能力。\n\n#### Secret\n\nSecret 解决了密码、token、密钥等敏感数据的存储问题，主要分为三种类型：\n- Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的 / run/secrets/kubernetes.io/serviceaccount 目录中\n- Opaque ：Base64 编码格式的 Secret，用来存储密码、密钥等\n- kubernetes.io/dockerconfigjson ：用来存储 docker registry 的认证信息\n\n接下来，我们就来创建一个 Opaque 类型的 Secret，使得 ingress nginx controller 支持服务端的 SSL。\n由于篇幅有限，这里简单介绍下证书相关的概念：\n- CA：证书授权中心(certificate authority)，用来签发私钥，并验证公钥，私钥的合法性\n- 私钥，公钥：私钥用于加密，公钥用于解密\n\nSecret 支持不编写 yaml，直接从文件中创建 Secret\n```bash\nkubectl create secret generic fortune-teller-ssl-verify \\\n  --from-file=ca.crt=CA.pem \\\n  --from-file=tls.crt=server-public.pem \\\n  --from-file=tls.key=server-private.key\n```\n\n修改 Ingress，使其使用新创建的 Secret 提供服务侧的 SSL。\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: GRPC\n  name: fortune-ingress\n  namespace: default\nspec:\n  rules:\n  - host: fortune.test.com\n    http:\n      paths:\n      - backend:\n          serviceName: fortune-teller-service\n          servicePort: grpc\n  tls:\n  - hosts:\n    - fortune.test.com\n    secretName: fortune-teller-ssl-verify\n```\n\n因为我们使用的是自签名的证书，不被公共的 CA 所信任，所以在发送请求是需要手动指定自己所信任的 CA。\n\n```bash\ngrpcurl -cacert CA.pem \\\n  -servername 'fortune.test.com' \\\n  127.0.0.1:32443 \\\n  build.stack.fortune.FortuneTeller/Predict\n```\n\n![](/asset/k8s-ramp-up/20.png)\n\n## 参考\n- https://draveness.me/docker/\n- https://www.yuque.com/kshare/2020/fe3b6f86-3b9b-48da-9770-897d838cbf41?language=zh-cn#Namespace\n- https://network.51cto.com/art/201907/598970.htm\n- https://blog.csdn.net/gatieme/article/details/51383322\n\n","slug":"k8s-ramp-up","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clapldfdb000h18mzc1a9fq89","content":"<h2 id=\"目标\"><a class=\"markdownIt-Anchor\" href=\"#目标\"></a> 目标</h2>\n<ul>\n<li>介绍 K8s，Docker 概念以及原理</li>\n<li>从 0 开始部署一个简单完整的服务</li>\n</ul>\n<h2 id=\"docker是什么\"><a class=\"markdownIt-Anchor\" href=\"#docker是什么\"></a> Docker是什么？</h2>\n<p>Docker是由Google推出的Go语言进行开发实现，基于Linux内核的 <font color=red>namespace</font>，对<font color=red>进程</font>进行封装<font color=red>隔离</font>，属于操作系统层面的容器化技术。</p>\n<p><img src=\"/asset/k8s-ramp-up/1.png\" alt=\"\" /></p>\n<h3 id=\"三大核心概念\"><a class=\"markdownIt-Anchor\" href=\"#三大核心概念\"></a> 三大核心概念</h3>\n<p>镜像（Image）</p>\n<p>容器（Container）</p>\n<p>仓库（Repository）</p>\n<p>从代码的角度来看，镜像就像一个类；容器是对象实例，运行时在系统中会有许多容器；仓库主要用于存储和维护这些镜像。</p>\n<h3 id=\"为什么使用-docker\"><a class=\"markdownIt-Anchor\" href=\"#为什么使用-docker\"></a> 为什么使用 Docker？</h3>\n<ul>\n<li>配置环境<br />\n开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性</li>\n<li>应用隔离<br />\n机器上可能同时运行多个服务。如果服务之间没有隔离，一个服务出现异常，往往可能会导致其他服务也挂掉。同时，不同服务所依赖的环境也可能发生冲突。</li>\n</ul>\n<h3 id=\"原理\"><a class=\"markdownIt-Anchor\" href=\"#原理\"></a> 原理</h3>\n<p>首先，要了解一下进程的命名空间。Linux 系统中的所有进程按照惯例是通过PID标识的，这意味着内核必须管理一个全局的PID列表。而且，所有调用者通过uname系统调用返回的系统相关信息（包括系统名称和有关内核的一些信息）都是相同的。</p>\n<p>Linux 的命名空间从内核层面上进行了虚拟化，对所有的全局资源进行一个抽象。本质上，建立了系统的不同视图。每一项全局资源都必须包装到命名空间的数据结构中，只有资源和包含资源的命名空间构成的二元组仍然是全局唯一的。不仅仅是 PID，Linux 通过同样的方法对其他资源也做了虚拟化处理。命名空间共有以下6种：</p>\n<p><img src=\"/asset/k8s-ramp-up/2.png\" alt=\"\" /></p>\n<p>借助 Linux 的命名空间，Docker 对进程进行隔离，可以从进程树的角度理解。</p>\n<p><img src=\"/asset/k8s-ramp-up/3.png\" alt=\"\" /></p>\n<p>每次在执行 <code>docker start</code> 或 <code>docker run</code> 的时候，其实是由 docker 的 daemon 进程 docker containerd，调用 Linux 系统调用 <code>clone()</code> 去创建新的进程。而创建进程的过程中就为新创建的进程分配了新的 Linux 命名空间。可以简单阅读一下 docker 的开源代码</p>\n<pre><code class=\"go\">// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L17\n// 创建容器的函数，其中又调用了设置\nfunc (daemon *Daemon) ContainerStart(name string, hostConfig *containertypes.HostConfig, checkpoint string, checkpointDir string) error\n\n// https://github.com/moby/moby/blob/470ae8422fc6f1845288eb7572253b08f1e6edf8/daemon/oci_linux.go#L212\n// 设置 Namespace\nfunc setNamespace(s *specs.Spec, ns specs.LinuxNamespace) &#123;\n   for i, n := range s.Linux.Namespaces &#123;\n      if n.Type == ns.Type &#123;\n         s.Linux.Namespaces[i] = ns\n         return\n      &#125;\n   &#125;\n   s.Linux.Namespaces = append(s.Linux.Namespaces, ns)\n&#125;\n\n// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L198\n// 创建新的进程\npid, err := daemon.containerd.Start(context.Background(), \n                                    container.ID, \n                                    checkpointDir,    \n                                    container.StreamConfig.Stdin() != nil | | container.Config.Tty, \n                                    container.InitializeStdio)\n</code></pre>\n<h3 id=\"如何安装\"><a class=\"markdownIt-Anchor\" href=\"#如何安装\"></a> 如何安装？</h3>\n<p><a href=\"https://docs.docker.com/get-docker/\">https://docs.docker.com/get-docker/</a></p>\n<h2 id=\"kubernetes是什么\"><a class=\"markdownIt-Anchor\" href=\"#kubernetes是什么\"></a> Kubernetes是什么？</h2>\n<p>Kubernetes 是 Google 于 2014 年基于其内部 Brog 系统开源的一个容器编排管理系统，可使用声明式的配置（以 yaml 文件的形式）自动地执行容器化应用程序的管理，包括部署、伸缩、负载均衡、回滚等。</p>\n<p>为什么叫 K8s？因为 K<font color=red>ubernete</font>s，中间是8个字母。</p>\n<p>kubernetes 提供的功能：</p>\n<ul>\n<li>自动发布与伸缩：可以通过声明式的配置文件定义想要部署的容器</li>\n<li>滚动升级与灰度发布：采用逐步替换的策略实现滚动升级</li>\n<li>服务发现与负载均衡：Kubernetes 通过 DNS 名称或 IP 地址暴露容器的访问方式，并且可在同一容器组内实现负载分发与均衡</li>\n<li>存储编排：Kubernetes 可以自动挂载指定的存储系统，如 local storage/nfs / 云存储等</li>\n<li>故障恢复：Kubernetes 自动重启已经停机的容器，替换不满足健康检查的容器</li>\n<li>密钥与配置管理：Kubernetes 可以存储与管理敏感信息，如 Docker Registry 的登录凭证，密码，ssh 密钥等</li>\n</ul>\n<h3 id=\"为什么使用-k8s\"><a class=\"markdownIt-Anchor\" href=\"#为什么使用-k8s\"></a> 为什么使用 K8s？</h3>\n<p>大型单体应用被逐渐拆分成小的、可独立运行的组件。随着部署组件的增多和数据中心的增长，配置、管理和运维变得很困难。(微服务）</p>\n<p>K8s 的定义就是容器编排和管理引擎，解决了这些问题。</p>\n<h3 id=\"如何安装-2\"><a class=\"markdownIt-Anchor\" href=\"#如何安装-2\"></a> 如何安装？</h3>\n<p>由难到易(๑•̀ㅂ•́)و✧</p>\n<ul>\n<li>Kubeadm: <a href=\"https://kubernetes.io/docs/reference/setup-tools/kubeadm/\">https://kubernetes.io/docs/reference/setup-tools/kubeadm/</a></li>\n<li>MiniKube: Local kubernetes <a href=\"https://minikube.sigs.k8s.io/docs/start/\">https://minikube.sigs.k8s.io/docs/start/</a></li>\n<li>Kind: Kubernetes in Docker <a href=\"https://github.com/kubernetes-sigs/kind\">https://github.com/kubernetes-sigs/kind</a></li>\n<li>Docker-desktop（仅限 Mac）: 一键开启<br />\n<img src=\"/asset/k8s-ramp-up/4.png\" alt=\"\" /></li>\n</ul>\n<p>其他版本的类 K8s 系统：</p>\n<ul>\n<li>K3s: <a href=\"https://github.com/k3s-io/k3s\">https://github.com/k3s-io/k3s</a></li>\n<li>K0s: <a href=\"https://github.com/k0sproject/k0s\">https://github.com/k0sproject/k0s</a></li>\n</ul>\n<h2 id=\"kubernetes-架构\"><a class=\"markdownIt-Anchor\" href=\"#kubernetes-架构\"></a> Kubernetes 架构</h2>\n<p><img src=\"/asset/k8s-ramp-up/5.png\" alt=\"\" /></p>\n<h3 id=\"master\"><a class=\"markdownIt-Anchor\" href=\"#master\"></a> master</h3>\n<p>Master 负责管理服务来对整个系统进行管理与控制，包括</p>\n<ul>\n<li>apiserver：作为整个系统的对外接口，提供一套 Restful API 供客户端调用，任何的资源请求 / 调用操作都是通过 kube-apiserver 提供的接口进行, 如 kubectl、kubernetes dashboard 等管理工具就是通过 apiserver 来实现对集群的管理</li>\n<li>kube-scheduler：资源调度器，负责将容器组分配到哪些节点上</li>\n<li>kube-controller-manager：管理控制器，集群中处理常规任务的后台线程，包括节点控制器（负责监听节点停机的事件并作出对应响应）、endpoint-controller（刷新服务与容器组的关联信息）、replication-controller（维护容器组的副本数为指定的数值）、Service Account &amp; Token 控制器（负责为新的命名空间创建默认的 Service Account 以及 API Access Token）</li>\n<li>etcd：数据存储，存储集群所有的配置信息</li>\n<li>coredns：实现集群内部通过服务名称进行容器组访问的功能</li>\n</ul>\n<h3 id=\"worker\"><a class=\"markdownIt-Anchor\" href=\"#worker\"></a> worker</h3>\n<p>Worker 负载执行 Master 分配的任务，包括</p>\n<ul>\n<li>kubelet：是工作节点上执行操作的代理程序，负责容器的生命周期管理，定期执行容器健康检查，并上报容器的运行状态</li>\n<li>kube-proxy：是一个具有负载均衡能力的简单的网络访问代理，负责将访问某个服务的请求分配到工作节点的具体某个容器上（kube-proxy 也运行于 master node 上）</li>\n<li>Docker Daemon：Kubernetes 其实不局限于 Docker（即将取消），它支持任何实现了 Kubernetes 容器引擎接口的容器引擎，如 containerd、rktlet</li>\n</ul>\n<h3 id=\"网络通信\"><a class=\"markdownIt-Anchor\" href=\"#网络通信\"></a> 网络通信</h3>\n<p>网络通信组件只需要符合 CNI （Container Network Interface）接口规范，主要作用在于给各个容器分配集群内 IP，使得其内网 IP 能够集群内唯一，并且可以互相访问，目前常用的有 Flannel，Calico等网络组件。</p>\n<p>简单介绍下比较常用的 Flannel 的原理。Flannel 运行在第3层网络层，基于 IPv4，创建一个大型内部网络，跨越集群中每个节点。每个节点组成一个子网，每个容器在内网中有唯一的IP。</p>\n<p>首先，Flannel 会为每台节点分配一个子网段。Flanneld 在 Docker 容器启动时修改其启动参数，将其 IP 限制在当前的子网段内，具体 IP 的分配仍是由 docker 进行。Flannel通过Etcd服务维护了一张节点间的路由表，详细记录了各节点子网网段，保证不同节点的子网网段不会重复。</p>\n<p>数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，flanneld服务监听在网卡的另外一端。</p>\n<p>源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。</p>\n<p><img src=\"/asset/k8s-ramp-up/6.png\" alt=\"\" /></p>\n<h2 id=\"快速上手-k8s-概念\"><a class=\"markdownIt-Anchor\" href=\"#快速上手-k8s-概念\"></a> 快速上手 K8s 概念</h2>\n<p>一些推荐的 K8s 概念介绍：</p>\n<ul>\n<li>微软的 50天 K8s 教程中（<a href=\"https://azure.microsoft.com/en-us/resources/kubernetes-learning-path/%EF%BC%89%E9%80%9A%E8%BF%87%E5%8A%A8%E7%89%A9%E5%9B%AD%E7%9A%84%E5%BD%A2%E5%BC%8F%E4%BB%8B%E7%BB%8D%E4%BA%86%E4%B8%80%E4%BA%9B\">https://azure.microsoft.com/en-us/resources/kubernetes-learning-path/）通过动物园的形式介绍了一些</a> K8s 概念 <a href=\"http://aka.ms/k8s/LearnwithPhippy\">http://aka.ms/k8s/LearnwithPhippy</a></li>\n<li>综述PPT：<a href=\"https://www2.slideshare.net/BobKillen/kubernetes-a-comprehensive-overview-updated?from_action=save\">https://www2.slideshare.net/BobKillen/kubernetes-a-comprehensive-overview-updated?from_action=save</a></li>\n</ul>\n<p>K8s 中的概念极多，比较零碎，这里通过一个简单的小例子，尽可能覆盖多的 K8s 概念。</p>\n<h2 id=\"概览\"><a class=\"markdownIt-Anchor\" href=\"#概览\"></a> 概览</h2>\n<p>例子使用一个开源的 fortune-teller 镜像（<code>quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1</code>） ，每次请求容器内的服务，服务会返回一句名言。希望在 MacOS 的环境下，展示一个应用在 K8s 中运行的全流程。</p>\n<p>准备环境<br />\n为了不影响大家本地的环境，这里使用 Kind 创建出一个独立的 K8s 集群，方便统一版本并且可以在完成快速清理掉。(Docker 双重隔离）</p>\n<ol>\n<li>安装 Kind 以及 gRpc 测试工具</li>\n</ol>\n<pre class=\"highlight\"><code class=\"bash\">brew install kind\nbrew install grpcurl\n</code></pre>\n<ol start=\"2\">\n<li>拉取镜像</li>\n</ol>\n<pre class=\"highlight\"><code class=\"bash\">docker pull kindest/node:v1.16.15\ndocker pull quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n</code></pre>\n<ol start=\"3\">\n<li>创建 K8s 集群，因为 Kind 是在 Docker 容器里面创建的 K8s，所以宿主机访问，需要把端口暴露出来。Kind 会默认把 K8s apiserver 的端口暴露出来，用来给 kubectl 命令使用。但为了之后的测试，我们提前把几个端口在创建的时候就暴露出来。</li>\n</ol>\n<p>Kind 同样支持通过yaml 的形式创建集群</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Cluster</span>\n<span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">kind.x-k8s.io/v1alpha4</span>\n<span class=\"hljs-attr\">nodes:</span>\n<span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">role:</span> <span class=\"hljs-string\">control-plane</span>\n  <span class=\"hljs-attr\">extraPortMappings:</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">containerPort:</span> <span class=\"hljs-number\">32080</span>\n    <span class=\"hljs-attr\">hostPort:</span> <span class=\"hljs-number\">32080</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">containerPort:</span> <span class=\"hljs-number\">32443</span>\n    <span class=\"hljs-attr\">hostPort:</span> <span class=\"hljs-number\">32443</span>\n</code></pre>\n<pre class=\"highlight\"><code class=\"bash\">kind create cluster --name=fortune-teller --image=kindest/node:v1.16.15 --config kind-config.yaml\n</code></pre>\n<h3 id=\"运行-docker-版本\"><a class=\"markdownIt-Anchor\" href=\"#运行-docker-版本\"></a> 运行 Docker 版本</h3>\n<ol>\n<li>启动容器</li>\n</ol>\n<pre class=\"highlight\"><code class=\"bash\">docker run -p 50051:50051 quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n</code></pre>\n<p><code>-p</code> 将容器内的 50051 端口映射到宿主机的 50051 端口</p>\n<ol start=\"2\">\n<li>测试应用是否正常运行，第一次运行时可能需要给 grpcurl 开启权限</li>\n</ol>\n<pre class=\"highlight\"><code class=\"bash\">grpcurl -plaintext 127.0.0.1:50051 build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n<p>应用在收到请求以后，会返回一句名言</p>\n<p><img src=\"/asset/k8s-ramp-up/7.png\" alt=\"\" /></p>\n<h3 id=\"将应用从-docker-迁移到-k8s-中\"><a class=\"markdownIt-Anchor\" href=\"#将应用从-docker-迁移到-k8s-中\"></a> 将应用从 Docker 迁移到 K8s 中</h3>\n<p>与 Docker 中容器概念相对应的，K8s 中也有着容器的概念。对于虚拟化的容器来说，最佳实践是一个容器一个应用，但当一个服务需要多个应用组合完成时，简单的将多个应用部署到一个容器内，就破坏了应用之间的隔离性，所以 K8s 对于容器进行了一层封装，形成了 Pod 的概念。</p>\n<h4 id=\"pod\"><a class=\"markdownIt-Anchor\" href=\"#pod\"></a> Pod</h4>\n<p>Pod 是 Kubernetes 创建或部署的最小基本单元。一个 Pod 封装一个或多个应用容器、存储资源、一个独立的网络 IP 以及管理控制容器运行方式的策略选项。Pod 中的每个容器共享网络命名空间（包括 IP 与端口），Pod 内的容器可以使用 localhost 相互通信。Pod 可以指定一组共享存储卷 Volumes，Pod 中所有容器都可以访问共享的 Volumes。</p>\n<p>通过 Pod，用户就可以非常方便地控制容器之间的隔离性。</p>\n<p>有了 Pod 作为基础以后，K8s 就要实现它最重要的功能，对容器的编排管理。当服务需要扩容时，K8s 需要能够快速复制 Pod，当 Pod 挂掉了，K8s 需要能够自动重启。所以 K8s 由此衍生出了 ReplicaSet 的概念。</p>\n<h4 id=\"replicaset\"><a class=\"markdownIt-Anchor\" href=\"#replicaset\"></a> ReplicaSet</h4>\n<p>ReplicaSet 确保在任何时候都有按配置的 Pod 副本数在运行，通过标签选择器的方式对 Pod 进行筛选和管理。在旧的版本中还有一个 ReplicaController 的概念，RC 与 RS 两者功能完全相同，区别仅仅在于 RS 对于 Pod 的标签选择器更加强大。</p>\n<p>开头提到了 K8s 使用声明式的配置自动去管理容器，而 ReplicaSet 的内容却太过具体，涉及到了 Pod 的具体维护细节。所以 K8s 在 ReplicaSet 之上又衍生出声明式配置容器的概念，Deployment。</p>\n<h4 id=\"deployment\"><a class=\"markdownIt-Anchor\" href=\"#deployment\"></a> Deployment</h4>\n<p>Deployment 为 Pod 与 ReplicaSet 提供了声明式的定义，描述你想要的目标状态是什么，Deployment controller 就会帮你将 Pod 与 ReplicaSet 的实际状态改变到你想要的目标状态。</p>\n<p>以 fortune-teller 为例子，可以编写一份下面这样的 Deployment 配置文件</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">apps/v1</span> <span class=\"hljs-comment\"># k8s api版本</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Deployment</span> <span class=\"hljs-comment\"># 资源类型</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">fortune-teller-app</span> <span class=\"hljs-comment\"># deployment 名字</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">replicas:</span> <span class=\"hljs-number\">1</span> <span class=\"hljs-comment\"># Pod 副本数量</span>\n  <span class=\"hljs-attr\">selector:</span>\n    <span class=\"hljs-attr\">matchLabels:</span>\n      <span class=\"hljs-attr\">k8s-app:</span> <span class=\"hljs-string\">fortune-teller-app</span> <span class=\"hljs-comment\"># 管理标签中包含 k8s-app: fortune-teller-app 的 Pod</span>\n  <span class=\"hljs-attr\">template:</span> <span class=\"hljs-comment\"># Pod 模板</span>\n    <span class=\"hljs-attr\">metadata:</span>\n      <span class=\"hljs-attr\">labels:</span>\n        <span class=\"hljs-attr\">k8s-app:</span> <span class=\"hljs-string\">fortune-teller-app</span> <span class=\"hljs-comment\"># Pod 标签</span>\n    <span class=\"hljs-attr\">spec:</span> <span class=\"hljs-comment\"># Pod 配置</span>\n      <span class=\"hljs-attr\">containers:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">image:</span> <span class=\"hljs-string\">quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1</span>\n        <span class=\"hljs-attr\">imagePullPolicy:</span> <span class=\"hljs-string\">IfNotPresent</span>\n        <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">fortune-teller-app</span>\n        <span class=\"hljs-attr\">ports:</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">containerPort:</span> <span class=\"hljs-number\">50051</span>\n          <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">grpc</span>\n          <span class=\"hljs-attr\">protocol:</span> <span class=\"hljs-string\">TCP</span>\n</code></pre>\n<p>将上面的内容保存到一份 yaml 文件中，执行以下命令，让 K8s 执行 yaml</p>\n<pre class=\"highlight\"><code class=\"bash\">kubectl apply -f deployment.yaml\n</code></pre>\n<p>通过以下命令，我们就可以看到刚刚创建的 deployment</p>\n<pre class=\"highlight\"><code class=\"bash\">kubectl get deployement\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/8.png\" alt=\"\" /></p>\n<p>此时，K8s 已经自动根据 deployment 中配置的 Pod 模板和配置，创建了 Pod。通过以下命令，我们就可以看到 K8s 自动创建的 Pod</p>\n<pre class=\"highlight\"><code class=\"bash\">kubectl get pods\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/9.png\" alt=\"\" /></p>\n<p>因为 K8s 采用声明式的配置去管理 Pod，所以我们可以动态地去修改 deployment 的配置，K8s 会自动根据新的配置去管理 Pod。</p>\n<pre class=\"highlight\"><code class=\"bash\">kubectl edit deployment fortune-teller-app\n</code></pre>\n<p>我们把配置文件中的副本数量修改为 2</p>\n<p><img src=\"/asset/k8s-ramp-up/10.png\" alt=\"\" /></p>\n<p>保存退出后，我们再次执行 kubectl get pods ，我们就可以看到 K8s 根据新的配置，创建了一个新的 Pod</p>\n<p><img src=\"/asset/k8s-ramp-up/11.png\" alt=\"\" /></p>\n<p>现在我们就有了两个 fortune-teller 的服务。在真实环境中，Pod 的调度由 K8s 进行管理，某个时刻服务可能在 Node1 上，而另一时刻服务可能就被调度到了 Node2 上。所以，访问具体 Pod 是一种不稳定的服务访问方法，而且目前大多数的后端服务都是无状态的服务，直接访问 Pod 也导致不能进行负载均衡。所以，K8s 在此基础上衍生出 Service 的概念。</p>\n<h4 id=\"service\"><a class=\"markdownIt-Anchor\" href=\"#service\"></a> Service</h4>\n<p>Service 可以看做一组提供相同服务的 Pod 的对外访问接口。Kubernetes 提供三种类型的 Service：</p>\n<ul>\n<li>NodePort： 集群外部可以通过 Node IP 与 Node Port 来访问具体某个 Pod，每台机器上都会暴露同样的端口</li>\n<li>ClusterIP：指通过集群的内部 IP 暴露服务，服务只能够在集群内部可以访问，这也是默认的 ServiceType</li>\n<li>ExternalName：不指向 Pod，指向外部服务<br />\nService 和 Deployment 是一对比较容易混淆的概念，两者都是对一组 Pod 进行管理，但它们两者之间的关系可以用下面这张图来概括</li>\n</ul>\n<p><img src=\"/asset/k8s-ramp-up/12.png\" alt=\"\" /></p>\n<p>Service 是面向服务调用者，也就是外部访问 K8s。而 Deployment 是面向 K8s 底层引擎的，面向内部管理者。</p>\n<p>Service 的配置文件格式与 Deployment 很类似</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">v1</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Service</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">fortune-teller-service</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">ports:</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">grpc</span>\n    <span class=\"hljs-attr\">port:</span> <span class=\"hljs-number\">50051</span>\n    <span class=\"hljs-attr\">protocol:</span> <span class=\"hljs-string\">TCP</span>\n    <span class=\"hljs-attr\">targetPort:</span> <span class=\"hljs-number\">50051</span>\n  <span class=\"hljs-attr\">selector:</span>\n    <span class=\"hljs-attr\">k8s-app:</span> <span class=\"hljs-string\">fortune-teller-app</span>\n  <span class=\"hljs-attr\">type:</span> <span class=\"hljs-string\">ClusterIP</span>\n</code></pre>\n<p>同样的，我们通过 kubectl apply -f service.yaml 命令，可以创建 Service。通过 kubectl get service 可以查看到刚刚创建的 Service。</p>\n<p><img src=\"/asset/k8s-ramp-up/13.png\" alt=\"\" /></p>\n<p>接下来，我们登陆到一个 Pod 里去测试一下是否可以访问服务。<br />\nNetshoot 镜像中包含了一些网络测试的工具，我们可以直接进入一个 netshoot 容器内测试。采用 deployment 的方式创建 Pod</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">apps/v1</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Deployment</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">netshoot</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">replicas:</span> <span class=\"hljs-number\">1</span>\n  <span class=\"hljs-attr\">selector:</span>\n    <span class=\"hljs-attr\">matchLabels:</span>\n      <span class=\"hljs-attr\">k8s-app:</span> <span class=\"hljs-string\">netshoot</span>\n  <span class=\"hljs-attr\">template:</span>\n    <span class=\"hljs-attr\">metadata:</span>\n      <span class=\"hljs-attr\">labels:</span>\n        <span class=\"hljs-attr\">k8s-app:</span> <span class=\"hljs-string\">netshoot</span>\n    <span class=\"hljs-attr\">spec:</span>\n      <span class=\"hljs-attr\">containers:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">args:</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">1000d</span>\n        <span class=\"hljs-attr\">command:</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">/bin/sleep</span>\n        <span class=\"hljs-attr\">image:</span> <span class=\"hljs-string\">nicolaka/netshoot</span>\n        <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">netshoot</span>\n</code></pre>\n<p>与 Docker 的命令类似，使用命令 <code>kubectl cp grpcurl_1.6.0_linux_x86_64.tar.gz &lt;pod name&gt;:/</code> 复制工具到容器内。</p>\n<p>复制成功后，使用命令 kubectl exec -it <pod name> bash 可以进入到容器内。</p>\n<p>解压</p>\n<pre class=\"highlight\"><code class=\"bash\"><span class=\"hljs-built_in\">cd</span> /\ntar -zxf grpcurl_1.6.0_linux_x86_64.tar.gz\n</code></pre>\n<p>然后我们可以测试是否可以从 K8s 集群内访问 Service。对于 K8s 集群内部的服务，K8s 有自己的 DNS 组件，所以可以直接通过服务名访问。</p>\n<pre class=\"highlight\"><code class=\"bash\">./grpcurl -plaintext fortune-teller-service:50051 build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/14.png\" alt=\"\" /></p>\n<p>验证服务可以从集群内访问之后，我们就需要解决如何从集群外访问服务的问题，毕竟大多数服务是面向 K8s 集群外的用户的。其实目前我们已经了解了一种解决方案，就是使用 Nodeport 类型的 Service。但采用这种方法有几个缺点：</p>\n<ol>\n<li>每个端口只能是一种服务</li>\n<li>端口范围只能是 30000-32767</li>\n<li>如果节点 的 IP 地址发生变化，调用方需要能够察觉。<br />\n所以，K8s 为服务的外部访问路由提供了新的类型 Ingress。</li>\n</ol>\n<h4 id=\"ingress\"><a class=\"markdownIt-Anchor\" href=\"#ingress\"></a> Ingress</h4>\n<p>Ingress 其实是一种类似于路由表一样的配置，实际的路由工作需要 Ingress Controller 执行。K8s 本身并没有提供 Ingress Controller，目前常用的是通过 Nginx 实现的版本 <a href=\"https://github.com/kubernetes/ingress-nginx\">https://github.com/kubernetes/ingress-nginx</a> 。可以使用上面压缩包中的 ingress-controller.yaml 安装</p>\n<p><img src=\"/asset/k8s-ramp-up/15.png\" alt=\"\" /></p>\n<p>Ingree nginx controller 通过宿主机暴露给外部访问的端口是随机的，所以我们修改 yaml，改成我们在一开始创建集群时映射的端口。</p>\n<p>通过 <code>kubectl get svc -n ingress-nginx</code> 我们就可以看到，暴露的是两个随机分配的端口</p>\n<p><img src=\"/asset/k8s-ramp-up/16.png\" alt=\"\" /></p>\n<p>我们手动将其改成 32080 和 32443。</p>\n<p><img src=\"/asset/k8s-ramp-up/17.png\" alt=\"\" /></p>\n<p>Ingress nginx controller 同样是通过标签选择器的方式管理 Ingress。</p>\n<p>下面是一个简单的 Ingress，将发往 Host <a href=\"http://fortune.bytedance.com\">fortune.bytedance.com</a> 的请求路由到 Service fortune-teller-service。</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">extensions/v1beta1</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Ingress</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">annotations:</span>\n    <span class=\"hljs-attr\">nginx.ingress.kubernetes.io/backend-protocol:</span> <span class=\"hljs-string\">GRPC</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">fortune-ingress</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">rules:</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">host:</span> <span class=\"hljs-string\">fortune.bytedance.com</span>\n    <span class=\"hljs-attr\">http:</span>\n      <span class=\"hljs-attr\">paths:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">backend:</span>\n          <span class=\"hljs-attr\">serviceName:</span> <span class=\"hljs-string\">fortune-teller-service</span>\n          <span class=\"hljs-attr\">servicePort:</span> <span class=\"hljs-string\">grpc</span>\n</code></pre>\n<p>安装 Ingress</p>\n<pre class=\"highlight\"><code class=\"bash\">kubectl apply -f ingress.yaml\nkubectl get ingress\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/18.png\" alt=\"\" /></p>\n<p>Ingress nginx controller 对于 gRpc 默认只支持 SSL 的形式，而Fedlearner 中的 controller 做了一些定制化操作，使得通过 80 端口，只使用 HTTP2 也可以转发 gRpc。详情可以参考 <a href=\"https://github.com/bytedance/ingress-nginx/pull/2/files\">https://github.com/bytedance/ingress-nginx/pull/2/files</a></p>\n<p>使用下面这个命令，我们可以测试一下从外部访问服务</p>\n<pre class=\"highlight\"><code class=\"bash\">grpcurl -insecure -servername <span class=\"hljs-string\">&#x27;fortune.bytedance.com&#x27;</span> 0.0.0.0:32443 build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/19.png\" alt=\"\" /></p>\n<p>刚才也提到了，gRpc 通常是使用 SSL 进行加密的，SSL 的关键在于公钥，私钥以及证书的验证。通过文件系统的方式确实可以处理证书的问题，但 K8s 抽象出 Secret 这种资源，大大提高了对这类文件的管理和复用能力。</p>\n<h4 id=\"secret\"><a class=\"markdownIt-Anchor\" href=\"#secret\"></a> Secret</h4>\n<p>Secret 解决了密码、token、密钥等敏感数据的存储问题，主要分为三种类型：</p>\n<ul>\n<li>Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的 / run/secrets/kubernetes.io/serviceaccount 目录中</li>\n<li>Opaque ：Base64 编码格式的 Secret，用来存储密码、密钥等</li>\n<li><a href=\"http://kubernetes.io/dockerconfigjson\">kubernetes.io/dockerconfigjson</a> ：用来存储 docker registry 的认证信息</li>\n</ul>\n<p>接下来，我们就来创建一个 Opaque 类型的 Secret，使得 ingress nginx controller 支持服务端的 SSL。<br />\n由于篇幅有限，这里简单介绍下证书相关的概念：</p>\n<ul>\n<li>CA：证书授权中心(certificate authority)，用来签发私钥，并验证公钥，私钥的合法性</li>\n<li>私钥，公钥：私钥用于加密，公钥用于解密</li>\n</ul>\n<p>Secret 支持不编写 yaml，直接从文件中创建 Secret</p>\n<pre class=\"highlight\"><code class=\"bash\">kubectl create secret generic fortune-teller-ssl-verify \\\n  --from-file=ca.crt=CA.pem \\\n  --from-file=tls.crt=server-public.pem \\\n  --from-file=tls.key=server-private.key\n</code></pre>\n<p>修改 Ingress，使其使用新创建的 Secret 提供服务侧的 SSL。</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">extensions/v1beta1</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Ingress</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">annotations:</span>\n    <span class=\"hljs-attr\">nginx.ingress.kubernetes.io/backend-protocol:</span> <span class=\"hljs-string\">GRPC</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">fortune-ingress</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">rules:</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">host:</span> <span class=\"hljs-string\">fortune.test.com</span>\n    <span class=\"hljs-attr\">http:</span>\n      <span class=\"hljs-attr\">paths:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">backend:</span>\n          <span class=\"hljs-attr\">serviceName:</span> <span class=\"hljs-string\">fortune-teller-service</span>\n          <span class=\"hljs-attr\">servicePort:</span> <span class=\"hljs-string\">grpc</span>\n  <span class=\"hljs-attr\">tls:</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">hosts:</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">fortune.test.com</span>\n    <span class=\"hljs-attr\">secretName:</span> <span class=\"hljs-string\">fortune-teller-ssl-verify</span>\n</code></pre>\n<p>因为我们使用的是自签名的证书，不被公共的 CA 所信任，所以在发送请求是需要手动指定自己所信任的 CA。</p>\n<pre class=\"highlight\"><code class=\"bash\">grpcurl -cacert CA.pem \\\n  -servername <span class=\"hljs-string\">&#x27;fortune.test.com&#x27;</span> \\\n  127.0.0.1:32443 \\\n  build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/20.png\" alt=\"\" /></p>\n<h2 id=\"参考\"><a class=\"markdownIt-Anchor\" href=\"#参考\"></a> 参考</h2>\n<ul>\n<li><a href=\"https://draveness.me/docker/\">https://draveness.me/docker/</a></li>\n<li><a href=\"https://www.yuque.com/kshare/2020/fe3b6f86-3b9b-48da-9770-897d838cbf41?language=zh-cn#Namespace\">https://www.yuque.com/kshare/2020/fe3b6f86-3b9b-48da-9770-897d838cbf41?language=zh-cn#Namespace</a></li>\n<li><a href=\"https://network.51cto.com/art/201907/598970.htm\">https://network.51cto.com/art/201907/598970.htm</a></li>\n<li><a href=\"https://blog.csdn.net/gatieme/article/details/51383322\">https://blog.csdn.net/gatieme/article/details/51383322</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"目标\"><a class=\"markdownIt-Anchor\" href=\"#目标\"></a> 目标</h2>\n<ul>\n<li>介绍 K8s，Docker 概念以及原理</li>\n<li>从 0 开始部署一个简单完整的服务</li>\n</ul>\n<h2 id=\"docker是什么\"><a class=\"markdownIt-Anchor\" href=\"#docker是什么\"></a> Docker是什么？</h2>\n<p>Docker是由Google推出的Go语言进行开发实现，基于Linux内核的 <font color=red>namespace</font>，对<font color=red>进程</font>进行封装<font color=red>隔离</font>，属于操作系统层面的容器化技术。</p>\n<p><img src=\"/asset/k8s-ramp-up/1.png\" alt=\"\" /></p>\n<h3 id=\"三大核心概念\"><a class=\"markdownIt-Anchor\" href=\"#三大核心概念\"></a> 三大核心概念</h3>\n<p>镜像（Image）</p>\n<p>容器（Container）</p>\n<p>仓库（Repository）</p>\n<p>从代码的角度来看，镜像就像一个类；容器是对象实例，运行时在系统中会有许多容器；仓库主要用于存储和维护这些镜像。</p>\n<h3 id=\"为什么使用-docker\"><a class=\"markdownIt-Anchor\" href=\"#为什么使用-docker\"></a> 为什么使用 Docker？</h3>\n<ul>\n<li>配置环境<br />\n开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性</li>\n<li>应用隔离<br />\n机器上可能同时运行多个服务。如果服务之间没有隔离，一个服务出现异常，往往可能会导致其他服务也挂掉。同时，不同服务所依赖的环境也可能发生冲突。</li>\n</ul>\n<h3 id=\"原理\"><a class=\"markdownIt-Anchor\" href=\"#原理\"></a> 原理</h3>\n<p>首先，要了解一下进程的命名空间。Linux 系统中的所有进程按照惯例是通过PID标识的，这意味着内核必须管理一个全局的PID列表。而且，所有调用者通过uname系统调用返回的系统相关信息（包括系统名称和有关内核的一些信息）都是相同的。</p>\n<p>Linux 的命名空间从内核层面上进行了虚拟化，对所有的全局资源进行一个抽象。本质上，建立了系统的不同视图。每一项全局资源都必须包装到命名空间的数据结构中，只有资源和包含资源的命名空间构成的二元组仍然是全局唯一的。不仅仅是 PID，Linux 通过同样的方法对其他资源也做了虚拟化处理。命名空间共有以下6种：</p>\n<p><img src=\"/asset/k8s-ramp-up/2.png\" alt=\"\" /></p>\n<p>借助 Linux 的命名空间，Docker 对进程进行隔离，可以从进程树的角度理解。</p>\n<p><img src=\"/asset/k8s-ramp-up/3.png\" alt=\"\" /></p>\n<p>每次在执行 <code>docker start</code> 或 <code>docker run</code> 的时候，其实是由 docker 的 daemon 进程 docker containerd，调用 Linux 系统调用 <code>clone()</code> 去创建新的进程。而创建进程的过程中就为新创建的进程分配了新的 Linux 命名空间。可以简单阅读一下 docker 的开源代码</p>\n<pre><code class=\"go\">// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L17\n// 创建容器的函数，其中又调用了设置\nfunc (daemon *Daemon) ContainerStart(name string, hostConfig *containertypes.HostConfig, checkpoint string, checkpointDir string) error\n\n// https://github.com/moby/moby/blob/470ae8422fc6f1845288eb7572253b08f1e6edf8/daemon/oci_linux.go#L212\n// 设置 Namespace\nfunc setNamespace(s *specs.Spec, ns specs.LinuxNamespace) &#123;\n   for i, n := range s.Linux.Namespaces &#123;\n      if n.Type == ns.Type &#123;\n         s.Linux.Namespaces[i] = ns\n         return\n      &#125;\n   &#125;\n   s.Linux.Namespaces = append(s.Linux.Namespaces, ns)\n&#125;\n\n// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L198\n// 创建新的进程\npid, err := daemon.containerd.Start(context.Background(), \n                                    container.ID, \n                                    checkpointDir,    \n                                    container.StreamConfig.Stdin() != nil | | container.Config.Tty, \n                                    container.InitializeStdio)\n</code></pre>\n<h3 id=\"如何安装\"><a class=\"markdownIt-Anchor\" href=\"#如何安装\"></a> 如何安装？</h3>\n<p><a href=\"https://docs.docker.com/get-docker/\">https://docs.docker.com/get-docker/</a></p>\n<h2 id=\"kubernetes是什么\"><a class=\"markdownIt-Anchor\" href=\"#kubernetes是什么\"></a> Kubernetes是什么？</h2>\n<p>Kubernetes 是 Google 于 2014 年基于其内部 Brog 系统开源的一个容器编排管理系统，可使用声明式的配置（以 yaml 文件的形式）自动地执行容器化应用程序的管理，包括部署、伸缩、负载均衡、回滚等。</p>\n<p>为什么叫 K8s？因为 K<font color=red>ubernete</font>s，中间是8个字母。</p>\n<p>kubernetes 提供的功能：</p>\n<ul>\n<li>自动发布与伸缩：可以通过声明式的配置文件定义想要部署的容器</li>\n<li>滚动升级与灰度发布：采用逐步替换的策略实现滚动升级</li>\n<li>服务发现与负载均衡：Kubernetes 通过 DNS 名称或 IP 地址暴露容器的访问方式，并且可在同一容器组内实现负载分发与均衡</li>\n<li>存储编排：Kubernetes 可以自动挂载指定的存储系统，如 local storage/nfs / 云存储等</li>\n<li>故障恢复：Kubernetes 自动重启已经停机的容器，替换不满足健康检查的容器</li>\n<li>密钥与配置管理：Kubernetes 可以存储与管理敏感信息，如 Docker Registry 的登录凭证，密码，ssh 密钥等</li>\n</ul>\n<h3 id=\"为什么使用-k8s\"><a class=\"markdownIt-Anchor\" href=\"#为什么使用-k8s\"></a> 为什么使用 K8s？</h3>\n<p>大型单体应用被逐渐拆分成小的、可独立运行的组件。随着部署组件的增多和数据中心的增长，配置、管理和运维变得很困难。(微服务）</p>\n<p>K8s 的定义就是容器编排和管理引擎，解决了这些问题。</p>\n<h3 id=\"如何安装-2\"><a class=\"markdownIt-Anchor\" href=\"#如何安装-2\"></a> 如何安装？</h3>\n<p>由难到易(๑•̀ㅂ•́)و✧</p>\n<ul>\n<li>Kubeadm: <a href=\"https://kubernetes.io/docs/reference/setup-tools/kubeadm/\">https://kubernetes.io/docs/reference/setup-tools/kubeadm/</a></li>\n<li>MiniKube: Local kubernetes <a href=\"https://minikube.sigs.k8s.io/docs/start/\">https://minikube.sigs.k8s.io/docs/start/</a></li>\n<li>Kind: Kubernetes in Docker <a href=\"https://github.com/kubernetes-sigs/kind\">https://github.com/kubernetes-sigs/kind</a></li>\n<li>Docker-desktop（仅限 Mac）: 一键开启<br />\n<img src=\"/asset/k8s-ramp-up/4.png\" alt=\"\" /></li>\n</ul>\n<p>其他版本的类 K8s 系统：</p>\n<ul>\n<li>K3s: <a href=\"https://github.com/k3s-io/k3s\">https://github.com/k3s-io/k3s</a></li>\n<li>K0s: <a href=\"https://github.com/k0sproject/k0s\">https://github.com/k0sproject/k0s</a></li>\n</ul>\n<h2 id=\"kubernetes-架构\"><a class=\"markdownIt-Anchor\" href=\"#kubernetes-架构\"></a> Kubernetes 架构</h2>\n<p><img src=\"/asset/k8s-ramp-up/5.png\" alt=\"\" /></p>\n<h3 id=\"master\"><a class=\"markdownIt-Anchor\" href=\"#master\"></a> master</h3>\n<p>Master 负责管理服务来对整个系统进行管理与控制，包括</p>\n<ul>\n<li>apiserver：作为整个系统的对外接口，提供一套 Restful API 供客户端调用，任何的资源请求 / 调用操作都是通过 kube-apiserver 提供的接口进行, 如 kubectl、kubernetes dashboard 等管理工具就是通过 apiserver 来实现对集群的管理</li>\n<li>kube-scheduler：资源调度器，负责将容器组分配到哪些节点上</li>\n<li>kube-controller-manager：管理控制器，集群中处理常规任务的后台线程，包括节点控制器（负责监听节点停机的事件并作出对应响应）、endpoint-controller（刷新服务与容器组的关联信息）、replication-controller（维护容器组的副本数为指定的数值）、Service Account &amp; Token 控制器（负责为新的命名空间创建默认的 Service Account 以及 API Access Token）</li>\n<li>etcd：数据存储，存储集群所有的配置信息</li>\n<li>coredns：实现集群内部通过服务名称进行容器组访问的功能</li>\n</ul>\n<h3 id=\"worker\"><a class=\"markdownIt-Anchor\" href=\"#worker\"></a> worker</h3>\n<p>Worker 负载执行 Master 分配的任务，包括</p>\n<ul>\n<li>kubelet：是工作节点上执行操作的代理程序，负责容器的生命周期管理，定期执行容器健康检查，并上报容器的运行状态</li>\n<li>kube-proxy：是一个具有负载均衡能力的简单的网络访问代理，负责将访问某个服务的请求分配到工作节点的具体某个容器上（kube-proxy 也运行于 master node 上）</li>\n<li>Docker Daemon：Kubernetes 其实不局限于 Docker（即将取消），它支持任何实现了 Kubernetes 容器引擎接口的容器引擎，如 containerd、rktlet</li>\n</ul>\n<h3 id=\"网络通信\"><a class=\"markdownIt-Anchor\" href=\"#网络通信\"></a> 网络通信</h3>\n<p>网络通信组件只需要符合 CNI （Container Network Interface）接口规范，主要作用在于给各个容器分配集群内 IP，使得其内网 IP 能够集群内唯一，并且可以互相访问，目前常用的有 Flannel，Calico等网络组件。</p>\n<p>简单介绍下比较常用的 Flannel 的原理。Flannel 运行在第3层网络层，基于 IPv4，创建一个大型内部网络，跨越集群中每个节点。每个节点组成一个子网，每个容器在内网中有唯一的IP。</p>\n<p>首先，Flannel 会为每台节点分配一个子网段。Flanneld 在 Docker 容器启动时修改其启动参数，将其 IP 限制在当前的子网段内，具体 IP 的分配仍是由 docker 进行。Flannel通过Etcd服务维护了一张节点间的路由表，详细记录了各节点子网网段，保证不同节点的子网网段不会重复。</p>\n<p>数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，flanneld服务监听在网卡的另外一端。</p>\n<p>源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。</p>\n<p><img src=\"/asset/k8s-ramp-up/6.png\" alt=\"\" /></p>\n<h2 id=\"快速上手-k8s-概念\"><a class=\"markdownIt-Anchor\" href=\"#快速上手-k8s-概念\"></a> 快速上手 K8s 概念</h2>\n<p>一些推荐的 K8s 概念介绍：</p>\n<ul>\n<li>微软的 50天 K8s 教程中（<a href=\"https://azure.microsoft.com/en-us/resources/kubernetes-learning-path/%EF%BC%89%E9%80%9A%E8%BF%87%E5%8A%A8%E7%89%A9%E5%9B%AD%E7%9A%84%E5%BD%A2%E5%BC%8F%E4%BB%8B%E7%BB%8D%E4%BA%86%E4%B8%80%E4%BA%9B\">https://azure.microsoft.com/en-us/resources/kubernetes-learning-path/）通过动物园的形式介绍了一些</a> K8s 概念 <a href=\"http://aka.ms/k8s/LearnwithPhippy\">http://aka.ms/k8s/LearnwithPhippy</a></li>\n<li>综述PPT：<a href=\"https://www2.slideshare.net/BobKillen/kubernetes-a-comprehensive-overview-updated?from_action=save\">https://www2.slideshare.net/BobKillen/kubernetes-a-comprehensive-overview-updated?from_action=save</a></li>\n</ul>\n<p>K8s 中的概念极多，比较零碎，这里通过一个简单的小例子，尽可能覆盖多的 K8s 概念。</p>\n<h2 id=\"概览\"><a class=\"markdownIt-Anchor\" href=\"#概览\"></a> 概览</h2>\n<p>例子使用一个开源的 fortune-teller 镜像（<code>quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1</code>） ，每次请求容器内的服务，服务会返回一句名言。希望在 MacOS 的环境下，展示一个应用在 K8s 中运行的全流程。</p>\n<p>准备环境<br />\n为了不影响大家本地的环境，这里使用 Kind 创建出一个独立的 K8s 集群，方便统一版本并且可以在完成快速清理掉。(Docker 双重隔离）</p>\n<ol>\n<li>安装 Kind 以及 gRpc 测试工具</li>\n</ol>\n<pre class=\"highlight\"><code class=\"bash\">brew install kind\nbrew install grpcurl\n</code></pre>\n<ol start=\"2\">\n<li>拉取镜像</li>\n</ol>\n<pre class=\"highlight\"><code class=\"bash\">docker pull kindest/node:v1.16.15\ndocker pull quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n</code></pre>\n<ol start=\"3\">\n<li>创建 K8s 集群，因为 Kind 是在 Docker 容器里面创建的 K8s，所以宿主机访问，需要把端口暴露出来。Kind 会默认把 K8s apiserver 的端口暴露出来，用来给 kubectl 命令使用。但为了之后的测试，我们提前把几个端口在创建的时候就暴露出来。</li>\n</ol>\n<p>Kind 同样支持通过yaml 的形式创建集群</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Cluster</span>\n<span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">kind.x-k8s.io/v1alpha4</span>\n<span class=\"hljs-attr\">nodes:</span>\n<span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">role:</span> <span class=\"hljs-string\">control-plane</span>\n  <span class=\"hljs-attr\">extraPortMappings:</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">containerPort:</span> <span class=\"hljs-number\">32080</span>\n    <span class=\"hljs-attr\">hostPort:</span> <span class=\"hljs-number\">32080</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">containerPort:</span> <span class=\"hljs-number\">32443</span>\n    <span class=\"hljs-attr\">hostPort:</span> <span class=\"hljs-number\">32443</span>\n</code></pre>\n<pre class=\"highlight\"><code class=\"bash\">kind create cluster --name=fortune-teller --image=kindest/node:v1.16.15 --config kind-config.yaml\n</code></pre>\n<h3 id=\"运行-docker-版本\"><a class=\"markdownIt-Anchor\" href=\"#运行-docker-版本\"></a> 运行 Docker 版本</h3>\n<ol>\n<li>启动容器</li>\n</ol>\n<pre class=\"highlight\"><code class=\"bash\">docker run -p 50051:50051 quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n</code></pre>\n<p><code>-p</code> 将容器内的 50051 端口映射到宿主机的 50051 端口</p>\n<ol start=\"2\">\n<li>测试应用是否正常运行，第一次运行时可能需要给 grpcurl 开启权限</li>\n</ol>\n<pre class=\"highlight\"><code class=\"bash\">grpcurl -plaintext 127.0.0.1:50051 build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n<p>应用在收到请求以后，会返回一句名言</p>\n<p><img src=\"/asset/k8s-ramp-up/7.png\" alt=\"\" /></p>\n<h3 id=\"将应用从-docker-迁移到-k8s-中\"><a class=\"markdownIt-Anchor\" href=\"#将应用从-docker-迁移到-k8s-中\"></a> 将应用从 Docker 迁移到 K8s 中</h3>\n<p>与 Docker 中容器概念相对应的，K8s 中也有着容器的概念。对于虚拟化的容器来说，最佳实践是一个容器一个应用，但当一个服务需要多个应用组合完成时，简单的将多个应用部署到一个容器内，就破坏了应用之间的隔离性，所以 K8s 对于容器进行了一层封装，形成了 Pod 的概念。</p>\n<h4 id=\"pod\"><a class=\"markdownIt-Anchor\" href=\"#pod\"></a> Pod</h4>\n<p>Pod 是 Kubernetes 创建或部署的最小基本单元。一个 Pod 封装一个或多个应用容器、存储资源、一个独立的网络 IP 以及管理控制容器运行方式的策略选项。Pod 中的每个容器共享网络命名空间（包括 IP 与端口），Pod 内的容器可以使用 localhost 相互通信。Pod 可以指定一组共享存储卷 Volumes，Pod 中所有容器都可以访问共享的 Volumes。</p>\n<p>通过 Pod，用户就可以非常方便地控制容器之间的隔离性。</p>\n<p>有了 Pod 作为基础以后，K8s 就要实现它最重要的功能，对容器的编排管理。当服务需要扩容时，K8s 需要能够快速复制 Pod，当 Pod 挂掉了，K8s 需要能够自动重启。所以 K8s 由此衍生出了 ReplicaSet 的概念。</p>\n<h4 id=\"replicaset\"><a class=\"markdownIt-Anchor\" href=\"#replicaset\"></a> ReplicaSet</h4>\n<p>ReplicaSet 确保在任何时候都有按配置的 Pod 副本数在运行，通过标签选择器的方式对 Pod 进行筛选和管理。在旧的版本中还有一个 ReplicaController 的概念，RC 与 RS 两者功能完全相同，区别仅仅在于 RS 对于 Pod 的标签选择器更加强大。</p>\n<p>开头提到了 K8s 使用声明式的配置自动去管理容器，而 ReplicaSet 的内容却太过具体，涉及到了 Pod 的具体维护细节。所以 K8s 在 ReplicaSet 之上又衍生出声明式配置容器的概念，Deployment。</p>\n<h4 id=\"deployment\"><a class=\"markdownIt-Anchor\" href=\"#deployment\"></a> Deployment</h4>\n<p>Deployment 为 Pod 与 ReplicaSet 提供了声明式的定义，描述你想要的目标状态是什么，Deployment controller 就会帮你将 Pod 与 ReplicaSet 的实际状态改变到你想要的目标状态。</p>\n<p>以 fortune-teller 为例子，可以编写一份下面这样的 Deployment 配置文件</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">apps/v1</span> <span class=\"hljs-comment\"># k8s api版本</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Deployment</span> <span class=\"hljs-comment\"># 资源类型</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">fortune-teller-app</span> <span class=\"hljs-comment\"># deployment 名字</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">replicas:</span> <span class=\"hljs-number\">1</span> <span class=\"hljs-comment\"># Pod 副本数量</span>\n  <span class=\"hljs-attr\">selector:</span>\n    <span class=\"hljs-attr\">matchLabels:</span>\n      <span class=\"hljs-attr\">k8s-app:</span> <span class=\"hljs-string\">fortune-teller-app</span> <span class=\"hljs-comment\"># 管理标签中包含 k8s-app: fortune-teller-app 的 Pod</span>\n  <span class=\"hljs-attr\">template:</span> <span class=\"hljs-comment\"># Pod 模板</span>\n    <span class=\"hljs-attr\">metadata:</span>\n      <span class=\"hljs-attr\">labels:</span>\n        <span class=\"hljs-attr\">k8s-app:</span> <span class=\"hljs-string\">fortune-teller-app</span> <span class=\"hljs-comment\"># Pod 标签</span>\n    <span class=\"hljs-attr\">spec:</span> <span class=\"hljs-comment\"># Pod 配置</span>\n      <span class=\"hljs-attr\">containers:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">image:</span> <span class=\"hljs-string\">quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1</span>\n        <span class=\"hljs-attr\">imagePullPolicy:</span> <span class=\"hljs-string\">IfNotPresent</span>\n        <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">fortune-teller-app</span>\n        <span class=\"hljs-attr\">ports:</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">containerPort:</span> <span class=\"hljs-number\">50051</span>\n          <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">grpc</span>\n          <span class=\"hljs-attr\">protocol:</span> <span class=\"hljs-string\">TCP</span>\n</code></pre>\n<p>将上面的内容保存到一份 yaml 文件中，执行以下命令，让 K8s 执行 yaml</p>\n<pre class=\"highlight\"><code class=\"bash\">kubectl apply -f deployment.yaml\n</code></pre>\n<p>通过以下命令，我们就可以看到刚刚创建的 deployment</p>\n<pre class=\"highlight\"><code class=\"bash\">kubectl get deployement\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/8.png\" alt=\"\" /></p>\n<p>此时，K8s 已经自动根据 deployment 中配置的 Pod 模板和配置，创建了 Pod。通过以下命令，我们就可以看到 K8s 自动创建的 Pod</p>\n<pre class=\"highlight\"><code class=\"bash\">kubectl get pods\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/9.png\" alt=\"\" /></p>\n<p>因为 K8s 采用声明式的配置去管理 Pod，所以我们可以动态地去修改 deployment 的配置，K8s 会自动根据新的配置去管理 Pod。</p>\n<pre class=\"highlight\"><code class=\"bash\">kubectl edit deployment fortune-teller-app\n</code></pre>\n<p>我们把配置文件中的副本数量修改为 2</p>\n<p><img src=\"/asset/k8s-ramp-up/10.png\" alt=\"\" /></p>\n<p>保存退出后，我们再次执行 kubectl get pods ，我们就可以看到 K8s 根据新的配置，创建了一个新的 Pod</p>\n<p><img src=\"/asset/k8s-ramp-up/11.png\" alt=\"\" /></p>\n<p>现在我们就有了两个 fortune-teller 的服务。在真实环境中，Pod 的调度由 K8s 进行管理，某个时刻服务可能在 Node1 上，而另一时刻服务可能就被调度到了 Node2 上。所以，访问具体 Pod 是一种不稳定的服务访问方法，而且目前大多数的后端服务都是无状态的服务，直接访问 Pod 也导致不能进行负载均衡。所以，K8s 在此基础上衍生出 Service 的概念。</p>\n<h4 id=\"service\"><a class=\"markdownIt-Anchor\" href=\"#service\"></a> Service</h4>\n<p>Service 可以看做一组提供相同服务的 Pod 的对外访问接口。Kubernetes 提供三种类型的 Service：</p>\n<ul>\n<li>NodePort： 集群外部可以通过 Node IP 与 Node Port 来访问具体某个 Pod，每台机器上都会暴露同样的端口</li>\n<li>ClusterIP：指通过集群的内部 IP 暴露服务，服务只能够在集群内部可以访问，这也是默认的 ServiceType</li>\n<li>ExternalName：不指向 Pod，指向外部服务<br />\nService 和 Deployment 是一对比较容易混淆的概念，两者都是对一组 Pod 进行管理，但它们两者之间的关系可以用下面这张图来概括</li>\n</ul>\n<p><img src=\"/asset/k8s-ramp-up/12.png\" alt=\"\" /></p>\n<p>Service 是面向服务调用者，也就是外部访问 K8s。而 Deployment 是面向 K8s 底层引擎的，面向内部管理者。</p>\n<p>Service 的配置文件格式与 Deployment 很类似</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">v1</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Service</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">fortune-teller-service</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">ports:</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">grpc</span>\n    <span class=\"hljs-attr\">port:</span> <span class=\"hljs-number\">50051</span>\n    <span class=\"hljs-attr\">protocol:</span> <span class=\"hljs-string\">TCP</span>\n    <span class=\"hljs-attr\">targetPort:</span> <span class=\"hljs-number\">50051</span>\n  <span class=\"hljs-attr\">selector:</span>\n    <span class=\"hljs-attr\">k8s-app:</span> <span class=\"hljs-string\">fortune-teller-app</span>\n  <span class=\"hljs-attr\">type:</span> <span class=\"hljs-string\">ClusterIP</span>\n</code></pre>\n<p>同样的，我们通过 kubectl apply -f service.yaml 命令，可以创建 Service。通过 kubectl get service 可以查看到刚刚创建的 Service。</p>\n<p><img src=\"/asset/k8s-ramp-up/13.png\" alt=\"\" /></p>\n<p>接下来，我们登陆到一个 Pod 里去测试一下是否可以访问服务。<br />\nNetshoot 镜像中包含了一些网络测试的工具，我们可以直接进入一个 netshoot 容器内测试。采用 deployment 的方式创建 Pod</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">apps/v1</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Deployment</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">netshoot</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">replicas:</span> <span class=\"hljs-number\">1</span>\n  <span class=\"hljs-attr\">selector:</span>\n    <span class=\"hljs-attr\">matchLabels:</span>\n      <span class=\"hljs-attr\">k8s-app:</span> <span class=\"hljs-string\">netshoot</span>\n  <span class=\"hljs-attr\">template:</span>\n    <span class=\"hljs-attr\">metadata:</span>\n      <span class=\"hljs-attr\">labels:</span>\n        <span class=\"hljs-attr\">k8s-app:</span> <span class=\"hljs-string\">netshoot</span>\n    <span class=\"hljs-attr\">spec:</span>\n      <span class=\"hljs-attr\">containers:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">args:</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">1000d</span>\n        <span class=\"hljs-attr\">command:</span>\n        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">/bin/sleep</span>\n        <span class=\"hljs-attr\">image:</span> <span class=\"hljs-string\">nicolaka/netshoot</span>\n        <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">netshoot</span>\n</code></pre>\n<p>与 Docker 的命令类似，使用命令 <code>kubectl cp grpcurl_1.6.0_linux_x86_64.tar.gz &lt;pod name&gt;:/</code> 复制工具到容器内。</p>\n<p>复制成功后，使用命令 kubectl exec -it <pod name> bash 可以进入到容器内。</p>\n<p>解压</p>\n<pre class=\"highlight\"><code class=\"bash\"><span class=\"hljs-built_in\">cd</span> /\ntar -zxf grpcurl_1.6.0_linux_x86_64.tar.gz\n</code></pre>\n<p>然后我们可以测试是否可以从 K8s 集群内访问 Service。对于 K8s 集群内部的服务，K8s 有自己的 DNS 组件，所以可以直接通过服务名访问。</p>\n<pre class=\"highlight\"><code class=\"bash\">./grpcurl -plaintext fortune-teller-service:50051 build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/14.png\" alt=\"\" /></p>\n<p>验证服务可以从集群内访问之后，我们就需要解决如何从集群外访问服务的问题，毕竟大多数服务是面向 K8s 集群外的用户的。其实目前我们已经了解了一种解决方案，就是使用 Nodeport 类型的 Service。但采用这种方法有几个缺点：</p>\n<ol>\n<li>每个端口只能是一种服务</li>\n<li>端口范围只能是 30000-32767</li>\n<li>如果节点 的 IP 地址发生变化，调用方需要能够察觉。<br />\n所以，K8s 为服务的外部访问路由提供了新的类型 Ingress。</li>\n</ol>\n<h4 id=\"ingress\"><a class=\"markdownIt-Anchor\" href=\"#ingress\"></a> Ingress</h4>\n<p>Ingress 其实是一种类似于路由表一样的配置，实际的路由工作需要 Ingress Controller 执行。K8s 本身并没有提供 Ingress Controller，目前常用的是通过 Nginx 实现的版本 <a href=\"https://github.com/kubernetes/ingress-nginx\">https://github.com/kubernetes/ingress-nginx</a> 。可以使用上面压缩包中的 ingress-controller.yaml 安装</p>\n<p><img src=\"/asset/k8s-ramp-up/15.png\" alt=\"\" /></p>\n<p>Ingree nginx controller 通过宿主机暴露给外部访问的端口是随机的，所以我们修改 yaml，改成我们在一开始创建集群时映射的端口。</p>\n<p>通过 <code>kubectl get svc -n ingress-nginx</code> 我们就可以看到，暴露的是两个随机分配的端口</p>\n<p><img src=\"/asset/k8s-ramp-up/16.png\" alt=\"\" /></p>\n<p>我们手动将其改成 32080 和 32443。</p>\n<p><img src=\"/asset/k8s-ramp-up/17.png\" alt=\"\" /></p>\n<p>Ingress nginx controller 同样是通过标签选择器的方式管理 Ingress。</p>\n<p>下面是一个简单的 Ingress，将发往 Host <a href=\"http://fortune.bytedance.com\">fortune.bytedance.com</a> 的请求路由到 Service fortune-teller-service。</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">extensions/v1beta1</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Ingress</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">annotations:</span>\n    <span class=\"hljs-attr\">nginx.ingress.kubernetes.io/backend-protocol:</span> <span class=\"hljs-string\">GRPC</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">fortune-ingress</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">rules:</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">host:</span> <span class=\"hljs-string\">fortune.bytedance.com</span>\n    <span class=\"hljs-attr\">http:</span>\n      <span class=\"hljs-attr\">paths:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">backend:</span>\n          <span class=\"hljs-attr\">serviceName:</span> <span class=\"hljs-string\">fortune-teller-service</span>\n          <span class=\"hljs-attr\">servicePort:</span> <span class=\"hljs-string\">grpc</span>\n</code></pre>\n<p>安装 Ingress</p>\n<pre class=\"highlight\"><code class=\"bash\">kubectl apply -f ingress.yaml\nkubectl get ingress\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/18.png\" alt=\"\" /></p>\n<p>Ingress nginx controller 对于 gRpc 默认只支持 SSL 的形式，而Fedlearner 中的 controller 做了一些定制化操作，使得通过 80 端口，只使用 HTTP2 也可以转发 gRpc。详情可以参考 <a href=\"https://github.com/bytedance/ingress-nginx/pull/2/files\">https://github.com/bytedance/ingress-nginx/pull/2/files</a></p>\n<p>使用下面这个命令，我们可以测试一下从外部访问服务</p>\n<pre class=\"highlight\"><code class=\"bash\">grpcurl -insecure -servername <span class=\"hljs-string\">&#x27;fortune.bytedance.com&#x27;</span> 0.0.0.0:32443 build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/19.png\" alt=\"\" /></p>\n<p>刚才也提到了，gRpc 通常是使用 SSL 进行加密的，SSL 的关键在于公钥，私钥以及证书的验证。通过文件系统的方式确实可以处理证书的问题，但 K8s 抽象出 Secret 这种资源，大大提高了对这类文件的管理和复用能力。</p>\n<h4 id=\"secret\"><a class=\"markdownIt-Anchor\" href=\"#secret\"></a> Secret</h4>\n<p>Secret 解决了密码、token、密钥等敏感数据的存储问题，主要分为三种类型：</p>\n<ul>\n<li>Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的 / run/secrets/kubernetes.io/serviceaccount 目录中</li>\n<li>Opaque ：Base64 编码格式的 Secret，用来存储密码、密钥等</li>\n<li><a href=\"http://kubernetes.io/dockerconfigjson\">kubernetes.io/dockerconfigjson</a> ：用来存储 docker registry 的认证信息</li>\n</ul>\n<p>接下来，我们就来创建一个 Opaque 类型的 Secret，使得 ingress nginx controller 支持服务端的 SSL。<br />\n由于篇幅有限，这里简单介绍下证书相关的概念：</p>\n<ul>\n<li>CA：证书授权中心(certificate authority)，用来签发私钥，并验证公钥，私钥的合法性</li>\n<li>私钥，公钥：私钥用于加密，公钥用于解密</li>\n</ul>\n<p>Secret 支持不编写 yaml，直接从文件中创建 Secret</p>\n<pre class=\"highlight\"><code class=\"bash\">kubectl create secret generic fortune-teller-ssl-verify \\\n  --from-file=ca.crt=CA.pem \\\n  --from-file=tls.crt=server-public.pem \\\n  --from-file=tls.key=server-private.key\n</code></pre>\n<p>修改 Ingress，使其使用新创建的 Secret 提供服务侧的 SSL。</p>\n<pre class=\"highlight\"><code class=\"yaml\"><span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">extensions/v1beta1</span>\n<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Ingress</span>\n<span class=\"hljs-attr\">metadata:</span>\n  <span class=\"hljs-attr\">annotations:</span>\n    <span class=\"hljs-attr\">nginx.ingress.kubernetes.io/backend-protocol:</span> <span class=\"hljs-string\">GRPC</span>\n  <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">fortune-ingress</span>\n  <span class=\"hljs-attr\">namespace:</span> <span class=\"hljs-string\">default</span>\n<span class=\"hljs-attr\">spec:</span>\n  <span class=\"hljs-attr\">rules:</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">host:</span> <span class=\"hljs-string\">fortune.test.com</span>\n    <span class=\"hljs-attr\">http:</span>\n      <span class=\"hljs-attr\">paths:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">backend:</span>\n          <span class=\"hljs-attr\">serviceName:</span> <span class=\"hljs-string\">fortune-teller-service</span>\n          <span class=\"hljs-attr\">servicePort:</span> <span class=\"hljs-string\">grpc</span>\n  <span class=\"hljs-attr\">tls:</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">hosts:</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">fortune.test.com</span>\n    <span class=\"hljs-attr\">secretName:</span> <span class=\"hljs-string\">fortune-teller-ssl-verify</span>\n</code></pre>\n<p>因为我们使用的是自签名的证书，不被公共的 CA 所信任，所以在发送请求是需要手动指定自己所信任的 CA。</p>\n<pre class=\"highlight\"><code class=\"bash\">grpcurl -cacert CA.pem \\\n  -servername <span class=\"hljs-string\">&#x27;fortune.test.com&#x27;</span> \\\n  127.0.0.1:32443 \\\n  build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/20.png\" alt=\"\" /></p>\n<h2 id=\"参考\"><a class=\"markdownIt-Anchor\" href=\"#参考\"></a> 参考</h2>\n<ul>\n<li><a href=\"https://draveness.me/docker/\">https://draveness.me/docker/</a></li>\n<li><a href=\"https://www.yuque.com/kshare/2020/fe3b6f86-3b9b-48da-9770-897d838cbf41?language=zh-cn#Namespace\">https://www.yuque.com/kshare/2020/fe3b6f86-3b9b-48da-9770-897d838cbf41?language=zh-cn#Namespace</a></li>\n<li><a href=\"https://network.51cto.com/art/201907/598970.htm\">https://network.51cto.com/art/201907/598970.htm</a></li>\n<li><a href=\"https://blog.csdn.net/gatieme/article/details/51383322\">https://blog.csdn.net/gatieme/article/details/51383322</a></li>\n</ul>\n"},{"title":"Sealos X Casdoor: 在 Kubernetes 上集成 SSO 服务","math":true,"date":"2022-09-04T23:56:19.000Z","updated":"2022-09-04T23:56:19.000Z","_content":"","source":"_posts/sealos-casdoor.md","raw":"---\ntitle: \"Sealos X Casdoor: 在 Kubernetes 上集成 SSO 服务\"\nmath: true\ndate: 2022-09-04 23:56:19\nupdated: 2022-09-04 23:56:19\n---\n","slug":"sealos-casdoor","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clapldfdd000i18mz6uy24yy2","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Sealos is All You Need —— 3分钟部署 Kubernetes","math":true,"date":"2022-10-30T21:00:00.000Z","updated":"2022-10-30T21:00:00.000Z","_content":"\n![title.png](/asset/sealos-ramp-up/f6ce5cbedc6aa338007cd4633935ad371086271c.png)\n\n## Sealos 是什么？\nKubernetes（K8s）发展至今，已经成为了一个极其复杂的系统。而作为云原生的基石，涌现了一大批辅助工具，帮助用户快速搭建 k8s 集群。而其中，[sealos](https://github.com/labring/sealos) 是一个以 kubernetes 为内核的云操作系统，kuberentes 生命周期管理是 sealos 的一个重要功能，sealos 可以非常方便的安装/升级/伸缩/备份恢复集群等。接下来，让我们通过一个例子来看看 sealos 的强大。\n\n## 如何使用\n\n![Frame 3.png](/asset/sealos-ramp-up/313c52bda7b27b5b64a479e488404bcef56b2669.png)\n\n假设我们想要在 192.168.0.100，192.168.0.101 和 192.168.0.102 这三台机器上部署一主两从的 K8s 集群，那么使用 Sealos 的话，主要输入以下的命令：\n\n```\nsudo sealos run labring/kubernetes:v1.24.0 labring/calico:v3.22.1 \\\n    --masters 192.168.0.100 \\\n    --nodes 192.168.0.101,192.168.0.102 \\\n    --passwd xxx\n```\n\n是的，Sealos 将一个复杂的 K8s 的集群部署简化成了短短一行命令，将部署体验拉到了极致。甚至不需要过多的文档解释，仅凭这一行命令就可以满足大多数普通的部署场景。\n\n## 原理\n\n那么，如此强大的 Sealos 是如何运行的呢？除了刚才演示的 `run` 命令之外，Sealos 还提供了大量提升用户体验的命令，例如 `create` 创建镜像、`reset` 格式化集群等等。由于篇幅有限，本章节就以最核心的 `run` 命令为例，看看 Sealos 在背后替用户完成了哪些自动化的操作。（本文以 Sealos V4.1.0 代码为例）\n\n### Applier\n\n首先，Sealos 会创建一个 `Applier` 结构体，负责了部署集群的核心逻辑。`Applier` 采用了 k8s 的声明式的设计思想，用户声明一个期望的集群状态，而 `Applier` 负责将集群现在的状态转换成用户期望的状态。\n\n```go\ntype Applier struct {\n    ClusterDesired     *v2.Cluster // 用户期望的集群状态\n    ClusterCurrent     *v2.Cluster // 集群当前状态\n    ClusterFile        clusterfile.Interface // 当前集群接口\n    Client             kubernetes.Client\n    CurrentClusterInfo *version.Info\n    RunNewImages       []string // run 命令新增的镜像名称\n}\n```\n\n`clusterfile.Interface` 是一个接口类型，Sealos 中通过 `ClusterFile` 实现了这一接口。因此，`Applier` 结构体中最重要的就是 `Cluster` 和 `ClusterFile` 这两个类型，它们定义了集群的状态和配置。接下来，我们展开介绍一下两者。\n\n#### Cluster\n\n```go\ntype Cluster struct {\n    metav1.TypeMeta   `json:\",inline\"`\n    metav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n    Spec   ClusterSpec   `json:\"spec,omitempty\"`\n    Status ClusterStatus `json:\"status,omitempty\"`\n}\ntype ClusterSpec struct {\n    Image ImageList `json:\"image,omitempty\"`\n    SSH   SSH       `json:\"ssh\"`\n    Hosts []Host    `json:\"hosts,omitempty\"`\n    Env []string `json:\"env,omitempty\"`\n    Command []string `json:\"command,omitempty\"`\n}\ntype ClusterStatus struct {\n    Phase      ClusterPhase       `json:\"phase,omitempty\"`\n    Mounts     []MountImage       `json:\"mounts,omitempty\"`\n    Conditions []ClusterCondition `json:\"conditions,omitempty\" `\n}\n```\n\n`Cluster` 的内容按照 K8s Resource 的格式进行了设计，这非常的 K8s 哈哈。在 `ClusterSpec` 中，定义了一系列用于部署 K8s 集群的参数，例如，镜像、SSH参数、节点等等。\n\n而在 `ClusterStatus` 中，`Phase` 定义了当前集群的状态，`Mounts` 定义了集群使用的镜像，`Conditions` 保存了集群中所发生的一系列事件。\n\n#### ClusterFile\n\n```go\ntype ClusterFile struct {\n    path         string // 保存路径\n    customValues []string\n    customSets   []string\n    customEnvs   []string\n    Cluster      *v2.Cluster // 集群状态\n    Configs      []v2.Config\n    KubeConfig   *runtime.KubeadmConfig // 集群配置\n}\n```\n\n`ClusterFile` 是真正被 `Applier` 操作的对象，以及持久化到文件中的内容。这里包含了所有集群的当前状态信息，同时还包含了 kubeconfig。这里的 kubeconfig 并不是我们平时操作 k8s 时所用的 config 文件，而是一系列用于搭建集群所需的配置项。在使用 `kubeadm` 时，这些配置项往往需要我们手动配置，而 Sealos 在这里会自动帮我们填写并应用于集群中。可以看出，`Cluster` 更像是 `ClusterFile` 的一个实例，记录了集群实时的状态。\n\n### 创建 Applier\n\n创建一个 `Applier` 会经过以下步骤：\n\n1. 判断是否已经存在 `ClusterFile` ，如果存在，那么直接读取，构建出集群状态 `Cluster`。否则，初始化创建一个空的集群状态 `Cluster`。\n\n2. 根据用户本次的参数，更新集群状态 `Cluster` 中的 spec，此时，`Cluster` 即为目标的集群状态。\n\n3. 再次从文件中构建 `ClusterFile`，作为集群当前的状态和对象。\n\n4. 构建 `Applier` 结构体返回。\n\n### Apply\n\n接下来，通过 `Applier.Apply()`，Sealos 开始正式的部署集群，使集群状态向目标靠近。首先，Sealos 会将当前集群的状态置为 `ClusterInProcess`。接下来，根据集群创建或是更新，分别进入两个分支。\n\n#### initCluster\n\n`initCluster` 负责从零开始创建一个集群。函数中会通过 `CreateProcessor` 去部署期望状态的集群。\n\n```go\ntype CreateProcessor struct {\n    ClusterFile     clusterfile.Interface // 当前集群对象\n    ImageManager    types.ImageService // 处理镜像\n    ClusterManager  types.ClusterService // 管理 clusterfile\n    RegistryManager types.RegistryService // 管理镜像 registry\n    Runtime         runtime.Interface // kubeadm 对象\n    Guest           guest.Interface // 基于 sealos 的应用对象\n}\n```\n\n![Slide 16_9 - 1.png](/asset/sealos-ramp-up/c668a66b40dbc788695a9cbb8ec3f76a9897503a.png)\n\n\n\n`CreateProcessor.Execute` 接收期望的集群状态 `ClusterDesired`。接下来会执行一系列 pipeline，正式进入实际的集群部署过程中：\n\n1. Check：检查集群的 host\n\n2. PreProcess：负责集群部署前的镜像预处理操作，在这里就会利用 `CreateProcessor` 中的各个 Manager。\n   \n   1. 拉取镜像\n   \n   2. 检查镜像格式\n   \n   3. 使用 `buildah` 从 OCI 格式的镜像中创建 working container，并将容器挂载到 rootfs 上\n   \n   4. 将容器的 manifest 添加到集群状态中\n\n3. RunConfig：将集群状态中的 working container 导出成 yaml 格式的配置并持久化到宿主机的文件系统中\n\n4. MountRootfs：将挂载的镜像内容按照类别，以 `rootfs`，`addons`，`app` 的顺序分发到每台机器上。\n   \n   这里需要介绍一下 sealos 镜像的一般结构，以最基础的 k8s 镜像为例：\n   \n   ```\n   labring/kubernetes\n   - etc // 配置项\n   - scripts // 脚本\n       - init-containerd.sh\n       - init-kube.sh\n       - init-shim.sh\n       - init-registry.sh\n       - init.sh\n   - Kubefile // dockerfile 语法，定义了镜像的执行逻辑\n   ```\n   \n   K8s 作为整个集群的基础，虽然最终镜像内的目录结构与其他一致，但其构建过程稍微有所不同。在 CI [https://github.com/labring/cluster-image/blob/faca63809e7a3eae512100a1eb8f9b7384973175/.github/scripts/kubernetes.sh#L35](https://github.com/labring/cluster-image/blob/faca63809e7a3eae512100a1eb8f9b7384973175/.github/scripts/kubernetes.sh#L35) 中，我们可以看到，k8s 镜像其实是合并了 cluster-image 仓库下的多个文件夹，`containerd`，`rootfs` 和 `registry`。这些独立的文件夹中包含有安装对应组件的脚本。\n   \n   Sealos 在挂载一个镜像后，会首先执行 `init.sh` 脚本。例如，以下是 k8s 镜像的脚本中，分别按顺序执行了 `init-containerd.sh` 安装 containerd，`init-shim.sh` 安装 image-cri-shim 和 `init-kube.sh` 安装 kubelet。\n   \n   ```\n   source common.sh\n   REGISTRY_DOMAIN=${1:-sealos.hub}\n   REGISTRY_PORT=${2:-5000}\n   \n   # Install containerd\n   chmod a+x init-containerd.sh\n   bash init-containerd.sh ${REGISTRY_DOMAIN} ${REGISTRY_PORT}\n   \n   if [ $? != 0 ]; then\n      error \"====init containerd failed!====\"\n   fi\n   \n   chmod a+x init-shim.sh\n   bash init-shim.sh\n   \n   if [ $? != 0 ]; then\n      error \"====init image-cri-shim failed!====\"\n   fi\n   \n   chmod a+x init-kube.sh\n   bash init-kube.sh\n   \n   logger \"init containerd rootfs success\"\n   ```\n   \n   在 MountRootfs 这步中，只会执行 `rootfs` 和 `addons` 类型的 `init.sh` 脚本。这也很好理解，因为到目前为止，Sealos 仅仅在每台机器上安装成功了 kubelet，整个 k8s 集群还未可用。\n\n5. Init：初始化 k8s 集群。在这步中，其实也是执行了一系列的子操作。\n   \n   1. Sealos 会从 `ClusterFile` 中加载 `kubeadm` 的配置，然后拷贝到 master0 上。\n   \n   2. 根据 master0 的 hostname 生成证书以及 k8s 配置文件，例如 `admin.conf`，`controller-manager.conf`，`scheduler.conf`，`kubelet.conf`。\n   \n   3. Sealos 将这些配置以及 rootfs 中的静态文件（主要是一些 policy 的配置）拷贝到 master0 上。\n   \n   4. Sealos 通过 link 的方式将 rootfs 中的 registry 链接到宿主机的目录上，然后执行脚本 `init-registry.sh`，启动 registry 守护进程。\n   \n   5. 最后也是最重要的，初始化 master0。首先，将 registry 的域名，api server的域名（IP 为 master0 的 IP）添加到 master0 宿主机上。然后，调用 `kubeadm init` 创建 k8s 集群。最后，将生成的管理员 kubeconfig 拷贝到 `.kube/config`。\n\n6. Join：使用 kubeadm 将其余 master 和 node 加入现有的集群，然后更新 `ClusterFile`。此时，整个 k8s 集群就已经搭建完毕了。\n\n7. RunGuest: 运行所有类型为 `app` 的镜像的 CMD，安装所有应用。\n\n至此一个 k8s 集群以及基于这个集群的所有应用都被安装完毕。\n\n#### reconcileCluster\n\n第二个分支是负责集群的更新，大部分内容与 `initCluster` 都比较类似。执行主要包含了以下几步：\n\n1. ConfirmOverrideApps: 确认是否覆盖已有的应用。\n\n2. PreProcess, RunConfig, MountRootfs, RunGuest: 都与 `initCluster` 类似。\n\n3. PostProcess: 执行一些安装后的操作，但目前似乎并没有进行任何操作。\n\n## 不仅仅如此...\n\n经过上文的介绍，可以看到 Sealos 对于 Kubernetes 生命周期管理有着非常好的抽象，而不只是个简单的安装脚本，你甚至可以扩展其它的 runtime 来支持 k3s k0s 等，而大部分定制化只需要修改集群镜像而不用修改 sealos 的源代码。不仅如此，sealos 还可以让你像使用 PC 操作系统一样用云，各种分布式软件信手拈来，真正让用云的门槛降到足够低。\n\n![](/asset/sealos-ramp-up/1ef54cfebd1c76bc8ecfd9897f9f127107b6e555.png)\n","source":"_posts/sealos-ramp-up.md","raw":"---\ntitle: \"Sealos is All You Need —— 3分钟部署 Kubernetes\"\nmath: true\ndate: 2022-10-30 21:00:00\nupdated: 2022-10-30 21:00:00\n---\n\n![title.png](/asset/sealos-ramp-up/f6ce5cbedc6aa338007cd4633935ad371086271c.png)\n\n## Sealos 是什么？\nKubernetes（K8s）发展至今，已经成为了一个极其复杂的系统。而作为云原生的基石，涌现了一大批辅助工具，帮助用户快速搭建 k8s 集群。而其中，[sealos](https://github.com/labring/sealos) 是一个以 kubernetes 为内核的云操作系统，kuberentes 生命周期管理是 sealos 的一个重要功能，sealos 可以非常方便的安装/升级/伸缩/备份恢复集群等。接下来，让我们通过一个例子来看看 sealos 的强大。\n\n## 如何使用\n\n![Frame 3.png](/asset/sealos-ramp-up/313c52bda7b27b5b64a479e488404bcef56b2669.png)\n\n假设我们想要在 192.168.0.100，192.168.0.101 和 192.168.0.102 这三台机器上部署一主两从的 K8s 集群，那么使用 Sealos 的话，主要输入以下的命令：\n\n```\nsudo sealos run labring/kubernetes:v1.24.0 labring/calico:v3.22.1 \\\n    --masters 192.168.0.100 \\\n    --nodes 192.168.0.101,192.168.0.102 \\\n    --passwd xxx\n```\n\n是的，Sealos 将一个复杂的 K8s 的集群部署简化成了短短一行命令，将部署体验拉到了极致。甚至不需要过多的文档解释，仅凭这一行命令就可以满足大多数普通的部署场景。\n\n## 原理\n\n那么，如此强大的 Sealos 是如何运行的呢？除了刚才演示的 `run` 命令之外，Sealos 还提供了大量提升用户体验的命令，例如 `create` 创建镜像、`reset` 格式化集群等等。由于篇幅有限，本章节就以最核心的 `run` 命令为例，看看 Sealos 在背后替用户完成了哪些自动化的操作。（本文以 Sealos V4.1.0 代码为例）\n\n### Applier\n\n首先，Sealos 会创建一个 `Applier` 结构体，负责了部署集群的核心逻辑。`Applier` 采用了 k8s 的声明式的设计思想，用户声明一个期望的集群状态，而 `Applier` 负责将集群现在的状态转换成用户期望的状态。\n\n```go\ntype Applier struct {\n    ClusterDesired     *v2.Cluster // 用户期望的集群状态\n    ClusterCurrent     *v2.Cluster // 集群当前状态\n    ClusterFile        clusterfile.Interface // 当前集群接口\n    Client             kubernetes.Client\n    CurrentClusterInfo *version.Info\n    RunNewImages       []string // run 命令新增的镜像名称\n}\n```\n\n`clusterfile.Interface` 是一个接口类型，Sealos 中通过 `ClusterFile` 实现了这一接口。因此，`Applier` 结构体中最重要的就是 `Cluster` 和 `ClusterFile` 这两个类型，它们定义了集群的状态和配置。接下来，我们展开介绍一下两者。\n\n#### Cluster\n\n```go\ntype Cluster struct {\n    metav1.TypeMeta   `json:\",inline\"`\n    metav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n    Spec   ClusterSpec   `json:\"spec,omitempty\"`\n    Status ClusterStatus `json:\"status,omitempty\"`\n}\ntype ClusterSpec struct {\n    Image ImageList `json:\"image,omitempty\"`\n    SSH   SSH       `json:\"ssh\"`\n    Hosts []Host    `json:\"hosts,omitempty\"`\n    Env []string `json:\"env,omitempty\"`\n    Command []string `json:\"command,omitempty\"`\n}\ntype ClusterStatus struct {\n    Phase      ClusterPhase       `json:\"phase,omitempty\"`\n    Mounts     []MountImage       `json:\"mounts,omitempty\"`\n    Conditions []ClusterCondition `json:\"conditions,omitempty\" `\n}\n```\n\n`Cluster` 的内容按照 K8s Resource 的格式进行了设计，这非常的 K8s 哈哈。在 `ClusterSpec` 中，定义了一系列用于部署 K8s 集群的参数，例如，镜像、SSH参数、节点等等。\n\n而在 `ClusterStatus` 中，`Phase` 定义了当前集群的状态，`Mounts` 定义了集群使用的镜像，`Conditions` 保存了集群中所发生的一系列事件。\n\n#### ClusterFile\n\n```go\ntype ClusterFile struct {\n    path         string // 保存路径\n    customValues []string\n    customSets   []string\n    customEnvs   []string\n    Cluster      *v2.Cluster // 集群状态\n    Configs      []v2.Config\n    KubeConfig   *runtime.KubeadmConfig // 集群配置\n}\n```\n\n`ClusterFile` 是真正被 `Applier` 操作的对象，以及持久化到文件中的内容。这里包含了所有集群的当前状态信息，同时还包含了 kubeconfig。这里的 kubeconfig 并不是我们平时操作 k8s 时所用的 config 文件，而是一系列用于搭建集群所需的配置项。在使用 `kubeadm` 时，这些配置项往往需要我们手动配置，而 Sealos 在这里会自动帮我们填写并应用于集群中。可以看出，`Cluster` 更像是 `ClusterFile` 的一个实例，记录了集群实时的状态。\n\n### 创建 Applier\n\n创建一个 `Applier` 会经过以下步骤：\n\n1. 判断是否已经存在 `ClusterFile` ，如果存在，那么直接读取，构建出集群状态 `Cluster`。否则，初始化创建一个空的集群状态 `Cluster`。\n\n2. 根据用户本次的参数，更新集群状态 `Cluster` 中的 spec，此时，`Cluster` 即为目标的集群状态。\n\n3. 再次从文件中构建 `ClusterFile`，作为集群当前的状态和对象。\n\n4. 构建 `Applier` 结构体返回。\n\n### Apply\n\n接下来，通过 `Applier.Apply()`，Sealos 开始正式的部署集群，使集群状态向目标靠近。首先，Sealos 会将当前集群的状态置为 `ClusterInProcess`。接下来，根据集群创建或是更新，分别进入两个分支。\n\n#### initCluster\n\n`initCluster` 负责从零开始创建一个集群。函数中会通过 `CreateProcessor` 去部署期望状态的集群。\n\n```go\ntype CreateProcessor struct {\n    ClusterFile     clusterfile.Interface // 当前集群对象\n    ImageManager    types.ImageService // 处理镜像\n    ClusterManager  types.ClusterService // 管理 clusterfile\n    RegistryManager types.RegistryService // 管理镜像 registry\n    Runtime         runtime.Interface // kubeadm 对象\n    Guest           guest.Interface // 基于 sealos 的应用对象\n}\n```\n\n![Slide 16_9 - 1.png](/asset/sealos-ramp-up/c668a66b40dbc788695a9cbb8ec3f76a9897503a.png)\n\n\n\n`CreateProcessor.Execute` 接收期望的集群状态 `ClusterDesired`。接下来会执行一系列 pipeline，正式进入实际的集群部署过程中：\n\n1. Check：检查集群的 host\n\n2. PreProcess：负责集群部署前的镜像预处理操作，在这里就会利用 `CreateProcessor` 中的各个 Manager。\n   \n   1. 拉取镜像\n   \n   2. 检查镜像格式\n   \n   3. 使用 `buildah` 从 OCI 格式的镜像中创建 working container，并将容器挂载到 rootfs 上\n   \n   4. 将容器的 manifest 添加到集群状态中\n\n3. RunConfig：将集群状态中的 working container 导出成 yaml 格式的配置并持久化到宿主机的文件系统中\n\n4. MountRootfs：将挂载的镜像内容按照类别，以 `rootfs`，`addons`，`app` 的顺序分发到每台机器上。\n   \n   这里需要介绍一下 sealos 镜像的一般结构，以最基础的 k8s 镜像为例：\n   \n   ```\n   labring/kubernetes\n   - etc // 配置项\n   - scripts // 脚本\n       - init-containerd.sh\n       - init-kube.sh\n       - init-shim.sh\n       - init-registry.sh\n       - init.sh\n   - Kubefile // dockerfile 语法，定义了镜像的执行逻辑\n   ```\n   \n   K8s 作为整个集群的基础，虽然最终镜像内的目录结构与其他一致，但其构建过程稍微有所不同。在 CI [https://github.com/labring/cluster-image/blob/faca63809e7a3eae512100a1eb8f9b7384973175/.github/scripts/kubernetes.sh#L35](https://github.com/labring/cluster-image/blob/faca63809e7a3eae512100a1eb8f9b7384973175/.github/scripts/kubernetes.sh#L35) 中，我们可以看到，k8s 镜像其实是合并了 cluster-image 仓库下的多个文件夹，`containerd`，`rootfs` 和 `registry`。这些独立的文件夹中包含有安装对应组件的脚本。\n   \n   Sealos 在挂载一个镜像后，会首先执行 `init.sh` 脚本。例如，以下是 k8s 镜像的脚本中，分别按顺序执行了 `init-containerd.sh` 安装 containerd，`init-shim.sh` 安装 image-cri-shim 和 `init-kube.sh` 安装 kubelet。\n   \n   ```\n   source common.sh\n   REGISTRY_DOMAIN=${1:-sealos.hub}\n   REGISTRY_PORT=${2:-5000}\n   \n   # Install containerd\n   chmod a+x init-containerd.sh\n   bash init-containerd.sh ${REGISTRY_DOMAIN} ${REGISTRY_PORT}\n   \n   if [ $? != 0 ]; then\n      error \"====init containerd failed!====\"\n   fi\n   \n   chmod a+x init-shim.sh\n   bash init-shim.sh\n   \n   if [ $? != 0 ]; then\n      error \"====init image-cri-shim failed!====\"\n   fi\n   \n   chmod a+x init-kube.sh\n   bash init-kube.sh\n   \n   logger \"init containerd rootfs success\"\n   ```\n   \n   在 MountRootfs 这步中，只会执行 `rootfs` 和 `addons` 类型的 `init.sh` 脚本。这也很好理解，因为到目前为止，Sealos 仅仅在每台机器上安装成功了 kubelet，整个 k8s 集群还未可用。\n\n5. Init：初始化 k8s 集群。在这步中，其实也是执行了一系列的子操作。\n   \n   1. Sealos 会从 `ClusterFile` 中加载 `kubeadm` 的配置，然后拷贝到 master0 上。\n   \n   2. 根据 master0 的 hostname 生成证书以及 k8s 配置文件，例如 `admin.conf`，`controller-manager.conf`，`scheduler.conf`，`kubelet.conf`。\n   \n   3. Sealos 将这些配置以及 rootfs 中的静态文件（主要是一些 policy 的配置）拷贝到 master0 上。\n   \n   4. Sealos 通过 link 的方式将 rootfs 中的 registry 链接到宿主机的目录上，然后执行脚本 `init-registry.sh`，启动 registry 守护进程。\n   \n   5. 最后也是最重要的，初始化 master0。首先，将 registry 的域名，api server的域名（IP 为 master0 的 IP）添加到 master0 宿主机上。然后，调用 `kubeadm init` 创建 k8s 集群。最后，将生成的管理员 kubeconfig 拷贝到 `.kube/config`。\n\n6. Join：使用 kubeadm 将其余 master 和 node 加入现有的集群，然后更新 `ClusterFile`。此时，整个 k8s 集群就已经搭建完毕了。\n\n7. RunGuest: 运行所有类型为 `app` 的镜像的 CMD，安装所有应用。\n\n至此一个 k8s 集群以及基于这个集群的所有应用都被安装完毕。\n\n#### reconcileCluster\n\n第二个分支是负责集群的更新，大部分内容与 `initCluster` 都比较类似。执行主要包含了以下几步：\n\n1. ConfirmOverrideApps: 确认是否覆盖已有的应用。\n\n2. PreProcess, RunConfig, MountRootfs, RunGuest: 都与 `initCluster` 类似。\n\n3. PostProcess: 执行一些安装后的操作，但目前似乎并没有进行任何操作。\n\n## 不仅仅如此...\n\n经过上文的介绍，可以看到 Sealos 对于 Kubernetes 生命周期管理有着非常好的抽象，而不只是个简单的安装脚本，你甚至可以扩展其它的 runtime 来支持 k3s k0s 等，而大部分定制化只需要修改集群镜像而不用修改 sealos 的源代码。不仅如此，sealos 还可以让你像使用 PC 操作系统一样用云，各种分布式软件信手拈来，真正让用云的门槛降到足够低。\n\n![](/asset/sealos-ramp-up/1ef54cfebd1c76bc8ecfd9897f9f127107b6e555.png)\n","slug":"sealos-ramp-up","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clapldfdk000s18mz47f4d48z","content":"<p><img src=\"/asset/sealos-ramp-up/f6ce5cbedc6aa338007cd4633935ad371086271c.png\" alt=\"title.png\" /></p>\n<h2 id=\"sealos-是什么\"><a class=\"markdownIt-Anchor\" href=\"#sealos-是什么\"></a> Sealos 是什么？</h2>\n<p>Kubernetes（K8s）发展至今，已经成为了一个极其复杂的系统。而作为云原生的基石，涌现了一大批辅助工具，帮助用户快速搭建 k8s 集群。而其中，<a href=\"https://github.com/labring/sealos\">sealos</a> 是一个以 kubernetes 为内核的云操作系统，kuberentes 生命周期管理是 sealos 的一个重要功能，sealos 可以非常方便的安装/升级/伸缩/备份恢复集群等。接下来，让我们通过一个例子来看看 sealos 的强大。</p>\n<h2 id=\"如何使用\"><a class=\"markdownIt-Anchor\" href=\"#如何使用\"></a> 如何使用</h2>\n<p><img src=\"/asset/sealos-ramp-up/313c52bda7b27b5b64a479e488404bcef56b2669.png\" alt=\"Frame 3.png\" /></p>\n<p>假设我们想要在 192.168.0.100，192.168.0.101 和 192.168.0.102 这三台机器上部署一主两从的 K8s 集群，那么使用 Sealos 的话，主要输入以下的命令：</p>\n<pre class=\"highlight\"><code class=\"\">sudo sealos run labring/kubernetes:v1.24.0 labring/calico:v3.22.1 \\\n    --masters 192.168.0.100 \\\n    --nodes 192.168.0.101,192.168.0.102 \\\n    --passwd xxx\n</code></pre>\n<p>是的，Sealos 将一个复杂的 K8s 的集群部署简化成了短短一行命令，将部署体验拉到了极致。甚至不需要过多的文档解释，仅凭这一行命令就可以满足大多数普通的部署场景。</p>\n<h2 id=\"原理\"><a class=\"markdownIt-Anchor\" href=\"#原理\"></a> 原理</h2>\n<p>那么，如此强大的 Sealos 是如何运行的呢？除了刚才演示的 <code>run</code> 命令之外，Sealos 还提供了大量提升用户体验的命令，例如 <code>create</code> 创建镜像、<code>reset</code> 格式化集群等等。由于篇幅有限，本章节就以最核心的 <code>run</code> 命令为例，看看 Sealos 在背后替用户完成了哪些自动化的操作。（本文以 Sealos V4.1.0 代码为例）</p>\n<h3 id=\"applier\"><a class=\"markdownIt-Anchor\" href=\"#applier\"></a> Applier</h3>\n<p>首先，Sealos 会创建一个 <code>Applier</code> 结构体，负责了部署集群的核心逻辑。<code>Applier</code> 采用了 k8s 的声明式的设计思想，用户声明一个期望的集群状态，而 <code>Applier</code> 负责将集群现在的状态转换成用户期望的状态。</p>\n<pre class=\"highlight\"><code class=\"go\"><span class=\"hljs-keyword\">type</span> Applier <span class=\"hljs-keyword\">struct</span> &#123;\n    ClusterDesired     *v2.Cluster <span class=\"hljs-comment\">// 用户期望的集群状态</span>\n    ClusterCurrent     *v2.Cluster <span class=\"hljs-comment\">// 集群当前状态</span>\n    ClusterFile        clusterfile.Interface <span class=\"hljs-comment\">// 当前集群接口</span>\n    Client             kubernetes.Client\n    CurrentClusterInfo *version.Info\n    RunNewImages       []<span class=\"hljs-type\">string</span> <span class=\"hljs-comment\">// run 命令新增的镜像名称</span>\n&#125;\n</code></pre>\n<p><code>clusterfile.Interface</code> 是一个接口类型，Sealos 中通过 <code>ClusterFile</code> 实现了这一接口。因此，<code>Applier</code> 结构体中最重要的就是 <code>Cluster</code> 和 <code>ClusterFile</code> 这两个类型，它们定义了集群的状态和配置。接下来，我们展开介绍一下两者。</p>\n<h4 id=\"cluster\"><a class=\"markdownIt-Anchor\" href=\"#cluster\"></a> Cluster</h4>\n<pre class=\"highlight\"><code class=\"go\"><span class=\"hljs-keyword\">type</span> Cluster <span class=\"hljs-keyword\">struct</span> &#123;\n    metav1.TypeMeta   <span class=\"hljs-string\">`json:&quot;,inline&quot;`</span>\n    metav1.ObjectMeta <span class=\"hljs-string\">`json:&quot;metadata,omitempty&quot;`</span>\n\n    Spec   ClusterSpec   <span class=\"hljs-string\">`json:&quot;spec,omitempty&quot;`</span>\n    Status ClusterStatus <span class=\"hljs-string\">`json:&quot;status,omitempty&quot;`</span>\n&#125;\n<span class=\"hljs-keyword\">type</span> ClusterSpec <span class=\"hljs-keyword\">struct</span> &#123;\n    Image ImageList <span class=\"hljs-string\">`json:&quot;image,omitempty&quot;`</span>\n    SSH   SSH       <span class=\"hljs-string\">`json:&quot;ssh&quot;`</span>\n    Hosts []Host    <span class=\"hljs-string\">`json:&quot;hosts,omitempty&quot;`</span>\n    Env []<span class=\"hljs-type\">string</span> <span class=\"hljs-string\">`json:&quot;env,omitempty&quot;`</span>\n    Command []<span class=\"hljs-type\">string</span> <span class=\"hljs-string\">`json:&quot;command,omitempty&quot;`</span>\n&#125;\n<span class=\"hljs-keyword\">type</span> ClusterStatus <span class=\"hljs-keyword\">struct</span> &#123;\n    Phase      ClusterPhase       <span class=\"hljs-string\">`json:&quot;phase,omitempty&quot;`</span>\n    Mounts     []MountImage       <span class=\"hljs-string\">`json:&quot;mounts,omitempty&quot;`</span>\n    Conditions []ClusterCondition <span class=\"hljs-string\">`json:&quot;conditions,omitempty&quot; `</span>\n&#125;\n</code></pre>\n<p><code>Cluster</code> 的内容按照 K8s Resource 的格式进行了设计，这非常的 K8s 哈哈。在 <code>ClusterSpec</code> 中，定义了一系列用于部署 K8s 集群的参数，例如，镜像、SSH参数、节点等等。</p>\n<p>而在 <code>ClusterStatus</code> 中，<code>Phase</code> 定义了当前集群的状态，<code>Mounts</code> 定义了集群使用的镜像，<code>Conditions</code> 保存了集群中所发生的一系列事件。</p>\n<h4 id=\"clusterfile\"><a class=\"markdownIt-Anchor\" href=\"#clusterfile\"></a> ClusterFile</h4>\n<pre class=\"highlight\"><code class=\"go\"><span class=\"hljs-keyword\">type</span> ClusterFile <span class=\"hljs-keyword\">struct</span> &#123;\n    path         <span class=\"hljs-type\">string</span> <span class=\"hljs-comment\">// 保存路径</span>\n    customValues []<span class=\"hljs-type\">string</span>\n    customSets   []<span class=\"hljs-type\">string</span>\n    customEnvs   []<span class=\"hljs-type\">string</span>\n    Cluster      *v2.Cluster <span class=\"hljs-comment\">// 集群状态</span>\n    Configs      []v2.Config\n    KubeConfig   *runtime.KubeadmConfig <span class=\"hljs-comment\">// 集群配置</span>\n&#125;\n</code></pre>\n<p><code>ClusterFile</code> 是真正被 <code>Applier</code> 操作的对象，以及持久化到文件中的内容。这里包含了所有集群的当前状态信息，同时还包含了 kubeconfig。这里的 kubeconfig 并不是我们平时操作 k8s 时所用的 config 文件，而是一系列用于搭建集群所需的配置项。在使用 <code>kubeadm</code> 时，这些配置项往往需要我们手动配置，而 Sealos 在这里会自动帮我们填写并应用于集群中。可以看出，<code>Cluster</code> 更像是 <code>ClusterFile</code> 的一个实例，记录了集群实时的状态。</p>\n<h3 id=\"创建-applier\"><a class=\"markdownIt-Anchor\" href=\"#创建-applier\"></a> 创建 Applier</h3>\n<p>创建一个 <code>Applier</code> 会经过以下步骤：</p>\n<ol>\n<li>\n<p>判断是否已经存在 <code>ClusterFile</code> ，如果存在，那么直接读取，构建出集群状态 <code>Cluster</code>。否则，初始化创建一个空的集群状态 <code>Cluster</code>。</p>\n</li>\n<li>\n<p>根据用户本次的参数，更新集群状态 <code>Cluster</code> 中的 spec，此时，<code>Cluster</code> 即为目标的集群状态。</p>\n</li>\n<li>\n<p>再次从文件中构建 <code>ClusterFile</code>，作为集群当前的状态和对象。</p>\n</li>\n<li>\n<p>构建 <code>Applier</code> 结构体返回。</p>\n</li>\n</ol>\n<h3 id=\"apply\"><a class=\"markdownIt-Anchor\" href=\"#apply\"></a> Apply</h3>\n<p>接下来，通过 <code>Applier.Apply()</code>，Sealos 开始正式的部署集群，使集群状态向目标靠近。首先，Sealos 会将当前集群的状态置为 <code>ClusterInProcess</code>。接下来，根据集群创建或是更新，分别进入两个分支。</p>\n<h4 id=\"initcluster\"><a class=\"markdownIt-Anchor\" href=\"#initcluster\"></a> initCluster</h4>\n<p><code>initCluster</code> 负责从零开始创建一个集群。函数中会通过 <code>CreateProcessor</code> 去部署期望状态的集群。</p>\n<pre class=\"highlight\"><code class=\"go\"><span class=\"hljs-keyword\">type</span> CreateProcessor <span class=\"hljs-keyword\">struct</span> &#123;\n    ClusterFile     clusterfile.Interface <span class=\"hljs-comment\">// 当前集群对象</span>\n    ImageManager    types.ImageService <span class=\"hljs-comment\">// 处理镜像</span>\n    ClusterManager  types.ClusterService <span class=\"hljs-comment\">// 管理 clusterfile</span>\n    RegistryManager types.RegistryService <span class=\"hljs-comment\">// 管理镜像 registry</span>\n    Runtime         runtime.Interface <span class=\"hljs-comment\">// kubeadm 对象</span>\n    Guest           guest.Interface <span class=\"hljs-comment\">// 基于 sealos 的应用对象</span>\n&#125;\n</code></pre>\n<p><img src=\"/asset/sealos-ramp-up/c668a66b40dbc788695a9cbb8ec3f76a9897503a.png\" alt=\"Slide 16_9 - 1.png\" /></p>\n<p><code>CreateProcessor.Execute</code> 接收期望的集群状态 <code>ClusterDesired</code>。接下来会执行一系列 pipeline，正式进入实际的集群部署过程中：</p>\n<ol>\n<li>\n<p>Check：检查集群的 host</p>\n</li>\n<li>\n<p>PreProcess：负责集群部署前的镜像预处理操作，在这里就会利用 <code>CreateProcessor</code> 中的各个 Manager。</p>\n<ol>\n<li>\n<p>拉取镜像</p>\n</li>\n<li>\n<p>检查镜像格式</p>\n</li>\n<li>\n<p>使用 <code>buildah</code> 从 OCI 格式的镜像中创建 working container，并将容器挂载到 rootfs 上</p>\n</li>\n<li>\n<p>将容器的 manifest 添加到集群状态中</p>\n</li>\n</ol>\n</li>\n<li>\n<p>RunConfig：将集群状态中的 working container 导出成 yaml 格式的配置并持久化到宿主机的文件系统中</p>\n</li>\n<li>\n<p>MountRootfs：将挂载的镜像内容按照类别，以 <code>rootfs</code>，<code>addons</code>，<code>app</code> 的顺序分发到每台机器上。</p>\n<p>这里需要介绍一下 sealos 镜像的一般结构，以最基础的 k8s 镜像为例：</p>\n<pre class=\"highlight\"><code class=\"\">labring/kubernetes\n- etc // 配置项\n- scripts // 脚本\n    - init-containerd.sh\n    - init-kube.sh\n    - init-shim.sh\n    - init-registry.sh\n    - init.sh\n- Kubefile // dockerfile 语法，定义了镜像的执行逻辑\n</code></pre>\n<p>K8s 作为整个集群的基础，虽然最终镜像内的目录结构与其他一致，但其构建过程稍微有所不同。在 CI <a href=\"https://github.com/labring/cluster-image/blob/faca63809e7a3eae512100a1eb8f9b7384973175/.github/scripts/kubernetes.sh#L35\">https://github.com/labring/cluster-image/blob/faca63809e7a3eae512100a1eb8f9b7384973175/.github/scripts/kubernetes.sh#L35</a> 中，我们可以看到，k8s 镜像其实是合并了 cluster-image 仓库下的多个文件夹，<code>containerd</code>，<code>rootfs</code> 和 <code>registry</code>。这些独立的文件夹中包含有安装对应组件的脚本。</p>\n<p>Sealos 在挂载一个镜像后，会首先执行 <code>init.sh</code> 脚本。例如，以下是 k8s 镜像的脚本中，分别按顺序执行了 <code>init-containerd.sh</code> 安装 containerd，<code>init-shim.sh</code> 安装 image-cri-shim 和 <code>init-kube.sh</code> 安装 kubelet。</p>\n<pre class=\"highlight\"><code class=\"\">source common.sh\nREGISTRY_DOMAIN=$&#123;1:-sealos.hub&#125;\nREGISTRY_PORT=$&#123;2:-5000&#125;\n\n# Install containerd\nchmod a+x init-containerd.sh\nbash init-containerd.sh $&#123;REGISTRY_DOMAIN&#125; $&#123;REGISTRY_PORT&#125;\n\nif [ $? != 0 ]; then\n   error &quot;====init containerd failed!====&quot;\nfi\n\nchmod a+x init-shim.sh\nbash init-shim.sh\n\nif [ $? != 0 ]; then\n   error &quot;====init image-cri-shim failed!====&quot;\nfi\n\nchmod a+x init-kube.sh\nbash init-kube.sh\n\nlogger &quot;init containerd rootfs success&quot;\n</code></pre>\n<p>在 MountRootfs 这步中，只会执行 <code>rootfs</code> 和 <code>addons</code> 类型的 <code>init.sh</code> 脚本。这也很好理解，因为到目前为止，Sealos 仅仅在每台机器上安装成功了 kubelet，整个 k8s 集群还未可用。</p>\n</li>\n<li>\n<p>Init：初始化 k8s 集群。在这步中，其实也是执行了一系列的子操作。</p>\n<ol>\n<li>\n<p>Sealos 会从 <code>ClusterFile</code> 中加载 <code>kubeadm</code> 的配置，然后拷贝到 master0 上。</p>\n</li>\n<li>\n<p>根据 master0 的 hostname 生成证书以及 k8s 配置文件，例如 <code>admin.conf</code>，<code>controller-manager.conf</code>，<code>scheduler.conf</code>，<code>kubelet.conf</code>。</p>\n</li>\n<li>\n<p>Sealos 将这些配置以及 rootfs 中的静态文件（主要是一些 policy 的配置）拷贝到 master0 上。</p>\n</li>\n<li>\n<p>Sealos 通过 link 的方式将 rootfs 中的 registry 链接到宿主机的目录上，然后执行脚本 <code>init-registry.sh</code>，启动 registry 守护进程。</p>\n</li>\n<li>\n<p>最后也是最重要的，初始化 master0。首先，将 registry 的域名，api server的域名（IP 为 master0 的 IP）添加到 master0 宿主机上。然后，调用 <code>kubeadm init</code> 创建 k8s 集群。最后，将生成的管理员 kubeconfig 拷贝到 <code>.kube/config</code>。</p>\n</li>\n</ol>\n</li>\n<li>\n<p>Join：使用 kubeadm 将其余 master 和 node 加入现有的集群，然后更新 <code>ClusterFile</code>。此时，整个 k8s 集群就已经搭建完毕了。</p>\n</li>\n<li>\n<p>RunGuest: 运行所有类型为 <code>app</code> 的镜像的 CMD，安装所有应用。</p>\n</li>\n</ol>\n<p>至此一个 k8s 集群以及基于这个集群的所有应用都被安装完毕。</p>\n<h4 id=\"reconcilecluster\"><a class=\"markdownIt-Anchor\" href=\"#reconcilecluster\"></a> reconcileCluster</h4>\n<p>第二个分支是负责集群的更新，大部分内容与 <code>initCluster</code> 都比较类似。执行主要包含了以下几步：</p>\n<ol>\n<li>\n<p>ConfirmOverrideApps: 确认是否覆盖已有的应用。</p>\n</li>\n<li>\n<p>PreProcess, RunConfig, MountRootfs, RunGuest: 都与 <code>initCluster</code> 类似。</p>\n</li>\n<li>\n<p>PostProcess: 执行一些安装后的操作，但目前似乎并没有进行任何操作。</p>\n</li>\n</ol>\n<h2 id=\"不仅仅如此\"><a class=\"markdownIt-Anchor\" href=\"#不仅仅如此\"></a> 不仅仅如此…</h2>\n<p>经过上文的介绍，可以看到 Sealos 对于 Kubernetes 生命周期管理有着非常好的抽象，而不只是个简单的安装脚本，你甚至可以扩展其它的 runtime 来支持 k3s k0s 等，而大部分定制化只需要修改集群镜像而不用修改 sealos 的源代码。不仅如此，sealos 还可以让你像使用 PC 操作系统一样用云，各种分布式软件信手拈来，真正让用云的门槛降到足够低。</p>\n<p><img src=\"/asset/sealos-ramp-up/1ef54cfebd1c76bc8ecfd9897f9f127107b6e555.png\" alt=\"\" /></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"/asset/sealos-ramp-up/f6ce5cbedc6aa338007cd4633935ad371086271c.png\" alt=\"title.png\" /></p>\n<h2 id=\"sealos-是什么\"><a class=\"markdownIt-Anchor\" href=\"#sealos-是什么\"></a> Sealos 是什么？</h2>\n<p>Kubernetes（K8s）发展至今，已经成为了一个极其复杂的系统。而作为云原生的基石，涌现了一大批辅助工具，帮助用户快速搭建 k8s 集群。而其中，<a href=\"https://github.com/labring/sealos\">sealos</a> 是一个以 kubernetes 为内核的云操作系统，kuberentes 生命周期管理是 sealos 的一个重要功能，sealos 可以非常方便的安装/升级/伸缩/备份恢复集群等。接下来，让我们通过一个例子来看看 sealos 的强大。</p>\n<h2 id=\"如何使用\"><a class=\"markdownIt-Anchor\" href=\"#如何使用\"></a> 如何使用</h2>\n<p><img src=\"/asset/sealos-ramp-up/313c52bda7b27b5b64a479e488404bcef56b2669.png\" alt=\"Frame 3.png\" /></p>\n<p>假设我们想要在 192.168.0.100，192.168.0.101 和 192.168.0.102 这三台机器上部署一主两从的 K8s 集群，那么使用 Sealos 的话，主要输入以下的命令：</p>\n<pre class=\"highlight\"><code class=\"\">sudo sealos run labring/kubernetes:v1.24.0 labring/calico:v3.22.1 \\\n    --masters 192.168.0.100 \\\n    --nodes 192.168.0.101,192.168.0.102 \\\n    --passwd xxx\n</code></pre>\n<p>是的，Sealos 将一个复杂的 K8s 的集群部署简化成了短短一行命令，将部署体验拉到了极致。甚至不需要过多的文档解释，仅凭这一行命令就可以满足大多数普通的部署场景。</p>\n<h2 id=\"原理\"><a class=\"markdownIt-Anchor\" href=\"#原理\"></a> 原理</h2>\n<p>那么，如此强大的 Sealos 是如何运行的呢？除了刚才演示的 <code>run</code> 命令之外，Sealos 还提供了大量提升用户体验的命令，例如 <code>create</code> 创建镜像、<code>reset</code> 格式化集群等等。由于篇幅有限，本章节就以最核心的 <code>run</code> 命令为例，看看 Sealos 在背后替用户完成了哪些自动化的操作。（本文以 Sealos V4.1.0 代码为例）</p>\n<h3 id=\"applier\"><a class=\"markdownIt-Anchor\" href=\"#applier\"></a> Applier</h3>\n<p>首先，Sealos 会创建一个 <code>Applier</code> 结构体，负责了部署集群的核心逻辑。<code>Applier</code> 采用了 k8s 的声明式的设计思想，用户声明一个期望的集群状态，而 <code>Applier</code> 负责将集群现在的状态转换成用户期望的状态。</p>\n<pre class=\"highlight\"><code class=\"go\"><span class=\"hljs-keyword\">type</span> Applier <span class=\"hljs-keyword\">struct</span> &#123;\n    ClusterDesired     *v2.Cluster <span class=\"hljs-comment\">// 用户期望的集群状态</span>\n    ClusterCurrent     *v2.Cluster <span class=\"hljs-comment\">// 集群当前状态</span>\n    ClusterFile        clusterfile.Interface <span class=\"hljs-comment\">// 当前集群接口</span>\n    Client             kubernetes.Client\n    CurrentClusterInfo *version.Info\n    RunNewImages       []<span class=\"hljs-type\">string</span> <span class=\"hljs-comment\">// run 命令新增的镜像名称</span>\n&#125;\n</code></pre>\n<p><code>clusterfile.Interface</code> 是一个接口类型，Sealos 中通过 <code>ClusterFile</code> 实现了这一接口。因此，<code>Applier</code> 结构体中最重要的就是 <code>Cluster</code> 和 <code>ClusterFile</code> 这两个类型，它们定义了集群的状态和配置。接下来，我们展开介绍一下两者。</p>\n<h4 id=\"cluster\"><a class=\"markdownIt-Anchor\" href=\"#cluster\"></a> Cluster</h4>\n<pre class=\"highlight\"><code class=\"go\"><span class=\"hljs-keyword\">type</span> Cluster <span class=\"hljs-keyword\">struct</span> &#123;\n    metav1.TypeMeta   <span class=\"hljs-string\">`json:&quot;,inline&quot;`</span>\n    metav1.ObjectMeta <span class=\"hljs-string\">`json:&quot;metadata,omitempty&quot;`</span>\n\n    Spec   ClusterSpec   <span class=\"hljs-string\">`json:&quot;spec,omitempty&quot;`</span>\n    Status ClusterStatus <span class=\"hljs-string\">`json:&quot;status,omitempty&quot;`</span>\n&#125;\n<span class=\"hljs-keyword\">type</span> ClusterSpec <span class=\"hljs-keyword\">struct</span> &#123;\n    Image ImageList <span class=\"hljs-string\">`json:&quot;image,omitempty&quot;`</span>\n    SSH   SSH       <span class=\"hljs-string\">`json:&quot;ssh&quot;`</span>\n    Hosts []Host    <span class=\"hljs-string\">`json:&quot;hosts,omitempty&quot;`</span>\n    Env []<span class=\"hljs-type\">string</span> <span class=\"hljs-string\">`json:&quot;env,omitempty&quot;`</span>\n    Command []<span class=\"hljs-type\">string</span> <span class=\"hljs-string\">`json:&quot;command,omitempty&quot;`</span>\n&#125;\n<span class=\"hljs-keyword\">type</span> ClusterStatus <span class=\"hljs-keyword\">struct</span> &#123;\n    Phase      ClusterPhase       <span class=\"hljs-string\">`json:&quot;phase,omitempty&quot;`</span>\n    Mounts     []MountImage       <span class=\"hljs-string\">`json:&quot;mounts,omitempty&quot;`</span>\n    Conditions []ClusterCondition <span class=\"hljs-string\">`json:&quot;conditions,omitempty&quot; `</span>\n&#125;\n</code></pre>\n<p><code>Cluster</code> 的内容按照 K8s Resource 的格式进行了设计，这非常的 K8s 哈哈。在 <code>ClusterSpec</code> 中，定义了一系列用于部署 K8s 集群的参数，例如，镜像、SSH参数、节点等等。</p>\n<p>而在 <code>ClusterStatus</code> 中，<code>Phase</code> 定义了当前集群的状态，<code>Mounts</code> 定义了集群使用的镜像，<code>Conditions</code> 保存了集群中所发生的一系列事件。</p>\n<h4 id=\"clusterfile\"><a class=\"markdownIt-Anchor\" href=\"#clusterfile\"></a> ClusterFile</h4>\n<pre class=\"highlight\"><code class=\"go\"><span class=\"hljs-keyword\">type</span> ClusterFile <span class=\"hljs-keyword\">struct</span> &#123;\n    path         <span class=\"hljs-type\">string</span> <span class=\"hljs-comment\">// 保存路径</span>\n    customValues []<span class=\"hljs-type\">string</span>\n    customSets   []<span class=\"hljs-type\">string</span>\n    customEnvs   []<span class=\"hljs-type\">string</span>\n    Cluster      *v2.Cluster <span class=\"hljs-comment\">// 集群状态</span>\n    Configs      []v2.Config\n    KubeConfig   *runtime.KubeadmConfig <span class=\"hljs-comment\">// 集群配置</span>\n&#125;\n</code></pre>\n<p><code>ClusterFile</code> 是真正被 <code>Applier</code> 操作的对象，以及持久化到文件中的内容。这里包含了所有集群的当前状态信息，同时还包含了 kubeconfig。这里的 kubeconfig 并不是我们平时操作 k8s 时所用的 config 文件，而是一系列用于搭建集群所需的配置项。在使用 <code>kubeadm</code> 时，这些配置项往往需要我们手动配置，而 Sealos 在这里会自动帮我们填写并应用于集群中。可以看出，<code>Cluster</code> 更像是 <code>ClusterFile</code> 的一个实例，记录了集群实时的状态。</p>\n<h3 id=\"创建-applier\"><a class=\"markdownIt-Anchor\" href=\"#创建-applier\"></a> 创建 Applier</h3>\n<p>创建一个 <code>Applier</code> 会经过以下步骤：</p>\n<ol>\n<li>\n<p>判断是否已经存在 <code>ClusterFile</code> ，如果存在，那么直接读取，构建出集群状态 <code>Cluster</code>。否则，初始化创建一个空的集群状态 <code>Cluster</code>。</p>\n</li>\n<li>\n<p>根据用户本次的参数，更新集群状态 <code>Cluster</code> 中的 spec，此时，<code>Cluster</code> 即为目标的集群状态。</p>\n</li>\n<li>\n<p>再次从文件中构建 <code>ClusterFile</code>，作为集群当前的状态和对象。</p>\n</li>\n<li>\n<p>构建 <code>Applier</code> 结构体返回。</p>\n</li>\n</ol>\n<h3 id=\"apply\"><a class=\"markdownIt-Anchor\" href=\"#apply\"></a> Apply</h3>\n<p>接下来，通过 <code>Applier.Apply()</code>，Sealos 开始正式的部署集群，使集群状态向目标靠近。首先，Sealos 会将当前集群的状态置为 <code>ClusterInProcess</code>。接下来，根据集群创建或是更新，分别进入两个分支。</p>\n<h4 id=\"initcluster\"><a class=\"markdownIt-Anchor\" href=\"#initcluster\"></a> initCluster</h4>\n<p><code>initCluster</code> 负责从零开始创建一个集群。函数中会通过 <code>CreateProcessor</code> 去部署期望状态的集群。</p>\n<pre class=\"highlight\"><code class=\"go\"><span class=\"hljs-keyword\">type</span> CreateProcessor <span class=\"hljs-keyword\">struct</span> &#123;\n    ClusterFile     clusterfile.Interface <span class=\"hljs-comment\">// 当前集群对象</span>\n    ImageManager    types.ImageService <span class=\"hljs-comment\">// 处理镜像</span>\n    ClusterManager  types.ClusterService <span class=\"hljs-comment\">// 管理 clusterfile</span>\n    RegistryManager types.RegistryService <span class=\"hljs-comment\">// 管理镜像 registry</span>\n    Runtime         runtime.Interface <span class=\"hljs-comment\">// kubeadm 对象</span>\n    Guest           guest.Interface <span class=\"hljs-comment\">// 基于 sealos 的应用对象</span>\n&#125;\n</code></pre>\n<p><img src=\"/asset/sealos-ramp-up/c668a66b40dbc788695a9cbb8ec3f76a9897503a.png\" alt=\"Slide 16_9 - 1.png\" /></p>\n<p><code>CreateProcessor.Execute</code> 接收期望的集群状态 <code>ClusterDesired</code>。接下来会执行一系列 pipeline，正式进入实际的集群部署过程中：</p>\n<ol>\n<li>\n<p>Check：检查集群的 host</p>\n</li>\n<li>\n<p>PreProcess：负责集群部署前的镜像预处理操作，在这里就会利用 <code>CreateProcessor</code> 中的各个 Manager。</p>\n<ol>\n<li>\n<p>拉取镜像</p>\n</li>\n<li>\n<p>检查镜像格式</p>\n</li>\n<li>\n<p>使用 <code>buildah</code> 从 OCI 格式的镜像中创建 working container，并将容器挂载到 rootfs 上</p>\n</li>\n<li>\n<p>将容器的 manifest 添加到集群状态中</p>\n</li>\n</ol>\n</li>\n<li>\n<p>RunConfig：将集群状态中的 working container 导出成 yaml 格式的配置并持久化到宿主机的文件系统中</p>\n</li>\n<li>\n<p>MountRootfs：将挂载的镜像内容按照类别，以 <code>rootfs</code>，<code>addons</code>，<code>app</code> 的顺序分发到每台机器上。</p>\n<p>这里需要介绍一下 sealos 镜像的一般结构，以最基础的 k8s 镜像为例：</p>\n<pre class=\"highlight\"><code class=\"\">labring/kubernetes\n- etc // 配置项\n- scripts // 脚本\n    - init-containerd.sh\n    - init-kube.sh\n    - init-shim.sh\n    - init-registry.sh\n    - init.sh\n- Kubefile // dockerfile 语法，定义了镜像的执行逻辑\n</code></pre>\n<p>K8s 作为整个集群的基础，虽然最终镜像内的目录结构与其他一致，但其构建过程稍微有所不同。在 CI <a href=\"https://github.com/labring/cluster-image/blob/faca63809e7a3eae512100a1eb8f9b7384973175/.github/scripts/kubernetes.sh#L35\">https://github.com/labring/cluster-image/blob/faca63809e7a3eae512100a1eb8f9b7384973175/.github/scripts/kubernetes.sh#L35</a> 中，我们可以看到，k8s 镜像其实是合并了 cluster-image 仓库下的多个文件夹，<code>containerd</code>，<code>rootfs</code> 和 <code>registry</code>。这些独立的文件夹中包含有安装对应组件的脚本。</p>\n<p>Sealos 在挂载一个镜像后，会首先执行 <code>init.sh</code> 脚本。例如，以下是 k8s 镜像的脚本中，分别按顺序执行了 <code>init-containerd.sh</code> 安装 containerd，<code>init-shim.sh</code> 安装 image-cri-shim 和 <code>init-kube.sh</code> 安装 kubelet。</p>\n<pre class=\"highlight\"><code class=\"\">source common.sh\nREGISTRY_DOMAIN=$&#123;1:-sealos.hub&#125;\nREGISTRY_PORT=$&#123;2:-5000&#125;\n\n# Install containerd\nchmod a+x init-containerd.sh\nbash init-containerd.sh $&#123;REGISTRY_DOMAIN&#125; $&#123;REGISTRY_PORT&#125;\n\nif [ $? != 0 ]; then\n   error &quot;====init containerd failed!====&quot;\nfi\n\nchmod a+x init-shim.sh\nbash init-shim.sh\n\nif [ $? != 0 ]; then\n   error &quot;====init image-cri-shim failed!====&quot;\nfi\n\nchmod a+x init-kube.sh\nbash init-kube.sh\n\nlogger &quot;init containerd rootfs success&quot;\n</code></pre>\n<p>在 MountRootfs 这步中，只会执行 <code>rootfs</code> 和 <code>addons</code> 类型的 <code>init.sh</code> 脚本。这也很好理解，因为到目前为止，Sealos 仅仅在每台机器上安装成功了 kubelet，整个 k8s 集群还未可用。</p>\n</li>\n<li>\n<p>Init：初始化 k8s 集群。在这步中，其实也是执行了一系列的子操作。</p>\n<ol>\n<li>\n<p>Sealos 会从 <code>ClusterFile</code> 中加载 <code>kubeadm</code> 的配置，然后拷贝到 master0 上。</p>\n</li>\n<li>\n<p>根据 master0 的 hostname 生成证书以及 k8s 配置文件，例如 <code>admin.conf</code>，<code>controller-manager.conf</code>，<code>scheduler.conf</code>，<code>kubelet.conf</code>。</p>\n</li>\n<li>\n<p>Sealos 将这些配置以及 rootfs 中的静态文件（主要是一些 policy 的配置）拷贝到 master0 上。</p>\n</li>\n<li>\n<p>Sealos 通过 link 的方式将 rootfs 中的 registry 链接到宿主机的目录上，然后执行脚本 <code>init-registry.sh</code>，启动 registry 守护进程。</p>\n</li>\n<li>\n<p>最后也是最重要的，初始化 master0。首先，将 registry 的域名，api server的域名（IP 为 master0 的 IP）添加到 master0 宿主机上。然后，调用 <code>kubeadm init</code> 创建 k8s 集群。最后，将生成的管理员 kubeconfig 拷贝到 <code>.kube/config</code>。</p>\n</li>\n</ol>\n</li>\n<li>\n<p>Join：使用 kubeadm 将其余 master 和 node 加入现有的集群，然后更新 <code>ClusterFile</code>。此时，整个 k8s 集群就已经搭建完毕了。</p>\n</li>\n<li>\n<p>RunGuest: 运行所有类型为 <code>app</code> 的镜像的 CMD，安装所有应用。</p>\n</li>\n</ol>\n<p>至此一个 k8s 集群以及基于这个集群的所有应用都被安装完毕。</p>\n<h4 id=\"reconcilecluster\"><a class=\"markdownIt-Anchor\" href=\"#reconcilecluster\"></a> reconcileCluster</h4>\n<p>第二个分支是负责集群的更新，大部分内容与 <code>initCluster</code> 都比较类似。执行主要包含了以下几步：</p>\n<ol>\n<li>\n<p>ConfirmOverrideApps: 确认是否覆盖已有的应用。</p>\n</li>\n<li>\n<p>PreProcess, RunConfig, MountRootfs, RunGuest: 都与 <code>initCluster</code> 类似。</p>\n</li>\n<li>\n<p>PostProcess: 执行一些安装后的操作，但目前似乎并没有进行任何操作。</p>\n</li>\n</ol>\n<h2 id=\"不仅仅如此\"><a class=\"markdownIt-Anchor\" href=\"#不仅仅如此\"></a> 不仅仅如此…</h2>\n<p>经过上文的介绍，可以看到 Sealos 对于 Kubernetes 生命周期管理有着非常好的抽象，而不只是个简单的安装脚本，你甚至可以扩展其它的 runtime 来支持 k3s k0s 等，而大部分定制化只需要修改集群镜像而不用修改 sealos 的源代码。不仅如此，sealos 还可以让你像使用 PC 操作系统一样用云，各种分布式软件信手拈来，真正让用云的门槛降到足够低。</p>\n<p><img src=\"/asset/sealos-ramp-up/1ef54cfebd1c76bc8ecfd9897f9f127107b6e555.png\" alt=\"\" /></p>\n"},{"title":"[CodeRead] TiDB Lightning","date":"2021-03-19T18:48:44.000Z","updated":"2021-03-19T18:48:44.000Z","_content":"\nTiDB Lightning 是一个将数据导入到 TiDB 中的工具，使用 Go 编写。支持 `Local`, `Importer`, `TiDB` 三种导入模式。在 TiDB 的官网中，对其[原理]()有着详细的介绍。\n\n本文从代码的角度，带领大家走过一个数据导入的过程，所以只关注一些逻辑上重要的步骤，而一些其他的细节可能不会涉及到。\n<!-- more -->\n\n![](/asset/tidb-lightning/1.png)\n首先，进入 main 函数，程序调用了两个主要的启动函数：`GoServer` 和 `RunServer`。\n\n## GoServer\n\n[https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96](https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96)\n\nGoServer 启动了一个 Api Server，有着以下这些 endpoint\n- /web\n- /metrics\n- /debug\n- /tasks\n- /progress\n- /pause\n- /resume\n- /loglevel\n\nApi Server 为整个 lightning 提供了控制，监控和查看进度等功能。是用户与 lightning 交互的入口。而当用户通过 `POST /tasks` 提交了一个数据导入任务时，Api Server 就会向队列中添加一个任务，而同样在监听着队列的 RunServer 就会开始执行。\n\n## RunServer\n\n[https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194](https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194)\n\nRunServer 是真正执行导入的地方。一个 for 循环一直从队列中获取任务配置。在获取到一个任务的配置后，开始正式执行导入任务。\n\n### RegisterMySQL\n\n这个函数的命名有点迷惑，但所幸有注释。RegisterMySQL 同时包含了向 gomysql driver 配置 TLS 和解除配置的两个作用。在运行的一开始，注册 TiDB 的 TLS 配置，这样直接可以使用 `sql.open()` 连接 TiDB。在执行导入任务结束后，通过将 `CAPath` 置为空，删除 TLS 配置。\n\n### Glue\n\nGlue 将数据和导入粘合在一起。这里在创建 Glue 时，就将 TiDB 设为了导入模式。\n\n### MyDumper\n\n确定好目标数据库也就是 backend 后，我们就可以看看数据导入的 frontend 了。`MyDumper` 是一款由 PingCAP 基于社区版，为 TiDB 定制化开发的数据导出工具。但是目前似乎已经不推荐使用了，推荐改用 `dumpling` 了。\n\n在这里创建 MyDumper 时，就会在 `setup` 中从 SQL 文件中。 这里会对表按从小到大进行排序，让小表之后先进行导入，这样可以避免大表导入时阻塞小表释放 index worker。\n\n```go\ntype mdLoaderSetup struct {\n\tloader        *MDLoader\n\tdbSchemas     []FileInfo\n\ttableSchemas  []FileInfo\n\tviewSchemas   []FileInfo\n\ttableDatas    []FileInfo\n\tdbIndexMap    map[string]int\n\ttableIndexMap map[filter.Table]int\n}\n```\n\n### Check\n\n初始化并配置好 MyDumper 后，在正式开跑前，程序还需要进行检查。这里检查了两项：1. `CheckSystemRequirement` 系统要求是否满足，lightning 对于内存要求似乎还挺高。2. `CheckSchemaConflict` 检查目标数据库的 Schema 是不是有冲突。\n\n## RestoreController\n\nlightning 的导入过程由 `RestoreController` 进行控制。`Run` 中的 `opts` 定义了导入过程所有的操作。\n\n### CheckRequirements\n\n通过多态实现，三种 backend 执行不同的检查。\n\n### SetGlobalVariables\n\n在 Server 模式下，目标数据库的 `Collation` 设置都可能不同，所以每次执行任务都需要设置一下。\n\n### RestoreSchema\n\n从这里，lightning 真正开始导入数据。首先，导入 Schema。Schema 包含 `database`, `table`, `view` 三个部分，分别对应着之前从 MyDumper 中导出的三个部分。Lightning 使用 Glue 中封装的目标数据库连接，执行 SQL，导入 Schema 信息。\n\n这里 lightning 使用了一种非常 Go 风格的实现方法。先创建出多个 goroutine，每个 goroutine 都监听着同一个 channel。之后再不断向 channel 中加入要执行的 SQL 任务，从而实现一个类似于线程池的功能。\n\n在导入 Schema 成功后，RestoreSchema 还会负责初始化 Checkpoint，并创建一个 goroutine，用来监听 Checkpoint 的变化，将多个 Checkpoint 合并到一起。\n\n最后，RestoreSchema 还会根据 Schema 中包含的元信息，对之后导入数据时划分的 chunk 数量进行一个估计。\n\n### RestoreTables\n\nRestoreTables 负责从源数据库中导出数据到目标数据库中。\n\n首先，在 `populateChunks` 函数中，使用 MyDumper 将数据划分成多个 chunk，接着将 chunk 加入到其对应的数据 engine 中。然后添加一个索引 engine 的 Checkpoint。\n\n然后，通过 `InsertEngineCheckpoint` 创建每张表的 Checkpoint。\n\n然后，在 `restoreEngines` 中，先将源数据库的数据从导出的文件，并发地转化成 KV 格式的 engine，然后再导入到目标数据库中。关于 engine 的状态机模型，可以参考我的另一篇[文章](https://blog.abingcbc.cn/2021/03/17/tikv-importer)中的总结。这个函数中包含了 lightning 最核心的逻辑，所以比较复杂，大致可以分为以下几个过程：\n\n1. 在 chunkRestore 的 `restore` 中，在 `encodeLoop` 中，利用 MyDumper 的 parser，将数据从不同的导出格式统一转换成 KV 格式，插入到 engine 中。此时，所有的 Checkpoint，包括 data 和 index，都已经写入完成。\n2. 接着，根据 engine 的状态机，关闭 engine 才能开始导入，所以我们现在需要将 engine 的状态置为 close。\n3. 关闭后，开始调用 backend 导入 engine。在所有的 data engine 导入完成后，开始导入 index engine。\n\n这里，又使用了一种非常 Go 风格的方式实现线程池的功能。利用 channel 的缓冲特性，向其中添加 worker，使用时取走，使用完再放回到 channel 中。\n\n最后，对于某些需要执行 post process 的任务，执行一下相应的任务。\n\n### FullCompact\n\n调用 TiKV 的接口，对所有新导入的数据进行压缩。但是由于之前导入数据的时候，会根据是否存在压缩任务，去尝试执行 level-1 的压缩，所以进行全量压缩的时候，需要等待之前启动的 level-1 压缩任务结束。\n\n### SwitchToNormalMode\n\n将 TiKV 切换成正常模式\n\n### CleanCheckpoints\n\n所有的导入任务都结束了，此时 checkpoint 已经无用了。所以，根据用户配置是否删除，处理 checkpoint。","source":"_posts/tidb-lightning.md","raw":"---\ntitle: \"[CodeRead] TiDB Lightning\"\ndate: 2021-03-19 18:48:44\nupdated: 2021-03-19 18:48:44\n---\n\nTiDB Lightning 是一个将数据导入到 TiDB 中的工具，使用 Go 编写。支持 `Local`, `Importer`, `TiDB` 三种导入模式。在 TiDB 的官网中，对其[原理]()有着详细的介绍。\n\n本文从代码的角度，带领大家走过一个数据导入的过程，所以只关注一些逻辑上重要的步骤，而一些其他的细节可能不会涉及到。\n<!-- more -->\n\n![](/asset/tidb-lightning/1.png)\n首先，进入 main 函数，程序调用了两个主要的启动函数：`GoServer` 和 `RunServer`。\n\n## GoServer\n\n[https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96](https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96)\n\nGoServer 启动了一个 Api Server，有着以下这些 endpoint\n- /web\n- /metrics\n- /debug\n- /tasks\n- /progress\n- /pause\n- /resume\n- /loglevel\n\nApi Server 为整个 lightning 提供了控制，监控和查看进度等功能。是用户与 lightning 交互的入口。而当用户通过 `POST /tasks` 提交了一个数据导入任务时，Api Server 就会向队列中添加一个任务，而同样在监听着队列的 RunServer 就会开始执行。\n\n## RunServer\n\n[https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194](https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194)\n\nRunServer 是真正执行导入的地方。一个 for 循环一直从队列中获取任务配置。在获取到一个任务的配置后，开始正式执行导入任务。\n\n### RegisterMySQL\n\n这个函数的命名有点迷惑，但所幸有注释。RegisterMySQL 同时包含了向 gomysql driver 配置 TLS 和解除配置的两个作用。在运行的一开始，注册 TiDB 的 TLS 配置，这样直接可以使用 `sql.open()` 连接 TiDB。在执行导入任务结束后，通过将 `CAPath` 置为空，删除 TLS 配置。\n\n### Glue\n\nGlue 将数据和导入粘合在一起。这里在创建 Glue 时，就将 TiDB 设为了导入模式。\n\n### MyDumper\n\n确定好目标数据库也就是 backend 后，我们就可以看看数据导入的 frontend 了。`MyDumper` 是一款由 PingCAP 基于社区版，为 TiDB 定制化开发的数据导出工具。但是目前似乎已经不推荐使用了，推荐改用 `dumpling` 了。\n\n在这里创建 MyDumper 时，就会在 `setup` 中从 SQL 文件中。 这里会对表按从小到大进行排序，让小表之后先进行导入，这样可以避免大表导入时阻塞小表释放 index worker。\n\n```go\ntype mdLoaderSetup struct {\n\tloader        *MDLoader\n\tdbSchemas     []FileInfo\n\ttableSchemas  []FileInfo\n\tviewSchemas   []FileInfo\n\ttableDatas    []FileInfo\n\tdbIndexMap    map[string]int\n\ttableIndexMap map[filter.Table]int\n}\n```\n\n### Check\n\n初始化并配置好 MyDumper 后，在正式开跑前，程序还需要进行检查。这里检查了两项：1. `CheckSystemRequirement` 系统要求是否满足，lightning 对于内存要求似乎还挺高。2. `CheckSchemaConflict` 检查目标数据库的 Schema 是不是有冲突。\n\n## RestoreController\n\nlightning 的导入过程由 `RestoreController` 进行控制。`Run` 中的 `opts` 定义了导入过程所有的操作。\n\n### CheckRequirements\n\n通过多态实现，三种 backend 执行不同的检查。\n\n### SetGlobalVariables\n\n在 Server 模式下，目标数据库的 `Collation` 设置都可能不同，所以每次执行任务都需要设置一下。\n\n### RestoreSchema\n\n从这里，lightning 真正开始导入数据。首先，导入 Schema。Schema 包含 `database`, `table`, `view` 三个部分，分别对应着之前从 MyDumper 中导出的三个部分。Lightning 使用 Glue 中封装的目标数据库连接，执行 SQL，导入 Schema 信息。\n\n这里 lightning 使用了一种非常 Go 风格的实现方法。先创建出多个 goroutine，每个 goroutine 都监听着同一个 channel。之后再不断向 channel 中加入要执行的 SQL 任务，从而实现一个类似于线程池的功能。\n\n在导入 Schema 成功后，RestoreSchema 还会负责初始化 Checkpoint，并创建一个 goroutine，用来监听 Checkpoint 的变化，将多个 Checkpoint 合并到一起。\n\n最后，RestoreSchema 还会根据 Schema 中包含的元信息，对之后导入数据时划分的 chunk 数量进行一个估计。\n\n### RestoreTables\n\nRestoreTables 负责从源数据库中导出数据到目标数据库中。\n\n首先，在 `populateChunks` 函数中，使用 MyDumper 将数据划分成多个 chunk，接着将 chunk 加入到其对应的数据 engine 中。然后添加一个索引 engine 的 Checkpoint。\n\n然后，通过 `InsertEngineCheckpoint` 创建每张表的 Checkpoint。\n\n然后，在 `restoreEngines` 中，先将源数据库的数据从导出的文件，并发地转化成 KV 格式的 engine，然后再导入到目标数据库中。关于 engine 的状态机模型，可以参考我的另一篇[文章](https://blog.abingcbc.cn/2021/03/17/tikv-importer)中的总结。这个函数中包含了 lightning 最核心的逻辑，所以比较复杂，大致可以分为以下几个过程：\n\n1. 在 chunkRestore 的 `restore` 中，在 `encodeLoop` 中，利用 MyDumper 的 parser，将数据从不同的导出格式统一转换成 KV 格式，插入到 engine 中。此时，所有的 Checkpoint，包括 data 和 index，都已经写入完成。\n2. 接着，根据 engine 的状态机，关闭 engine 才能开始导入，所以我们现在需要将 engine 的状态置为 close。\n3. 关闭后，开始调用 backend 导入 engine。在所有的 data engine 导入完成后，开始导入 index engine。\n\n这里，又使用了一种非常 Go 风格的方式实现线程池的功能。利用 channel 的缓冲特性，向其中添加 worker，使用时取走，使用完再放回到 channel 中。\n\n最后，对于某些需要执行 post process 的任务，执行一下相应的任务。\n\n### FullCompact\n\n调用 TiKV 的接口，对所有新导入的数据进行压缩。但是由于之前导入数据的时候，会根据是否存在压缩任务，去尝试执行 level-1 的压缩，所以进行全量压缩的时候，需要等待之前启动的 level-1 压缩任务结束。\n\n### SwitchToNormalMode\n\n将 TiKV 切换成正常模式\n\n### CleanCheckpoints\n\n所有的导入任务都结束了，此时 checkpoint 已经无用了。所以，根据用户配置是否删除，处理 checkpoint。","slug":"tidb-lightning","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clapldfdl000t18mz5e5cb3l2","content":"<p>TiDB Lightning 是一个将数据导入到 TiDB 中的工具，使用 Go 编写。支持 <code>Local</code>, <code>Importer</code>, <code>TiDB</code> 三种导入模式。在 TiDB 的官网中，对其<a href=\"\">原理</a>有着详细的介绍。</p>\n<p>本文从代码的角度，带领大家走过一个数据导入的过程，所以只关注一些逻辑上重要的步骤，而一些其他的细节可能不会涉及到。</p>\n<span id=\"more\"></span>\n<p><img src=\"/asset/tidb-lightning/1.png\" alt=\"\" /><br />\n首先，进入 main 函数，程序调用了两个主要的启动函数：<code>GoServer</code> 和 <code>RunServer</code>。</p>\n<h2 id=\"goserver\"><a class=\"markdownIt-Anchor\" href=\"#goserver\"></a> GoServer</h2>\n<p><a href=\"https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96\">https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96</a></p>\n<p>GoServer 启动了一个 Api Server，有着以下这些 endpoint</p>\n<ul>\n<li>/web</li>\n<li>/metrics</li>\n<li>/debug</li>\n<li>/tasks</li>\n<li>/progress</li>\n<li>/pause</li>\n<li>/resume</li>\n<li>/loglevel</li>\n</ul>\n<p>Api Server 为整个 lightning 提供了控制，监控和查看进度等功能。是用户与 lightning 交互的入口。而当用户通过 <code>POST /tasks</code> 提交了一个数据导入任务时，Api Server 就会向队列中添加一个任务，而同样在监听着队列的 RunServer 就会开始执行。</p>\n<h2 id=\"runserver\"><a class=\"markdownIt-Anchor\" href=\"#runserver\"></a> RunServer</h2>\n<p><a href=\"https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194\">https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194</a></p>\n<p>RunServer 是真正执行导入的地方。一个 for 循环一直从队列中获取任务配置。在获取到一个任务的配置后，开始正式执行导入任务。</p>\n<h3 id=\"registermysql\"><a class=\"markdownIt-Anchor\" href=\"#registermysql\"></a> RegisterMySQL</h3>\n<p>这个函数的命名有点迷惑，但所幸有注释。RegisterMySQL 同时包含了向 gomysql driver 配置 TLS 和解除配置的两个作用。在运行的一开始，注册 TiDB 的 TLS 配置，这样直接可以使用 <code>sql.open()</code> 连接 TiDB。在执行导入任务结束后，通过将 <code>CAPath</code> 置为空，删除 TLS 配置。</p>\n<h3 id=\"glue\"><a class=\"markdownIt-Anchor\" href=\"#glue\"></a> Glue</h3>\n<p>Glue 将数据和导入粘合在一起。这里在创建 Glue 时，就将 TiDB 设为了导入模式。</p>\n<h3 id=\"mydumper\"><a class=\"markdownIt-Anchor\" href=\"#mydumper\"></a> MyDumper</h3>\n<p>确定好目标数据库也就是 backend 后，我们就可以看看数据导入的 frontend 了。<code>MyDumper</code> 是一款由 PingCAP 基于社区版，为 TiDB 定制化开发的数据导出工具。但是目前似乎已经不推荐使用了，推荐改用 <code>dumpling</code> 了。</p>\n<p>在这里创建 MyDumper 时，就会在 <code>setup</code> 中从 SQL 文件中。 这里会对表按从小到大进行排序，让小表之后先进行导入，这样可以避免大表导入时阻塞小表释放 index worker。</p>\n<pre class=\"highlight\"><code class=\"go\"><span class=\"hljs-keyword\">type</span> mdLoaderSetup <span class=\"hljs-keyword\">struct</span> &#123;\n\tloader        *MDLoader\n\tdbSchemas     []FileInfo\n\ttableSchemas  []FileInfo\n\tviewSchemas   []FileInfo\n\ttableDatas    []FileInfo\n\tdbIndexMap    <span class=\"hljs-keyword\">map</span>[<span class=\"hljs-type\">string</span>]<span class=\"hljs-type\">int</span>\n\ttableIndexMap <span class=\"hljs-keyword\">map</span>[filter.Table]<span class=\"hljs-type\">int</span>\n&#125;\n</code></pre>\n<h3 id=\"check\"><a class=\"markdownIt-Anchor\" href=\"#check\"></a> Check</h3>\n<p>初始化并配置好 MyDumper 后，在正式开跑前，程序还需要进行检查。这里检查了两项：1. <code>CheckSystemRequirement</code> 系统要求是否满足，lightning 对于内存要求似乎还挺高。2. <code>CheckSchemaConflict</code> 检查目标数据库的 Schema 是不是有冲突。</p>\n<h2 id=\"restorecontroller\"><a class=\"markdownIt-Anchor\" href=\"#restorecontroller\"></a> RestoreController</h2>\n<p>lightning 的导入过程由 <code>RestoreController</code> 进行控制。<code>Run</code> 中的 <code>opts</code> 定义了导入过程所有的操作。</p>\n<h3 id=\"checkrequirements\"><a class=\"markdownIt-Anchor\" href=\"#checkrequirements\"></a> CheckRequirements</h3>\n<p>通过多态实现，三种 backend 执行不同的检查。</p>\n<h3 id=\"setglobalvariables\"><a class=\"markdownIt-Anchor\" href=\"#setglobalvariables\"></a> SetGlobalVariables</h3>\n<p>在 Server 模式下，目标数据库的 <code>Collation</code> 设置都可能不同，所以每次执行任务都需要设置一下。</p>\n<h3 id=\"restoreschema\"><a class=\"markdownIt-Anchor\" href=\"#restoreschema\"></a> RestoreSchema</h3>\n<p>从这里，lightning 真正开始导入数据。首先，导入 Schema。Schema 包含 <code>database</code>, <code>table</code>, <code>view</code> 三个部分，分别对应着之前从 MyDumper 中导出的三个部分。Lightning 使用 Glue 中封装的目标数据库连接，执行 SQL，导入 Schema 信息。</p>\n<p>这里 lightning 使用了一种非常 Go 风格的实现方法。先创建出多个 goroutine，每个 goroutine 都监听着同一个 channel。之后再不断向 channel 中加入要执行的 SQL 任务，从而实现一个类似于线程池的功能。</p>\n<p>在导入 Schema 成功后，RestoreSchema 还会负责初始化 Checkpoint，并创建一个 goroutine，用来监听 Checkpoint 的变化，将多个 Checkpoint 合并到一起。</p>\n<p>最后，RestoreSchema 还会根据 Schema 中包含的元信息，对之后导入数据时划分的 chunk 数量进行一个估计。</p>\n<h3 id=\"restoretables\"><a class=\"markdownIt-Anchor\" href=\"#restoretables\"></a> RestoreTables</h3>\n<p>RestoreTables 负责从源数据库中导出数据到目标数据库中。</p>\n<p>首先，在 <code>populateChunks</code> 函数中，使用 MyDumper 将数据划分成多个 chunk，接着将 chunk 加入到其对应的数据 engine 中。然后添加一个索引 engine 的 Checkpoint。</p>\n<p>然后，通过 <code>InsertEngineCheckpoint</code> 创建每张表的 Checkpoint。</p>\n<p>然后，在 <code>restoreEngines</code> 中，先将源数据库的数据从导出的文件，并发地转化成 KV 格式的 engine，然后再导入到目标数据库中。关于 engine 的状态机模型，可以参考我的另一篇<a href=\"https://blog.abingcbc.cn/2021/03/17/tikv-importer\">文章</a>中的总结。这个函数中包含了 lightning 最核心的逻辑，所以比较复杂，大致可以分为以下几个过程：</p>\n<ol>\n<li>在 chunkRestore 的 <code>restore</code> 中，在 <code>encodeLoop</code> 中，利用 MyDumper 的 parser，将数据从不同的导出格式统一转换成 KV 格式，插入到 engine 中。此时，所有的 Checkpoint，包括 data 和 index，都已经写入完成。</li>\n<li>接着，根据 engine 的状态机，关闭 engine 才能开始导入，所以我们现在需要将 engine 的状态置为 close。</li>\n<li>关闭后，开始调用 backend 导入 engine。在所有的 data engine 导入完成后，开始导入 index engine。</li>\n</ol>\n<p>这里，又使用了一种非常 Go 风格的方式实现线程池的功能。利用 channel 的缓冲特性，向其中添加 worker，使用时取走，使用完再放回到 channel 中。</p>\n<p>最后，对于某些需要执行 post process 的任务，执行一下相应的任务。</p>\n<h3 id=\"fullcompact\"><a class=\"markdownIt-Anchor\" href=\"#fullcompact\"></a> FullCompact</h3>\n<p>调用 TiKV 的接口，对所有新导入的数据进行压缩。但是由于之前导入数据的时候，会根据是否存在压缩任务，去尝试执行 level-1 的压缩，所以进行全量压缩的时候，需要等待之前启动的 level-1 压缩任务结束。</p>\n<h3 id=\"switchtonormalmode\"><a class=\"markdownIt-Anchor\" href=\"#switchtonormalmode\"></a> SwitchToNormalMode</h3>\n<p>将 TiKV 切换成正常模式</p>\n<h3 id=\"cleancheckpoints\"><a class=\"markdownIt-Anchor\" href=\"#cleancheckpoints\"></a> CleanCheckpoints</h3>\n<p>所有的导入任务都结束了，此时 checkpoint 已经无用了。所以，根据用户配置是否删除，处理 checkpoint。</p>\n","site":{"data":{}},"excerpt":"<p>TiDB Lightning 是一个将数据导入到 TiDB 中的工具，使用 Go 编写。支持 <code>Local</code>, <code>Importer</code>, <code>TiDB</code> 三种导入模式。在 TiDB 的官网中，对其<a href=\"\">原理</a>有着详细的介绍。</p>\n<p>本文从代码的角度，带领大家走过一个数据导入的过程，所以只关注一些逻辑上重要的步骤，而一些其他的细节可能不会涉及到。</p>","more":"<p><img src=\"/asset/tidb-lightning/1.png\" alt=\"\" /><br />\n首先，进入 main 函数，程序调用了两个主要的启动函数：<code>GoServer</code> 和 <code>RunServer</code>。</p>\n<h2 id=\"goserver\"><a class=\"markdownIt-Anchor\" href=\"#goserver\"></a> GoServer</h2>\n<p><a href=\"https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96\">https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96</a></p>\n<p>GoServer 启动了一个 Api Server，有着以下这些 endpoint</p>\n<ul>\n<li>/web</li>\n<li>/metrics</li>\n<li>/debug</li>\n<li>/tasks</li>\n<li>/progress</li>\n<li>/pause</li>\n<li>/resume</li>\n<li>/loglevel</li>\n</ul>\n<p>Api Server 为整个 lightning 提供了控制，监控和查看进度等功能。是用户与 lightning 交互的入口。而当用户通过 <code>POST /tasks</code> 提交了一个数据导入任务时，Api Server 就会向队列中添加一个任务，而同样在监听着队列的 RunServer 就会开始执行。</p>\n<h2 id=\"runserver\"><a class=\"markdownIt-Anchor\" href=\"#runserver\"></a> RunServer</h2>\n<p><a href=\"https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194\">https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194</a></p>\n<p>RunServer 是真正执行导入的地方。一个 for 循环一直从队列中获取任务配置。在获取到一个任务的配置后，开始正式执行导入任务。</p>\n<h3 id=\"registermysql\"><a class=\"markdownIt-Anchor\" href=\"#registermysql\"></a> RegisterMySQL</h3>\n<p>这个函数的命名有点迷惑，但所幸有注释。RegisterMySQL 同时包含了向 gomysql driver 配置 TLS 和解除配置的两个作用。在运行的一开始，注册 TiDB 的 TLS 配置，这样直接可以使用 <code>sql.open()</code> 连接 TiDB。在执行导入任务结束后，通过将 <code>CAPath</code> 置为空，删除 TLS 配置。</p>\n<h3 id=\"glue\"><a class=\"markdownIt-Anchor\" href=\"#glue\"></a> Glue</h3>\n<p>Glue 将数据和导入粘合在一起。这里在创建 Glue 时，就将 TiDB 设为了导入模式。</p>\n<h3 id=\"mydumper\"><a class=\"markdownIt-Anchor\" href=\"#mydumper\"></a> MyDumper</h3>\n<p>确定好目标数据库也就是 backend 后，我们就可以看看数据导入的 frontend 了。<code>MyDumper</code> 是一款由 PingCAP 基于社区版，为 TiDB 定制化开发的数据导出工具。但是目前似乎已经不推荐使用了，推荐改用 <code>dumpling</code> 了。</p>\n<p>在这里创建 MyDumper 时，就会在 <code>setup</code> 中从 SQL 文件中。 这里会对表按从小到大进行排序，让小表之后先进行导入，这样可以避免大表导入时阻塞小表释放 index worker。</p>\n<pre class=\"highlight\"><code class=\"go\"><span class=\"hljs-keyword\">type</span> mdLoaderSetup <span class=\"hljs-keyword\">struct</span> &#123;\n\tloader        *MDLoader\n\tdbSchemas     []FileInfo\n\ttableSchemas  []FileInfo\n\tviewSchemas   []FileInfo\n\ttableDatas    []FileInfo\n\tdbIndexMap    <span class=\"hljs-keyword\">map</span>[<span class=\"hljs-type\">string</span>]<span class=\"hljs-type\">int</span>\n\ttableIndexMap <span class=\"hljs-keyword\">map</span>[filter.Table]<span class=\"hljs-type\">int</span>\n&#125;\n</code></pre>\n<h3 id=\"check\"><a class=\"markdownIt-Anchor\" href=\"#check\"></a> Check</h3>\n<p>初始化并配置好 MyDumper 后，在正式开跑前，程序还需要进行检查。这里检查了两项：1. <code>CheckSystemRequirement</code> 系统要求是否满足，lightning 对于内存要求似乎还挺高。2. <code>CheckSchemaConflict</code> 检查目标数据库的 Schema 是不是有冲突。</p>\n<h2 id=\"restorecontroller\"><a class=\"markdownIt-Anchor\" href=\"#restorecontroller\"></a> RestoreController</h2>\n<p>lightning 的导入过程由 <code>RestoreController</code> 进行控制。<code>Run</code> 中的 <code>opts</code> 定义了导入过程所有的操作。</p>\n<h3 id=\"checkrequirements\"><a class=\"markdownIt-Anchor\" href=\"#checkrequirements\"></a> CheckRequirements</h3>\n<p>通过多态实现，三种 backend 执行不同的检查。</p>\n<h3 id=\"setglobalvariables\"><a class=\"markdownIt-Anchor\" href=\"#setglobalvariables\"></a> SetGlobalVariables</h3>\n<p>在 Server 模式下，目标数据库的 <code>Collation</code> 设置都可能不同，所以每次执行任务都需要设置一下。</p>\n<h3 id=\"restoreschema\"><a class=\"markdownIt-Anchor\" href=\"#restoreschema\"></a> RestoreSchema</h3>\n<p>从这里，lightning 真正开始导入数据。首先，导入 Schema。Schema 包含 <code>database</code>, <code>table</code>, <code>view</code> 三个部分，分别对应着之前从 MyDumper 中导出的三个部分。Lightning 使用 Glue 中封装的目标数据库连接，执行 SQL，导入 Schema 信息。</p>\n<p>这里 lightning 使用了一种非常 Go 风格的实现方法。先创建出多个 goroutine，每个 goroutine 都监听着同一个 channel。之后再不断向 channel 中加入要执行的 SQL 任务，从而实现一个类似于线程池的功能。</p>\n<p>在导入 Schema 成功后，RestoreSchema 还会负责初始化 Checkpoint，并创建一个 goroutine，用来监听 Checkpoint 的变化，将多个 Checkpoint 合并到一起。</p>\n<p>最后，RestoreSchema 还会根据 Schema 中包含的元信息，对之后导入数据时划分的 chunk 数量进行一个估计。</p>\n<h3 id=\"restoretables\"><a class=\"markdownIt-Anchor\" href=\"#restoretables\"></a> RestoreTables</h3>\n<p>RestoreTables 负责从源数据库中导出数据到目标数据库中。</p>\n<p>首先，在 <code>populateChunks</code> 函数中，使用 MyDumper 将数据划分成多个 chunk，接着将 chunk 加入到其对应的数据 engine 中。然后添加一个索引 engine 的 Checkpoint。</p>\n<p>然后，通过 <code>InsertEngineCheckpoint</code> 创建每张表的 Checkpoint。</p>\n<p>然后，在 <code>restoreEngines</code> 中，先将源数据库的数据从导出的文件，并发地转化成 KV 格式的 engine，然后再导入到目标数据库中。关于 engine 的状态机模型，可以参考我的另一篇<a href=\"https://blog.abingcbc.cn/2021/03/17/tikv-importer\">文章</a>中的总结。这个函数中包含了 lightning 最核心的逻辑，所以比较复杂，大致可以分为以下几个过程：</p>\n<ol>\n<li>在 chunkRestore 的 <code>restore</code> 中，在 <code>encodeLoop</code> 中，利用 MyDumper 的 parser，将数据从不同的导出格式统一转换成 KV 格式，插入到 engine 中。此时，所有的 Checkpoint，包括 data 和 index，都已经写入完成。</li>\n<li>接着，根据 engine 的状态机，关闭 engine 才能开始导入，所以我们现在需要将 engine 的状态置为 close。</li>\n<li>关闭后，开始调用 backend 导入 engine。在所有的 data engine 导入完成后，开始导入 index engine。</li>\n</ol>\n<p>这里，又使用了一种非常 Go 风格的方式实现线程池的功能。利用 channel 的缓冲特性，向其中添加 worker，使用时取走，使用完再放回到 channel 中。</p>\n<p>最后，对于某些需要执行 post process 的任务，执行一下相应的任务。</p>\n<h3 id=\"fullcompact\"><a class=\"markdownIt-Anchor\" href=\"#fullcompact\"></a> FullCompact</h3>\n<p>调用 TiKV 的接口，对所有新导入的数据进行压缩。但是由于之前导入数据的时候，会根据是否存在压缩任务，去尝试执行 level-1 的压缩，所以进行全量压缩的时候，需要等待之前启动的 level-1 压缩任务结束。</p>\n<h3 id=\"switchtonormalmode\"><a class=\"markdownIt-Anchor\" href=\"#switchtonormalmode\"></a> SwitchToNormalMode</h3>\n<p>将 TiKV 切换成正常模式</p>\n<h3 id=\"cleancheckpoints\"><a class=\"markdownIt-Anchor\" href=\"#cleancheckpoints\"></a> CleanCheckpoints</h3>\n<p>所有的导入任务都结束了，此时 checkpoint 已经无用了。所以，根据用户配置是否删除，处理 checkpoint。</p>"},{"title":"[PaperRead] Time2graph: Revisiting time series modeling with dynamic shapelets","date":"2021-09-06T18:39:53.000Z","updated":"2021-09-06T18:39:53.000Z","_content":"\nShapelet [<sup>1</sup>](#shapelet) 是一种常见的对时序数据进行建模的方法之一。从 2009 年在 KDD 上发布第一篇论文以来，其经过了各式各样的变形和改进。Time2graph [<sup>2</sup>](#time2graph) 观察到 shapelet 具有 time-aware 的性质，因此提出了一种基于 shapelet 的时序数据的建模方法，将数据建模成图结构。\n<!-- more -->\n\n## What\n### Shapelet\nA shapelet is a segment that is representative of a certain class.\nShapelet 是一段具有代表性的子序列，它可以将一条时序数据的所有分段分成两类，一类与其相似，另一类与其不同。\n### Time-aware shapelet\nTime2graph 中所创新性提出的包含时间信息的 shapelet，利用图对其在时间维度上的变化进行捕捉。\n\n## Why\n传统的 shaplet 以及其改进形式都忽略了 shapelet 在不同的时间区间上的表现能力。这造成两点不足：\n1. 相同 shapelet 在不同时间区间上的含义不同。\n2. shapelet 之间的变化关系也表示了时序数据的一些信息。\n\n## How\n### Time-Aware Shapelet Extraction\nTime-aware shapelet 的抽取可以分为以下三个步骤\n1. 分段\n\n    这里需要超参数如：每个分段的长度\n2. 选取 Candidate\n\n    这一部分在论文中没有叙述，但阅读源代码后，我们可以发现，针对所有分段后得到的片段，有两种方法从中得到 candidate shapelet。第一种是通过聚类，例如 K-Means，和 DTW 作为距离进行聚类，然后选取类中心的片段作为 candidate。第二种则是贪婪的方法，即遍历计算一个片段与其他所有片段之间的距离，然后选取平均最小的片段作为 candidate。\n3. 计算评分\n\n    Time2graph 设计了新的评分方法来衡量 shapelet 在时间维度上的重要度，这部分比较复杂，后文会进行详细的介绍。对所有的 shapelet candidate 打分后，我们就可以选取 top-k 作为真正的 shapelet。\n\n#### Distance\nTime2graph 设计了两个需要训练的参数，local factor **w** 和 global factor **u** 来分别衡量 shapelet 内部每个元素的重要度以及 shapelet 在不同时间段上的重要度。\n\n训练的过程需要进行梯度下降，损失函数如下图所示\n![](/asset/time2graph/loss_function.png)\n\n```\nv: shapelet\nT: time series\nS∗(v, T): the set of distances with respect to a specific group T∗\ng: differentiable function measures the distances between distributions of two finite sets \n```\n最后两项为 penalties。当梯度下降使得损失函数的值越来越小时，`w` 和 `u` 就越来越可以使得 shapelet 到 positive 和 negative 的距离之差越来越大，这样 shapelet 就越能代表一个类。\n\n接下来，详细介绍损失函数中所用到的两个距离函数。\n\n1. shapelet 与 segment 之间的距离\n\n![](/asset/time2graph/dist_seg.png)\n\n```\nw: local factor 需要学习的参数\nu: global factor 需要学习的参数\ns: segment\na: alignment，利用 DTW 可将长度为 i 的 v 和长度为 j 的 s 对齐成长度为 p 的 a1 和 a2\n```\n\n对于 DTW alignment，下面这张图形象地解释了整个对齐的过程\n\n![](/asset/time2graph/dtw.png)\n\n对于序列 s1 和 s2，s1 中的一个点可能对应 s2 中的多个点，同时 s2 中的一个点也可能对应 s1 中的多个点。所以，两者对齐后，会得到一个新的长度的序列。而 shapelet 与 segment 的距离，其实就是计算两者对齐后的欧氏距离。\n\n2. shapelet 与 time series 之间的距离\n\n![](/asset/time2graph/dist_series.png)\n\n这个就比较简单了，`d` 即为刚刚介绍的 shapelet 与 segment 之间的距离。shapelet 与 time series 的距离即为与 time series 的所有分段中的权重最小值。\n\n### Shapelet Evolution Graph\n接下来，就是要构建图来表示 shapelet 之间的转移关系。\n\n![](/asset/time2graph/graph.png)\n\n算法先将 shapelet 与 segment 关联起来，edge 的权重由以下这个公式计算\n\n![](/asset/time2graph/edge_weight.png)\n\n公式计算的是 shapelet 与 segment 相关的概率，如果 shapelet 与 segment 之间的距离越近，那么相关的概率越高。\n\n接下来，算法将相邻的 segment 的 shapelet 连接起来，而 edge 的权重则是前后两个 segment 与各自 shapelet 之间关联概率的乘积，代表了从前一个 shapelet 迁移到后一个 shapelet 的概率。\n\n![](/asset/time2graph/transition.png)\n\n最后，算法将一个 shapelet 所有迁移出去的 edge 的权重进行归一化，使其和为 1。\n\n算法的伪代码如下所示\n\n![](/asset/time2graph/graph_algo.png)\n\n### Representation Learning\n最后，time2graph 采用了 DeepWalk [<sup>3</sup>](#deepwalk)的算法，将 shapelet 转换为 embedding 的形式。\n\n接下来，就可以利用 shapelet 的 embedding 来表示 segment 和 time series。\n\nsegment 的向量化表示如下面这个公式，就是其相关 shapelet 的 embedding 的概率权重和\n\n![](/asset/time2graph/segment.png)\n\n而 time series 的向量化表示就是将其所有 segment 的 embedding 的拼接。\n\n算法的伪代码如下所示\n\n![](/asset/time2graph/representation.png)\n\n### Apply Time2graph\n至此，我们就完整的了解了 time2graph 的训练过程。那么 time2graph 的应用过程也与之类似。\n\n首先，算法的输入仍然是一个 time series。算法会先对其进行分段，然后为每个 segment 分配相关的 shapelet。接下来，就可以得到 segment 和 time series 的 embedding，进行后续聚类或者其他操作。\n\n## Reference\n<div id=\"shapelet\" />\n- [1] Ye, Lexiang, and Eamonn Keogh. \"Time series shapelets: a new primitive for data mining.\" Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 2009.\n\n<div id=\"time2graph\" />\n- [2] Cheng, Ziqiang, et al. \"Time2graph: Revisiting time series modeling with dynamic shapelets.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.\n\n<div id=\"deepwalk\" />\n- [3] Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. \"Deepwalk: Online learning of social representations.\" Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 2014.","source":"_posts/time2graph.md","raw":"---\ntitle: \"[PaperRead] Time2graph: Revisiting time series modeling with dynamic shapelets\"\ndate: 2021-09-06 18:39:53\nupdated: 2021-09-06 18:39:53\n---\n\nShapelet [<sup>1</sup>](#shapelet) 是一种常见的对时序数据进行建模的方法之一。从 2009 年在 KDD 上发布第一篇论文以来，其经过了各式各样的变形和改进。Time2graph [<sup>2</sup>](#time2graph) 观察到 shapelet 具有 time-aware 的性质，因此提出了一种基于 shapelet 的时序数据的建模方法，将数据建模成图结构。\n<!-- more -->\n\n## What\n### Shapelet\nA shapelet is a segment that is representative of a certain class.\nShapelet 是一段具有代表性的子序列，它可以将一条时序数据的所有分段分成两类，一类与其相似，另一类与其不同。\n### Time-aware shapelet\nTime2graph 中所创新性提出的包含时间信息的 shapelet，利用图对其在时间维度上的变化进行捕捉。\n\n## Why\n传统的 shaplet 以及其改进形式都忽略了 shapelet 在不同的时间区间上的表现能力。这造成两点不足：\n1. 相同 shapelet 在不同时间区间上的含义不同。\n2. shapelet 之间的变化关系也表示了时序数据的一些信息。\n\n## How\n### Time-Aware Shapelet Extraction\nTime-aware shapelet 的抽取可以分为以下三个步骤\n1. 分段\n\n    这里需要超参数如：每个分段的长度\n2. 选取 Candidate\n\n    这一部分在论文中没有叙述，但阅读源代码后，我们可以发现，针对所有分段后得到的片段，有两种方法从中得到 candidate shapelet。第一种是通过聚类，例如 K-Means，和 DTW 作为距离进行聚类，然后选取类中心的片段作为 candidate。第二种则是贪婪的方法，即遍历计算一个片段与其他所有片段之间的距离，然后选取平均最小的片段作为 candidate。\n3. 计算评分\n\n    Time2graph 设计了新的评分方法来衡量 shapelet 在时间维度上的重要度，这部分比较复杂，后文会进行详细的介绍。对所有的 shapelet candidate 打分后，我们就可以选取 top-k 作为真正的 shapelet。\n\n#### Distance\nTime2graph 设计了两个需要训练的参数，local factor **w** 和 global factor **u** 来分别衡量 shapelet 内部每个元素的重要度以及 shapelet 在不同时间段上的重要度。\n\n训练的过程需要进行梯度下降，损失函数如下图所示\n![](/asset/time2graph/loss_function.png)\n\n```\nv: shapelet\nT: time series\nS∗(v, T): the set of distances with respect to a specific group T∗\ng: differentiable function measures the distances between distributions of two finite sets \n```\n最后两项为 penalties。当梯度下降使得损失函数的值越来越小时，`w` 和 `u` 就越来越可以使得 shapelet 到 positive 和 negative 的距离之差越来越大，这样 shapelet 就越能代表一个类。\n\n接下来，详细介绍损失函数中所用到的两个距离函数。\n\n1. shapelet 与 segment 之间的距离\n\n![](/asset/time2graph/dist_seg.png)\n\n```\nw: local factor 需要学习的参数\nu: global factor 需要学习的参数\ns: segment\na: alignment，利用 DTW 可将长度为 i 的 v 和长度为 j 的 s 对齐成长度为 p 的 a1 和 a2\n```\n\n对于 DTW alignment，下面这张图形象地解释了整个对齐的过程\n\n![](/asset/time2graph/dtw.png)\n\n对于序列 s1 和 s2，s1 中的一个点可能对应 s2 中的多个点，同时 s2 中的一个点也可能对应 s1 中的多个点。所以，两者对齐后，会得到一个新的长度的序列。而 shapelet 与 segment 的距离，其实就是计算两者对齐后的欧氏距离。\n\n2. shapelet 与 time series 之间的距离\n\n![](/asset/time2graph/dist_series.png)\n\n这个就比较简单了，`d` 即为刚刚介绍的 shapelet 与 segment 之间的距离。shapelet 与 time series 的距离即为与 time series 的所有分段中的权重最小值。\n\n### Shapelet Evolution Graph\n接下来，就是要构建图来表示 shapelet 之间的转移关系。\n\n![](/asset/time2graph/graph.png)\n\n算法先将 shapelet 与 segment 关联起来，edge 的权重由以下这个公式计算\n\n![](/asset/time2graph/edge_weight.png)\n\n公式计算的是 shapelet 与 segment 相关的概率，如果 shapelet 与 segment 之间的距离越近，那么相关的概率越高。\n\n接下来，算法将相邻的 segment 的 shapelet 连接起来，而 edge 的权重则是前后两个 segment 与各自 shapelet 之间关联概率的乘积，代表了从前一个 shapelet 迁移到后一个 shapelet 的概率。\n\n![](/asset/time2graph/transition.png)\n\n最后，算法将一个 shapelet 所有迁移出去的 edge 的权重进行归一化，使其和为 1。\n\n算法的伪代码如下所示\n\n![](/asset/time2graph/graph_algo.png)\n\n### Representation Learning\n最后，time2graph 采用了 DeepWalk [<sup>3</sup>](#deepwalk)的算法，将 shapelet 转换为 embedding 的形式。\n\n接下来，就可以利用 shapelet 的 embedding 来表示 segment 和 time series。\n\nsegment 的向量化表示如下面这个公式，就是其相关 shapelet 的 embedding 的概率权重和\n\n![](/asset/time2graph/segment.png)\n\n而 time series 的向量化表示就是将其所有 segment 的 embedding 的拼接。\n\n算法的伪代码如下所示\n\n![](/asset/time2graph/representation.png)\n\n### Apply Time2graph\n至此，我们就完整的了解了 time2graph 的训练过程。那么 time2graph 的应用过程也与之类似。\n\n首先，算法的输入仍然是一个 time series。算法会先对其进行分段，然后为每个 segment 分配相关的 shapelet。接下来，就可以得到 segment 和 time series 的 embedding，进行后续聚类或者其他操作。\n\n## Reference\n<div id=\"shapelet\" />\n- [1] Ye, Lexiang, and Eamonn Keogh. \"Time series shapelets: a new primitive for data mining.\" Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 2009.\n\n<div id=\"time2graph\" />\n- [2] Cheng, Ziqiang, et al. \"Time2graph: Revisiting time series modeling with dynamic shapelets.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.\n\n<div id=\"deepwalk\" />\n- [3] Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. \"Deepwalk: Online learning of social representations.\" Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 2014.","slug":"time2graph","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"clapldfdm000u18mz836we3zo","content":"<p>Shapelet <a href=\"#shapelet\"><sup>1</sup></a> 是一种常见的对时序数据进行建模的方法之一。从 2009 年在 KDD 上发布第一篇论文以来，其经过了各式各样的变形和改进。Time2graph <a href=\"#time2graph\"><sup>2</sup></a> 观察到 shapelet 具有 time-aware 的性质，因此提出了一种基于 shapelet 的时序数据的建模方法，将数据建模成图结构。</p>\n<span id=\"more\"></span>\n<h2 id=\"what\"><a class=\"markdownIt-Anchor\" href=\"#what\"></a> What</h2>\n<h3 id=\"shapelet\"><a class=\"markdownIt-Anchor\" href=\"#shapelet\"></a> Shapelet</h3>\n<p>A shapelet is a segment that is representative of a certain class.<br />\nShapelet 是一段具有代表性的子序列，它可以将一条时序数据的所有分段分成两类，一类与其相似，另一类与其不同。</p>\n<h3 id=\"time-aware-shapelet\"><a class=\"markdownIt-Anchor\" href=\"#time-aware-shapelet\"></a> Time-aware shapelet</h3>\n<p>Time2graph 中所创新性提出的包含时间信息的 shapelet，利用图对其在时间维度上的变化进行捕捉。</p>\n<h2 id=\"why\"><a class=\"markdownIt-Anchor\" href=\"#why\"></a> Why</h2>\n<p>传统的 shaplet 以及其改进形式都忽略了 shapelet 在不同的时间区间上的表现能力。这造成两点不足：</p>\n<ol>\n<li>相同 shapelet 在不同时间区间上的含义不同。</li>\n<li>shapelet 之间的变化关系也表示了时序数据的一些信息。</li>\n</ol>\n<h2 id=\"how\"><a class=\"markdownIt-Anchor\" href=\"#how\"></a> How</h2>\n<h3 id=\"time-aware-shapelet-extraction\"><a class=\"markdownIt-Anchor\" href=\"#time-aware-shapelet-extraction\"></a> Time-Aware Shapelet Extraction</h3>\n<p>Time-aware shapelet 的抽取可以分为以下三个步骤</p>\n<ol>\n<li>\n<p>分段</p>\n<p>这里需要超参数如：每个分段的长度</p>\n</li>\n<li>\n<p>选取 Candidate</p>\n<p>这一部分在论文中没有叙述，但阅读源代码后，我们可以发现，针对所有分段后得到的片段，有两种方法从中得到 candidate shapelet。第一种是通过聚类，例如 K-Means，和 DTW 作为距离进行聚类，然后选取类中心的片段作为 candidate。第二种则是贪婪的方法，即遍历计算一个片段与其他所有片段之间的距离，然后选取平均最小的片段作为 candidate。</p>\n</li>\n<li>\n<p>计算评分</p>\n<p>Time2graph 设计了新的评分方法来衡量 shapelet 在时间维度上的重要度，这部分比较复杂，后文会进行详细的介绍。对所有的 shapelet candidate 打分后，我们就可以选取 top-k 作为真正的 shapelet。</p>\n</li>\n</ol>\n<h4 id=\"distance\"><a class=\"markdownIt-Anchor\" href=\"#distance\"></a> Distance</h4>\n<p>Time2graph 设计了两个需要训练的参数，local factor <strong>w</strong> 和 global factor <strong>u</strong> 来分别衡量 shapelet 内部每个元素的重要度以及 shapelet 在不同时间段上的重要度。</p>\n<p>训练的过程需要进行梯度下降，损失函数如下图所示<br />\n<img src=\"/asset/time2graph/loss_function.png\" alt=\"\" /></p>\n<pre class=\"highlight\"><code class=\"\">v: shapelet\nT: time series\nS∗(v, T): the set of distances with respect to a specific group T∗\ng: differentiable function measures the distances between distributions of two finite sets \n</code></pre>\n<p>最后两项为 penalties。当梯度下降使得损失函数的值越来越小时，<code>w</code> 和 <code>u</code> 就越来越可以使得 shapelet 到 positive 和 negative 的距离之差越来越大，这样 shapelet 就越能代表一个类。</p>\n<p>接下来，详细介绍损失函数中所用到的两个距离函数。</p>\n<ol>\n<li>shapelet 与 segment 之间的距离</li>\n</ol>\n<p><img src=\"/asset/time2graph/dist_seg.png\" alt=\"\" /></p>\n<pre class=\"highlight\"><code class=\"\">w: local factor 需要学习的参数\nu: global factor 需要学习的参数\ns: segment\na: alignment，利用 DTW 可将长度为 i 的 v 和长度为 j 的 s 对齐成长度为 p 的 a1 和 a2\n</code></pre>\n<p>对于 DTW alignment，下面这张图形象地解释了整个对齐的过程</p>\n<p><img src=\"/asset/time2graph/dtw.png\" alt=\"\" /></p>\n<p>对于序列 s1 和 s2，s1 中的一个点可能对应 s2 中的多个点，同时 s2 中的一个点也可能对应 s1 中的多个点。所以，两者对齐后，会得到一个新的长度的序列。而 shapelet 与 segment 的距离，其实就是计算两者对齐后的欧氏距离。</p>\n<ol start=\"2\">\n<li>shapelet 与 time series 之间的距离</li>\n</ol>\n<p><img src=\"/asset/time2graph/dist_series.png\" alt=\"\" /></p>\n<p>这个就比较简单了，<code>d</code> 即为刚刚介绍的 shapelet 与 segment 之间的距离。shapelet 与 time series 的距离即为与 time series 的所有分段中的权重最小值。</p>\n<h3 id=\"shapelet-evolution-graph\"><a class=\"markdownIt-Anchor\" href=\"#shapelet-evolution-graph\"></a> Shapelet Evolution Graph</h3>\n<p>接下来，就是要构建图来表示 shapelet 之间的转移关系。</p>\n<p><img src=\"/asset/time2graph/graph.png\" alt=\"\" /></p>\n<p>算法先将 shapelet 与 segment 关联起来，edge 的权重由以下这个公式计算</p>\n<p><img src=\"/asset/time2graph/edge_weight.png\" alt=\"\" /></p>\n<p>公式计算的是 shapelet 与 segment 相关的概率，如果 shapelet 与 segment 之间的距离越近，那么相关的概率越高。</p>\n<p>接下来，算法将相邻的 segment 的 shapelet 连接起来，而 edge 的权重则是前后两个 segment 与各自 shapelet 之间关联概率的乘积，代表了从前一个 shapelet 迁移到后一个 shapelet 的概率。</p>\n<p><img src=\"/asset/time2graph/transition.png\" alt=\"\" /></p>\n<p>最后，算法将一个 shapelet 所有迁移出去的 edge 的权重进行归一化，使其和为 1。</p>\n<p>算法的伪代码如下所示</p>\n<p><img src=\"/asset/time2graph/graph_algo.png\" alt=\"\" /></p>\n<h3 id=\"representation-learning\"><a class=\"markdownIt-Anchor\" href=\"#representation-learning\"></a> Representation Learning</h3>\n<p>最后，time2graph 采用了 DeepWalk <a href=\"#deepwalk\"><sup>3</sup></a>的算法，将 shapelet 转换为 embedding 的形式。</p>\n<p>接下来，就可以利用 shapelet 的 embedding 来表示 segment 和 time series。</p>\n<p>segment 的向量化表示如下面这个公式，就是其相关 shapelet 的 embedding 的概率权重和</p>\n<p><img src=\"/asset/time2graph/segment.png\" alt=\"\" /></p>\n<p>而 time series 的向量化表示就是将其所有 segment 的 embedding 的拼接。</p>\n<p>算法的伪代码如下所示</p>\n<p><img src=\"/asset/time2graph/representation.png\" alt=\"\" /></p>\n<h3 id=\"apply-time2graph\"><a class=\"markdownIt-Anchor\" href=\"#apply-time2graph\"></a> Apply Time2graph</h3>\n<p>至此，我们就完整的了解了 time2graph 的训练过程。那么 time2graph 的应用过程也与之类似。</p>\n<p>首先，算法的输入仍然是一个 time series。算法会先对其进行分段，然后为每个 segment 分配相关的 shapelet。接下来，就可以得到 segment 和 time series 的 embedding，进行后续聚类或者其他操作。</p>\n<h2 id=\"reference\"><a class=\"markdownIt-Anchor\" href=\"#reference\"></a> Reference</h2>\n<div id=\"shapelet\" />\n- [1] Ye, Lexiang, and Eamonn Keogh. \"Time series shapelets: a new primitive for data mining.\" Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 2009.\n<div id=\"time2graph\" />\n- [2] Cheng, Ziqiang, et al. \"Time2graph: Revisiting time series modeling with dynamic shapelets.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.\n<div id=\"deepwalk\" />\n- [3] Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. \"Deepwalk: Online learning of social representations.\" Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 2014.","site":{"data":{}},"excerpt":"<p>Shapelet <a href=\"#shapelet\"><sup>1</sup></a> 是一种常见的对时序数据进行建模的方法之一。从 2009 年在 KDD 上发布第一篇论文以来，其经过了各式各样的变形和改进。Time2graph <a href=\"#time2graph\"><sup>2</sup></a> 观察到 shapelet 具有 time-aware 的性质，因此提出了一种基于 shapelet 的时序数据的建模方法，将数据建模成图结构。</p>","more":"<h2 id=\"what\"><a class=\"markdownIt-Anchor\" href=\"#what\"></a> What</h2>\n<h3 id=\"shapelet\"><a class=\"markdownIt-Anchor\" href=\"#shapelet\"></a> Shapelet</h3>\n<p>A shapelet is a segment that is representative of a certain class.<br />\nShapelet 是一段具有代表性的子序列，它可以将一条时序数据的所有分段分成两类，一类与其相似，另一类与其不同。</p>\n<h3 id=\"time-aware-shapelet\"><a class=\"markdownIt-Anchor\" href=\"#time-aware-shapelet\"></a> Time-aware shapelet</h3>\n<p>Time2graph 中所创新性提出的包含时间信息的 shapelet，利用图对其在时间维度上的变化进行捕捉。</p>\n<h2 id=\"why\"><a class=\"markdownIt-Anchor\" href=\"#why\"></a> Why</h2>\n<p>传统的 shaplet 以及其改进形式都忽略了 shapelet 在不同的时间区间上的表现能力。这造成两点不足：</p>\n<ol>\n<li>相同 shapelet 在不同时间区间上的含义不同。</li>\n<li>shapelet 之间的变化关系也表示了时序数据的一些信息。</li>\n</ol>\n<h2 id=\"how\"><a class=\"markdownIt-Anchor\" href=\"#how\"></a> How</h2>\n<h3 id=\"time-aware-shapelet-extraction\"><a class=\"markdownIt-Anchor\" href=\"#time-aware-shapelet-extraction\"></a> Time-Aware Shapelet Extraction</h3>\n<p>Time-aware shapelet 的抽取可以分为以下三个步骤</p>\n<ol>\n<li>\n<p>分段</p>\n<p>这里需要超参数如：每个分段的长度</p>\n</li>\n<li>\n<p>选取 Candidate</p>\n<p>这一部分在论文中没有叙述，但阅读源代码后，我们可以发现，针对所有分段后得到的片段，有两种方法从中得到 candidate shapelet。第一种是通过聚类，例如 K-Means，和 DTW 作为距离进行聚类，然后选取类中心的片段作为 candidate。第二种则是贪婪的方法，即遍历计算一个片段与其他所有片段之间的距离，然后选取平均最小的片段作为 candidate。</p>\n</li>\n<li>\n<p>计算评分</p>\n<p>Time2graph 设计了新的评分方法来衡量 shapelet 在时间维度上的重要度，这部分比较复杂，后文会进行详细的介绍。对所有的 shapelet candidate 打分后，我们就可以选取 top-k 作为真正的 shapelet。</p>\n</li>\n</ol>\n<h4 id=\"distance\"><a class=\"markdownIt-Anchor\" href=\"#distance\"></a> Distance</h4>\n<p>Time2graph 设计了两个需要训练的参数，local factor <strong>w</strong> 和 global factor <strong>u</strong> 来分别衡量 shapelet 内部每个元素的重要度以及 shapelet 在不同时间段上的重要度。</p>\n<p>训练的过程需要进行梯度下降，损失函数如下图所示<br />\n<img src=\"/asset/time2graph/loss_function.png\" alt=\"\" /></p>\n<pre class=\"highlight\"><code class=\"\">v: shapelet\nT: time series\nS∗(v, T): the set of distances with respect to a specific group T∗\ng: differentiable function measures the distances between distributions of two finite sets \n</code></pre>\n<p>最后两项为 penalties。当梯度下降使得损失函数的值越来越小时，<code>w</code> 和 <code>u</code> 就越来越可以使得 shapelet 到 positive 和 negative 的距离之差越来越大，这样 shapelet 就越能代表一个类。</p>\n<p>接下来，详细介绍损失函数中所用到的两个距离函数。</p>\n<ol>\n<li>shapelet 与 segment 之间的距离</li>\n</ol>\n<p><img src=\"/asset/time2graph/dist_seg.png\" alt=\"\" /></p>\n<pre class=\"highlight\"><code class=\"\">w: local factor 需要学习的参数\nu: global factor 需要学习的参数\ns: segment\na: alignment，利用 DTW 可将长度为 i 的 v 和长度为 j 的 s 对齐成长度为 p 的 a1 和 a2\n</code></pre>\n<p>对于 DTW alignment，下面这张图形象地解释了整个对齐的过程</p>\n<p><img src=\"/asset/time2graph/dtw.png\" alt=\"\" /></p>\n<p>对于序列 s1 和 s2，s1 中的一个点可能对应 s2 中的多个点，同时 s2 中的一个点也可能对应 s1 中的多个点。所以，两者对齐后，会得到一个新的长度的序列。而 shapelet 与 segment 的距离，其实就是计算两者对齐后的欧氏距离。</p>\n<ol start=\"2\">\n<li>shapelet 与 time series 之间的距离</li>\n</ol>\n<p><img src=\"/asset/time2graph/dist_series.png\" alt=\"\" /></p>\n<p>这个就比较简单了，<code>d</code> 即为刚刚介绍的 shapelet 与 segment 之间的距离。shapelet 与 time series 的距离即为与 time series 的所有分段中的权重最小值。</p>\n<h3 id=\"shapelet-evolution-graph\"><a class=\"markdownIt-Anchor\" href=\"#shapelet-evolution-graph\"></a> Shapelet Evolution Graph</h3>\n<p>接下来，就是要构建图来表示 shapelet 之间的转移关系。</p>\n<p><img src=\"/asset/time2graph/graph.png\" alt=\"\" /></p>\n<p>算法先将 shapelet 与 segment 关联起来，edge 的权重由以下这个公式计算</p>\n<p><img src=\"/asset/time2graph/edge_weight.png\" alt=\"\" /></p>\n<p>公式计算的是 shapelet 与 segment 相关的概率，如果 shapelet 与 segment 之间的距离越近，那么相关的概率越高。</p>\n<p>接下来，算法将相邻的 segment 的 shapelet 连接起来，而 edge 的权重则是前后两个 segment 与各自 shapelet 之间关联概率的乘积，代表了从前一个 shapelet 迁移到后一个 shapelet 的概率。</p>\n<p><img src=\"/asset/time2graph/transition.png\" alt=\"\" /></p>\n<p>最后，算法将一个 shapelet 所有迁移出去的 edge 的权重进行归一化，使其和为 1。</p>\n<p>算法的伪代码如下所示</p>\n<p><img src=\"/asset/time2graph/graph_algo.png\" alt=\"\" /></p>\n<h3 id=\"representation-learning\"><a class=\"markdownIt-Anchor\" href=\"#representation-learning\"></a> Representation Learning</h3>\n<p>最后，time2graph 采用了 DeepWalk <a href=\"#deepwalk\"><sup>3</sup></a>的算法，将 shapelet 转换为 embedding 的形式。</p>\n<p>接下来，就可以利用 shapelet 的 embedding 来表示 segment 和 time series。</p>\n<p>segment 的向量化表示如下面这个公式，就是其相关 shapelet 的 embedding 的概率权重和</p>\n<p><img src=\"/asset/time2graph/segment.png\" alt=\"\" /></p>\n<p>而 time series 的向量化表示就是将其所有 segment 的 embedding 的拼接。</p>\n<p>算法的伪代码如下所示</p>\n<p><img src=\"/asset/time2graph/representation.png\" alt=\"\" /></p>\n<h3 id=\"apply-time2graph\"><a class=\"markdownIt-Anchor\" href=\"#apply-time2graph\"></a> Apply Time2graph</h3>\n<p>至此，我们就完整的了解了 time2graph 的训练过程。那么 time2graph 的应用过程也与之类似。</p>\n<p>首先，算法的输入仍然是一个 time series。算法会先对其进行分段，然后为每个 segment 分配相关的 shapelet。接下来，就可以得到 segment 和 time series 的 embedding，进行后续聚类或者其他操作。</p>\n<h2 id=\"reference\"><a class=\"markdownIt-Anchor\" href=\"#reference\"></a> Reference</h2>\n<div id=\"shapelet\" />\n- [1] Ye, Lexiang, and Eamonn Keogh. \"Time series shapelets: a new primitive for data mining.\" Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 2009.\n<div id=\"time2graph\" />\n- [2] Cheng, Ziqiang, et al. \"Time2graph: Revisiting time series modeling with dynamic shapelets.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.\n<div id=\"deepwalk\" />\n- [3] Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. \"Deepwalk: Online learning of social representations.\" Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 2014."}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"clapldfcs000118mzf5909wet","tag_id":"clapldfcz000418mz3vp16naa","_id":"clapldfd9000d18mz7tw7fw2x"},{"post_id":"clapldfcs000118mzf5909wet","tag_id":"clapldfd4000818mzdsune5jr","_id":"clapldfdb000f18mz8hyk5y50"},{"post_id":"clapldfcx000318mz6oi88rwv","tag_id":"clapldfcz000418mz3vp16naa","_id":"clapldfdf000k18mz27ice940"},{"post_id":"clapldfcx000318mz6oi88rwv","tag_id":"clapldfdb000g18mz3xpc70pc","_id":"clapldfdf000l18mzczwmbmez"},{"post_id":"clapldfd1000518mze6l9181s","tag_id":"clapldfcz000418mz3vp16naa","_id":"clapldfdg000o18mz4u2j4eag"},{"post_id":"clapldfd1000518mze6l9181s","tag_id":"clapldfdf000m18mzghijcmhb","_id":"clapldfdg000p18mz5flr23bv"},{"post_id":"clapldfd2000618mz6c7j01za","tag_id":"clapldfcz000418mz3vp16naa","_id":"clapldfdh000q18mzairt5hym"},{"post_id":"clapldfd2000618mz6c7j01za","tag_id":"clapldfdg000n18mz7a5z88b4","_id":"clapldfdh000r18mzcqj7gh54"}],"Tag":[{"name":"weekly","_id":"clapldfcz000418mz3vp16naa"},{"name":"2021","_id":"clapldfd4000818mzdsune5jr"},{"name":"2022-00","_id":"clapldfdb000g18mz3xpc70pc"},{"name":"2022-01","_id":"clapldfdf000m18mzghijcmhb"},{"name":"2022-02","_id":"clapldfdg000n18mz7a5z88b4"}]}}