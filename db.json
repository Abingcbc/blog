{"meta":{"version":1,"warehouse":"4.0.1"},"models":{"Asset":[{"_id":"themes/one-paper/source/css/a11y-dark.min.css","path":"css/a11y-dark.min.css","modified":1,"renderable":1},{"_id":"themes/one-paper/source/css/fonts.css","path":"css/fonts.css","modified":1,"renderable":1},{"_id":"themes/one-paper/source/css/markdown.css","path":"css/markdown.css","modified":1,"renderable":1},{"_id":"themes/one-paper/source/css/reset.css","path":"css/reset.css","modified":1,"renderable":1},{"_id":"themes/one-paper/source/css/style.css","path":"css/style.css","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600.woff","path":"fonts/montserrat-v23-latin-600.woff","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600.woff2","path":"fonts/montserrat-v23-latin-600.woff2","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600italic.woff","path":"fonts/montserrat-v23-latin-600italic.woff","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600italic.woff2","path":"fonts/montserrat-v23-latin-600italic.woff2","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-italic.woff","path":"fonts/montserrat-v23-latin-italic.woff","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-italic.woff2","path":"fonts/montserrat-v23-latin-italic.woff2","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-regular.woff","path":"fonts/montserrat-v23-latin-regular.woff","modified":1,"renderable":1},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-regular.woff2","path":"fonts/montserrat-v23-latin-regular.woff2","modified":1,"renderable":1},{"_id":"themes/one-paper/source/img/favicon.png","path":"img/favicon.png","modified":1,"renderable":1},{"_id":"themes/one-paper/source/img/one-paper.png","path":"img/one-paper.png","modified":1,"renderable":1},{"_id":"themes/one-paper/source/js/highlight.min.js","path":"js/highlight.min.js","modified":1,"renderable":1},{"_id":"themes/one-paper/source/js/highlightjs-line-numbers.js","path":"js/highlightjs-line-numbers.js","modified":1,"renderable":1},{"_id":"source/asset/gzh.jpeg","path":"asset/gzh.jpeg","modified":1,"renderable":0},{"_id":"source/asset/wechat.jpg","path":"asset/wechat.jpg","modified":1,"renderable":0},{"_id":"source/images/favicon.png","path":"images/favicon.png","modified":1,"renderable":0},{"_id":"source/images/logo.png","path":"images/logo.png","modified":1,"renderable":0},{"_id":"source/asset/casbin-ramp-up/2022-06-23-17-50-00-image.png","path":"asset/casbin-ramp-up/2022-06-23-17-50-00-image.png","modified":1,"renderable":0},{"_id":"source/asset/casbin-ramp-up/detail.png","path":"asset/casbin-ramp-up/detail.png","modified":1,"renderable":0},{"_id":"source/asset/casbin-ramp-up/overview.png","path":"asset/casbin-ramp-up/overview.png","modified":1,"renderable":0},{"_id":"source/asset/casbin-ramp-up/pml.png","path":"asset/casbin-ramp-up/pml.png","modified":1,"renderable":0},{"_id":"source/asset/ingress-nginx-bug-fix/demo1.png","path":"asset/ingress-nginx-bug-fix/demo1.png","modified":1,"renderable":0},{"_id":"source/asset/ingress-nginx-bug-fix/demo2.png","path":"asset/ingress-nginx-bug-fix/demo2.png","modified":1,"renderable":0},{"_id":"source/asset/ingress-nginx-bug-fix/error.png","path":"asset/ingress-nginx-bug-fix/error.png","modified":1,"renderable":0},{"_id":"source/asset/ingress-nginx-bug-fix/nginx-code.png","path":"asset/ingress-nginx-bug-fix/nginx-code.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/1.png","path":"asset/k8s-ramp-up/1.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/10.png","path":"asset/k8s-ramp-up/10.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/11.png","path":"asset/k8s-ramp-up/11.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/12.png","path":"asset/k8s-ramp-up/12.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/13.png","path":"asset/k8s-ramp-up/13.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/14.png","path":"asset/k8s-ramp-up/14.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/15.png","path":"asset/k8s-ramp-up/15.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/16.png","path":"asset/k8s-ramp-up/16.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/17.png","path":"asset/k8s-ramp-up/17.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/18.png","path":"asset/k8s-ramp-up/18.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/19.png","path":"asset/k8s-ramp-up/19.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/2.png","path":"asset/k8s-ramp-up/2.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/20.png","path":"asset/k8s-ramp-up/20.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/3.png","path":"asset/k8s-ramp-up/3.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/4.png","path":"asset/k8s-ramp-up/4.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/5.png","path":"asset/k8s-ramp-up/5.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/6.png","path":"asset/k8s-ramp-up/6.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/7.png","path":"asset/k8s-ramp-up/7.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/8.png","path":"asset/k8s-ramp-up/8.png","modified":1,"renderable":0},{"_id":"source/asset/k8s-ramp-up/9.png","path":"asset/k8s-ramp-up/9.png","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/1.jpg","path":"asset/spring_ipv6/1.jpg","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/2.jpg","path":"asset/spring_ipv6/2.jpg","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/3.jpg","path":"asset/spring_ipv6/3.jpg","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/4.jpg","path":"asset/spring_ipv6/4.jpg","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/5.jpg","path":"asset/spring_ipv6/5.jpg","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/6.jpg","path":"asset/spring_ipv6/6.jpg","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/7.jpg","path":"asset/spring_ipv6/7.jpg","modified":1,"renderable":0},{"_id":"source/asset/spring_ipv6/8.jpg","path":"asset/spring_ipv6/8.jpg","modified":1,"renderable":0},{"_id":"source/asset/tidb-lightning/1.png","path":"asset/tidb-lightning/1.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/dist_seg.png","path":"asset/time2graph/dist_seg.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/dist_series.png","path":"asset/time2graph/dist_series.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/dtw.png","path":"asset/time2graph/dtw.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/edge_weight.png","path":"asset/time2graph/edge_weight.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/graph.png","path":"asset/time2graph/graph.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/graph_algo.png","path":"asset/time2graph/graph_algo.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/loss_function.png","path":"asset/time2graph/loss_function.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/representation.png","path":"asset/time2graph/representation.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/segment.png","path":"asset/time2graph/segment.png","modified":1,"renderable":0},{"_id":"source/asset/time2graph/transition.png","path":"asset/time2graph/transition.png","modified":1,"renderable":0}],"Cache":[{"_id":"source/_posts/2021.md","hash":"0545709a27a989548b57de1fa6d25c9943289cf6","modified":1657634099033},{"_id":"source/_posts/Spring IPv6.md","hash":"b4adb3b6cbd5cc1c495a0989dab33cd62be7f3f7","modified":1657634099033},{"_id":"source/_posts/casbin-ramp-up.md","hash":"b12b4941756519fe6fe46c046cc8a8c72a15fee3","modified":1657634099033},{"_id":"source/_posts/ingress-nginx-bug-fix.md","hash":"1950e0dfb7868fd1f37ed7bd6f2e9cc858c690f9","modified":1657634099033},{"_id":"source/_posts/k8s-cronjob.md","hash":"dd7c5483287ea4507364f6a12456fea261e2ff74","modified":1657634099033},{"_id":"source/_posts/k8s-ramp-up.md","hash":"6bec53ebfdf62ad12bc6f14d9ab1e818603c49ca","modified":1657634099037},{"_id":"source/_posts/tidb-lightning.md","hash":"e7489c069ae87d361fabae3db5d3fc551dd5b742","modified":1657634099037},{"_id":"source/_posts/time2graph.md","hash":"b43727631ac7f026ffec630b32a296a7f83b19ff","modified":1657634099037},{"_id":"source/about/index.md","hash":"6e9df2520bea6f7be80a8346baae33e38508f1e8","modified":1657634099037},{"_id":"source/asset/gzh.jpeg","hash":"f4b4ef81c04cbb3db2b6c3cb98aa5b27bf7a51c8","modified":1657634099041},{"_id":"source/images/favicon.png","hash":"560e57ba077640c78fd41236652e7534816ec009","modified":1657634099065},{"_id":"source/links/index.md","hash":"0979ebfaa38310bc48f38a264a97eb80c2db38ac","modified":1657634099065},{"_id":"source/asset/casbin-ramp-up/2022-06-23-17-50-00-image.png","hash":"85dbee6e8a4e8a580796afdaca0d519c4b81b982","modified":1657634099037},{"_id":"source/asset/ingress-nginx-bug-fix/demo1.png","hash":"d1c15eebd8aa34dedff6d51450cc5a64d4279816","modified":1657634099041},{"_id":"source/asset/ingress-nginx-bug-fix/demo2.png","hash":"07e0faffc6e009107087727cd35b12c85b5881af","modified":1657634099041},{"_id":"source/asset/k8s-ramp-up/1.png","hash":"1145d712a9399a51a454f775f974e5862b44f98c","modified":1657634099045},{"_id":"source/asset/k8s-ramp-up/10.png","hash":"d3921df8340cdf5c5d9de6753980ed05dc4ecd06","modified":1657634099045},{"_id":"source/asset/k8s-ramp-up/11.png","hash":"49505e81f7ed4f201325cdca173f92402e1d694e","modified":1657634099045},{"_id":"source/asset/k8s-ramp-up/13.png","hash":"0e547f7ed382f42d554e2ddc2f56b1b55a35121a","modified":1657634099049},{"_id":"source/asset/k8s-ramp-up/14.png","hash":"236020ab61732ffb54c7f73d108627cab79e3780","modified":1657634099049},{"_id":"source/asset/k8s-ramp-up/15.png","hash":"13dfe7c37ce4ae14152f31b65cd78b98258417c8","modified":1657634099049},{"_id":"source/asset/k8s-ramp-up/16.png","hash":"bfc1602be4127162d5a27c53abcbe08565ed38a0","modified":1657634099049},{"_id":"source/asset/k8s-ramp-up/17.png","hash":"7fdada3095564db64c1a5766ebe60400040185e1","modified":1657634099053},{"_id":"source/asset/k8s-ramp-up/18.png","hash":"7097617a943f79f44293b875bd380d0343f7a1ed","modified":1657634099053},{"_id":"source/asset/k8s-ramp-up/19.png","hash":"f190e9ea5ce21409f6aa6acf21698cedf1296f16","modified":1657634099053},{"_id":"source/asset/k8s-ramp-up/2.png","hash":"67564259b9f4cc49111db67cf344e3c315220d04","modified":1657634099053},{"_id":"source/asset/k8s-ramp-up/20.png","hash":"2b79e55f2b4022ab98679f6cc9ce315cc93c82d4","modified":1657634099053},{"_id":"source/asset/k8s-ramp-up/3.png","hash":"9b61eafd03c74e9f1581294ae784e3b388a43060","modified":1657634099053},{"_id":"source/asset/k8s-ramp-up/7.png","hash":"3039d3abcb057f70af3f0544990973af1d32028b","modified":1657634099057},{"_id":"source/asset/k8s-ramp-up/8.png","hash":"fbd07cba2115dc1eec09d47d8206212f71c5739c","modified":1657634099057},{"_id":"source/asset/k8s-ramp-up/9.png","hash":"f2a631454ba533aace37b5434455bc67e3245010","modified":1657634099057},{"_id":"source/asset/spring_ipv6/1.jpg","hash":"906e5182a059d051b7ffab73fcfe166b28b31dbe","modified":1657634099057},{"_id":"source/asset/spring_ipv6/2.jpg","hash":"c18e62cbf298d52caa5b0d4faf4d9a692ed6da35","modified":1657634099057},{"_id":"source/asset/spring_ipv6/3.jpg","hash":"bbbcd41f80a39c98639a646aade9d0bda86eda8b","modified":1657634099057},{"_id":"source/asset/spring_ipv6/4.jpg","hash":"352ccca707eeb3283ca4ac10d527957e7857abae","modified":1657634099057},{"_id":"source/asset/spring_ipv6/5.jpg","hash":"ef9044806637a7c8859d68c3a6278759d80d1fac","modified":1657634099057},{"_id":"source/asset/tidb-lightning/1.png","hash":"a9ea9c9a6539847d38a68e77d82c6ce1f3fec0d4","modified":1657634099061},{"_id":"source/asset/time2graph/dist_seg.png","hash":"f193d5d4793b8661a110a33741cf6d5e378011a3","modified":1657634099061},{"_id":"source/asset/time2graph/dist_series.png","hash":"0e899b88e99c163dd7ea2f0a649e0aaee56c99eb","modified":1657634099061},{"_id":"source/asset/time2graph/edge_weight.png","hash":"f513a48ad11a011510687d036da37b4501f7074e","modified":1657634099061},{"_id":"source/asset/time2graph/loss_function.png","hash":"f193d5d4793b8661a110a33741cf6d5e378011a3","modified":1657634099061},{"_id":"source/asset/time2graph/segment.png","hash":"320e2b64f2caa5168692778a6b87c72eb12d8964","modified":1657634099061},{"_id":"themes/one-paper/LICENSE","hash":"aad1dcb7deccd18a89508fa2ad78101dafa10cc9","modified":1657634099081},{"_id":"themes/one-paper/README.md","hash":"39b4a0d2b37a27f5d247339b8e851ed4d89e0851","modified":1657634099081},{"_id":"themes/one-paper/_config.yml","hash":"23d1ee5eb17a24f1da14b0da3b5bec73e0c6011c","modified":1657634099081},{"_id":"themes/one-paper/layout/index.ejs","hash":"2efe70d35a8be1e8431fd70942fd9c6908b0f1b8","modified":1657634099081},{"_id":"themes/one-paper/layout/index1.ejs","hash":"c9f87f50f9e97761aa2334fe47e427b97d811836","modified":1657634099081},{"_id":"themes/one-paper/layout/layout.ejs","hash":"68d1bb31fccbe9810b9c8a6cd54b53938e0318a2","modified":1657634099081},{"_id":"themes/one-paper/layout/post.ejs","hash":"8fc34f7d71eac6690e4e2b99088c7b1760f6d250","modified":1657634099081},{"_id":"themes/one-paper/layout/_partial/footer.ejs","hash":"87498f5131b82e48e48a7ed05e35490c6cdd30c1","modified":1657634099081},{"_id":"themes/one-paper/layout/_partial/head.ejs","hash":"9a5bf16974fd66f6d790662858f9d7c936bb7e0c","modified":1657634099081},{"_id":"themes/one-paper/layout/_partial/header.ejs","hash":"2b54259f35942eeb1b99482c2a4517152fc87f0c","modified":1657634099081},{"_id":"themes/one-paper/layout/_partial/paginator.ejs","hash":"980bf0a0be798c19df4b0827aa4b90b35c872425","modified":1657634099081},{"_id":"themes/one-paper/layout/_partial/post-header.ejs","hash":"f6fa471122459b25a9948405d72cb5f197d2fc25","modified":1657634099081},{"_id":"themes/one-paper/source/css/a11y-dark.min.css","hash":"e0a3294faa7dfa1eae300caea5a01f438b643b93","modified":1657634099081},{"_id":"themes/one-paper/source/css/fonts.css","hash":"bd6171c8de8d9f4efafca3802c4d20099d7fca1c","modified":1657634099081},{"_id":"themes/one-paper/source/css/markdown.css","hash":"8116b5049847ca524b99e10a501b945dcff0f29d","modified":1657634099081},{"_id":"themes/one-paper/source/css/reset.css","hash":"f6184d3f74dc704f077ca4e0b91003652a9db978","modified":1657634099081},{"_id":"themes/one-paper/source/css/style.css","hash":"1fc1cf8671c8f2a50aea2e168a712401d02b4902","modified":1657634099081},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600.woff","hash":"925d9f095488dc77dd84e8414422f0113f4628a9","modified":1657634099081},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600.woff2","hash":"2fe30978041c41a2994ac0fd491e83d32a3203b7","modified":1657634099081},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600italic.woff","hash":"c0e80c18fac1cd10469c4f922ad92e81fc8b3b94","modified":1657634099081},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-600italic.woff2","hash":"1f24e9edcccd42d4694a4020d6a8f9b9cb28f471","modified":1657634099081},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-italic.woff","hash":"fca5ee87a17c57eb53265da1c2c75db7305ad69c","modified":1657634099081},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-regular.woff","hash":"285adda1da1fc15583ad53160d66032aeccb45ea","modified":1657634099081},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-regular.woff2","hash":"f7eefafb7bfdc6b5572714fa267268b845a67cf4","modified":1657634099081},{"_id":"themes/one-paper/source/img/favicon.png","hash":"0845678601e8b144ae45c448a25650f4d3d2182d","modified":1657634099081},{"_id":"themes/one-paper/source/js/highlightjs-line-numbers.js","hash":"690e96133591495fa847d828573bd0576b2d168a","modified":1657634099085},{"_id":"source/images/logo.png","hash":"e592e037815d3557acb05c4d64156dcf2d100b11","modified":1657634099065},{"_id":"themes/one-paper/source/fonts/montserrat-v23-latin-italic.woff2","hash":"ce1eae3f714702a82c1e9c05b5ba302a9e91ac20","modified":1657634099081},{"_id":"source/asset/casbin-ramp-up/pml.png","hash":"67b4266d1be6921cb896a25476ad95706dbf180b","modified":1657634099041},{"_id":"source/asset/spring_ipv6/8.jpg","hash":"071e4d22f20b22683d78ca8b328484e239f791c2","modified":1657634099057},{"_id":"source/asset/time2graph/dtw.png","hash":"8a4ae6f36b3c32b2093159e15484b87e9a3f3897","modified":1657634099061},{"_id":"source/asset/time2graph/graph_algo.png","hash":"59b2cb4afea4e79669eb16389f89ec5d82a775c9","modified":1657634099061},{"_id":"source/asset/time2graph/representation.png","hash":"cc40893416b246fb57a65fe6a338aa310f05df73","modified":1657634099061},{"_id":"source/asset/time2graph/transition.png","hash":"d942684dfc94087b7131fa3ccefffce571fcec62","modified":1657634099061},{"_id":"source/asset/spring_ipv6/7.jpg","hash":"3d111ed99d1fa8bbcb514391c34543693b023d56","modified":1657634099057},{"_id":"source/asset/spring_ipv6/6.jpg","hash":"b8aafcca7247b087cdc9e0882c698738f95c21eb","modified":1657634099057},{"_id":"themes/one-paper/source/js/highlight.min.js","hash":"d264ad16bdf39cfec2b06c20223b87fcb37ad27b","modified":1657634099085},{"_id":"source/asset/wechat.jpg","hash":"6bc1d7fab98a60524d136f1387805d233fc2ad53","modified":1657634099065},{"_id":"source/asset/k8s-ramp-up/4.png","hash":"c9a7ff67e06993bc632029af641f27bfe8409796","modified":1657634099053},{"_id":"source/asset/casbin-ramp-up/overview.png","hash":"3f0f7dc338aa80ffed952127f7bb876db21f63a7","modified":1657634099037},{"_id":"source/asset/time2graph/graph.png","hash":"f82e9eef1608afb0116be9358690665a1cd32fef","modified":1657634099061},{"_id":"source/asset/k8s-ramp-up/6.png","hash":"3718c5ba7b63809eccfb9c92ffd3063b8fbd0309","modified":1657634099057},{"_id":"source/asset/ingress-nginx-bug-fix/error.png","hash":"5816144a23ecd7690a25adc2492973eb920ed1e9","modified":1657634099041},{"_id":"source/asset/ingress-nginx-bug-fix/nginx-code.png","hash":"67bdbe585956405d3780bad1b08e686b824c5321","modified":1657634099045},{"_id":"source/asset/k8s-ramp-up/5.png","hash":"5187e67252cf14c89b7397833ad2618c6b89421b","modified":1657634099057},{"_id":"source/asset/casbin-ramp-up/detail.png","hash":"d6348e860505ce70f269432e2ca8a03079453069","modified":1657634099037},{"_id":"themes/one-paper/source/img/one-paper.png","hash":"eadd349e5316a154099cb06e41abe5a105940e37","modified":1657634099085},{"_id":"source/asset/k8s-ramp-up/12.png","hash":"c873022232968a954dfd899a7d717c155615c71e","modified":1657634099049},{"_id":"public/sitemap.xml","hash":"42e02ee7b35886c682565985e145140bbd9acb1a","modified":1657634128266},{"_id":"public/sitemap.txt","hash":"61d2a584483586104521a70fc758bbe2dee3bbe0","modified":1657634128266},{"_id":"public/submit_urls.txt","hash":"0a891234cb522430e53e31c5cb3c87589a373920","modified":1657634128266},{"_id":"public/about/index.html","hash":"0a42a1d92e6f48f4e70d5235f8363db96148b3b3","modified":1657634128266},{"_id":"public/links/index.html","hash":"e123646e5a883754a1aa4d0e70ad3bb8783deb18","modified":1657634128266},{"_id":"public/2022/06/01/2021/index.html","hash":"f6375f7eda3c3ccabe25017ee5d7ddccc13686a8","modified":1657634128266},{"_id":"public/2021/09/06/time2graph/index.html","hash":"b2a20ce5da53275967e686e3c0c8692925193b4b","modified":1657634128266},{"_id":"public/2021/03/19/tidb-lightning/index.html","hash":"a58613f31cf98980238f1273fa57269a1054615a","modified":1657634128266},{"_id":"public/2021/03/13/ingress-nginx-bug-fix/index.html","hash":"5a08a77543c06dd9a082fac771a457fb6f4b6a42","modified":1657634128266},{"_id":"public/2021/03/08/k8s-cronjob/index.html","hash":"0a5bbe895b4e6d05cc8f44c1dee8355519f5dd3a","modified":1657634128266},{"_id":"public/2021/03/07/Spring IPv6/index.html","hash":"b8a87adca1ee272ee46086ac75dd1efab567eb1c","modified":1657634128266},{"_id":"public/archives/index.html","hash":"08dac2e83470b74b6a913d2c456279b7e3ce54e6","modified":1657634128266},{"_id":"public/archives/2021/index.html","hash":"4741ca2aaf64a9dcf663b0e62409e005aef65a7e","modified":1657634128266},{"_id":"public/archives/2021/03/index.html","hash":"9b73b4905f18cbb5d960377d7908458c790b720e","modified":1657634128266},{"_id":"public/archives/2021/09/index.html","hash":"9ba9fd2d7ed69ca18082309fdd58545f9e8e8585","modified":1657634128266},{"_id":"public/archives/2022/index.html","hash":"4edaeb2013a3745fd27107d67e7da0d0abcb429e","modified":1657634128266},{"_id":"public/archives/2022/06/index.html","hash":"4edaeb2013a3745fd27107d67e7da0d0abcb429e","modified":1657634128266},{"_id":"public/index.html","hash":"08dac2e83470b74b6a913d2c456279b7e3ce54e6","modified":1657634128266},{"_id":"public/2022/06/22/casbin-ramp-up/index.html","hash":"cc95bd596a5556dbd13059af9e5ad20b52a19fb6","modified":1657634128266},{"_id":"public/2021/03/15/k8s-ramp-up/index.html","hash":"49d78013a73fe18d187564e5d0db43a2b6c885e4","modified":1657634128266},{"_id":"public/fonts/montserrat-v23-latin-600.woff","hash":"925d9f095488dc77dd84e8414422f0113f4628a9","modified":1657634128266},{"_id":"public/fonts/montserrat-v23-latin-600.woff2","hash":"2fe30978041c41a2994ac0fd491e83d32a3203b7","modified":1657634128266},{"_id":"public/fonts/montserrat-v23-latin-600italic.woff","hash":"c0e80c18fac1cd10469c4f922ad92e81fc8b3b94","modified":1657634128266},{"_id":"public/fonts/montserrat-v23-latin-600italic.woff2","hash":"1f24e9edcccd42d4694a4020d6a8f9b9cb28f471","modified":1657634128266},{"_id":"public/fonts/montserrat-v23-latin-italic.woff","hash":"fca5ee87a17c57eb53265da1c2c75db7305ad69c","modified":1657634128266},{"_id":"public/fonts/montserrat-v23-latin-italic.woff2","hash":"ce1eae3f714702a82c1e9c05b5ba302a9e91ac20","modified":1657634128266},{"_id":"public/fonts/montserrat-v23-latin-regular.woff","hash":"285adda1da1fc15583ad53160d66032aeccb45ea","modified":1657634128266},{"_id":"public/fonts/montserrat-v23-latin-regular.woff2","hash":"f7eefafb7bfdc6b5572714fa267268b845a67cf4","modified":1657634128266},{"_id":"public/img/favicon.png","hash":"0845678601e8b144ae45c448a25650f4d3d2182d","modified":1657634128266},{"_id":"public/asset/gzh.jpeg","hash":"f4b4ef81c04cbb3db2b6c3cb98aa5b27bf7a51c8","modified":1657634128266},{"_id":"public/images/favicon.png","hash":"560e57ba077640c78fd41236652e7534816ec009","modified":1657634128266},{"_id":"public/asset/casbin-ramp-up/2022-06-23-17-50-00-image.png","hash":"85dbee6e8a4e8a580796afdaca0d519c4b81b982","modified":1657634128266},{"_id":"public/asset/ingress-nginx-bug-fix/demo1.png","hash":"d1c15eebd8aa34dedff6d51450cc5a64d4279816","modified":1657634128266},{"_id":"public/asset/ingress-nginx-bug-fix/demo2.png","hash":"07e0faffc6e009107087727cd35b12c85b5881af","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/1.png","hash":"1145d712a9399a51a454f775f974e5862b44f98c","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/10.png","hash":"d3921df8340cdf5c5d9de6753980ed05dc4ecd06","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/11.png","hash":"49505e81f7ed4f201325cdca173f92402e1d694e","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/13.png","hash":"0e547f7ed382f42d554e2ddc2f56b1b55a35121a","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/14.png","hash":"236020ab61732ffb54c7f73d108627cab79e3780","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/16.png","hash":"bfc1602be4127162d5a27c53abcbe08565ed38a0","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/17.png","hash":"7fdada3095564db64c1a5766ebe60400040185e1","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/18.png","hash":"7097617a943f79f44293b875bd380d0343f7a1ed","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/19.png","hash":"f190e9ea5ce21409f6aa6acf21698cedf1296f16","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/2.png","hash":"67564259b9f4cc49111db67cf344e3c315220d04","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/15.png","hash":"13dfe7c37ce4ae14152f31b65cd78b98258417c8","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/7.png","hash":"3039d3abcb057f70af3f0544990973af1d32028b","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/8.png","hash":"fbd07cba2115dc1eec09d47d8206212f71c5739c","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/9.png","hash":"f2a631454ba533aace37b5434455bc67e3245010","modified":1657634128266},{"_id":"public/asset/spring_ipv6/1.jpg","hash":"906e5182a059d051b7ffab73fcfe166b28b31dbe","modified":1657634128266},{"_id":"public/asset/spring_ipv6/2.jpg","hash":"c18e62cbf298d52caa5b0d4faf4d9a692ed6da35","modified":1657634128266},{"_id":"public/asset/spring_ipv6/3.jpg","hash":"bbbcd41f80a39c98639a646aade9d0bda86eda8b","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/3.png","hash":"9b61eafd03c74e9f1581294ae784e3b388a43060","modified":1657634128266},{"_id":"public/asset/spring_ipv6/4.jpg","hash":"352ccca707eeb3283ca4ac10d527957e7857abae","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/20.png","hash":"2b79e55f2b4022ab98679f6cc9ce315cc93c82d4","modified":1657634128266},{"_id":"public/asset/spring_ipv6/5.jpg","hash":"ef9044806637a7c8859d68c3a6278759d80d1fac","modified":1657634128266},{"_id":"public/asset/spring_ipv6/6.jpg","hash":"b8aafcca7247b087cdc9e0882c698738f95c21eb","modified":1657634128266},{"_id":"public/asset/spring_ipv6/7.jpg","hash":"3d111ed99d1fa8bbcb514391c34543693b023d56","modified":1657634128266},{"_id":"public/asset/spring_ipv6/8.jpg","hash":"071e4d22f20b22683d78ca8b328484e239f791c2","modified":1657634128266},{"_id":"public/asset/tidb-lightning/1.png","hash":"a9ea9c9a6539847d38a68e77d82c6ce1f3fec0d4","modified":1657634128266},{"_id":"public/asset/time2graph/dist_seg.png","hash":"f193d5d4793b8661a110a33741cf6d5e378011a3","modified":1657634128266},{"_id":"public/asset/time2graph/dist_series.png","hash":"0e899b88e99c163dd7ea2f0a649e0aaee56c99eb","modified":1657634128266},{"_id":"public/asset/time2graph/edge_weight.png","hash":"f513a48ad11a011510687d036da37b4501f7074e","modified":1657634128266},{"_id":"public/asset/time2graph/loss_function.png","hash":"f193d5d4793b8661a110a33741cf6d5e378011a3","modified":1657634128266},{"_id":"public/asset/time2graph/segment.png","hash":"320e2b64f2caa5168692778a6b87c72eb12d8964","modified":1657634128266},{"_id":"public/images/logo.png","hash":"e592e037815d3557acb05c4d64156dcf2d100b11","modified":1657634128266},{"_id":"public/asset/casbin-ramp-up/pml.png","hash":"67b4266d1be6921cb896a25476ad95706dbf180b","modified":1657634128266},{"_id":"public/asset/time2graph/dtw.png","hash":"8a4ae6f36b3c32b2093159e15484b87e9a3f3897","modified":1657634128266},{"_id":"public/asset/time2graph/graph_algo.png","hash":"59b2cb4afea4e79669eb16389f89ec5d82a775c9","modified":1657634128266},{"_id":"public/asset/time2graph/representation.png","hash":"cc40893416b246fb57a65fe6a338aa310f05df73","modified":1657634128266},{"_id":"public/asset/time2graph/transition.png","hash":"d942684dfc94087b7131fa3ccefffce571fcec62","modified":1657634128266},{"_id":"public/css/a11y-dark.min.css","hash":"e0a3294faa7dfa1eae300caea5a01f438b643b93","modified":1657634128266},{"_id":"public/css/fonts.css","hash":"bd6171c8de8d9f4efafca3802c4d20099d7fca1c","modified":1657634128266},{"_id":"public/css/markdown.css","hash":"8116b5049847ca524b99e10a501b945dcff0f29d","modified":1657634128266},{"_id":"public/css/reset.css","hash":"f6184d3f74dc704f077ca4e0b91003652a9db978","modified":1657634128266},{"_id":"public/css/style.css","hash":"1fc1cf8671c8f2a50aea2e168a712401d02b4902","modified":1657634128266},{"_id":"public/js/highlightjs-line-numbers.js","hash":"690e96133591495fa847d828573bd0576b2d168a","modified":1657634128266},{"_id":"public/js/highlight.min.js","hash":"d264ad16bdf39cfec2b06c20223b87fcb37ad27b","modified":1657634128266},{"_id":"public/asset/wechat.jpg","hash":"6bc1d7fab98a60524d136f1387805d233fc2ad53","modified":1657634128266},{"_id":"public/asset/casbin-ramp-up/overview.png","hash":"3f0f7dc338aa80ffed952127f7bb876db21f63a7","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/4.png","hash":"c9a7ff67e06993bc632029af641f27bfe8409796","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/6.png","hash":"3718c5ba7b63809eccfb9c92ffd3063b8fbd0309","modified":1657634128266},{"_id":"public/asset/time2graph/graph.png","hash":"f82e9eef1608afb0116be9358690665a1cd32fef","modified":1657634128266},{"_id":"public/asset/ingress-nginx-bug-fix/error.png","hash":"5816144a23ecd7690a25adc2492973eb920ed1e9","modified":1657634128266},{"_id":"public/asset/ingress-nginx-bug-fix/nginx-code.png","hash":"67bdbe585956405d3780bad1b08e686b824c5321","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/5.png","hash":"5187e67252cf14c89b7397833ad2618c6b89421b","modified":1657634128266},{"_id":"public/img/one-paper.png","hash":"eadd349e5316a154099cb06e41abe5a105940e37","modified":1657634128266},{"_id":"public/asset/casbin-ramp-up/detail.png","hash":"d6348e860505ce70f269432e2ca8a03079453069","modified":1657634128266},{"_id":"public/asset/k8s-ramp-up/12.png","hash":"c873022232968a954dfd899a7d717c155615c71e","modified":1657634128266}],"Category":[],"Data":[],"Page":[{"title":"ABOUT","_content":"[<img src=\"https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white\" alt=\"https://github.com/Abingcbc\" style=\"display:inline\">](https://github.com/Abingcbc) [<img src=\"https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&amp;logo=gmail&amp;logoColor=white\" alt=\"mailto:abingcbc626@gmail.com\" style=\"display:inline\">](mailto:abingcbc626@gmail.com) [<img src=\"https://img.shields.io/badge/linkedin-%230077B5.svg?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white\" alt=\"https://www.linkedin.com/in/bingchang-chen-8b3812183/\" style=\"display:inline\">](https://www.linkedin.com/in/bingchang-chen-8b3812183/) [<img src=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" alt=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" style=\"display:inline\">](https://www.zhihu.com/people/llll-48-29)\n<hr>\n\n### 正在寻找2023年暑期实习 ❤️\n### Open for 2023 Summer Internship ❤️\n\n<hr>\n\n本科就读于同济大学软件学院，目前硕士就读于同济大学设计与创意学院人工智能与数据设计专业 IDvX 实验室 https://idvxlab.com/。\n\n#### 公众号\n\n<img src=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" alt=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" style=\"width:300px;margin:0 auto\">\n\n#### Misc\n\n- ISFP 懒狗\n- 尼康党\n- 轻度二次元 (巨人, 鬼灭, JoJo...)\n- 塞尔达YYDS\n\n","source":"about/index.md","raw":"---\ntitle: ABOUT\n---\n[<img src=\"https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white\" alt=\"https://github.com/Abingcbc\" style=\"display:inline\">](https://github.com/Abingcbc) [<img src=\"https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&amp;logo=gmail&amp;logoColor=white\" alt=\"mailto:abingcbc626@gmail.com\" style=\"display:inline\">](mailto:abingcbc626@gmail.com) [<img src=\"https://img.shields.io/badge/linkedin-%230077B5.svg?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white\" alt=\"https://www.linkedin.com/in/bingchang-chen-8b3812183/\" style=\"display:inline\">](https://www.linkedin.com/in/bingchang-chen-8b3812183/) [<img src=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" alt=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" style=\"display:inline\">](https://www.zhihu.com/people/llll-48-29)\n<hr>\n\n### 正在寻找2023年暑期实习 ❤️\n### Open for 2023 Summer Internship ❤️\n\n<hr>\n\n本科就读于同济大学软件学院，目前硕士就读于同济大学设计与创意学院人工智能与数据设计专业 IDvX 实验室 https://idvxlab.com/。\n\n#### 公众号\n\n<img src=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" alt=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" style=\"width:300px;margin:0 auto\">\n\n#### Misc\n\n- ISFP 懒狗\n- 尼康党\n- 轻度二次元 (巨人, 鬼灭, JoJo...)\n- 塞尔达YYDS\n\n","date":"2022-07-12T13:54:59.037Z","updated":"2022-07-12T13:54:59.037Z","path":"about/index.html","comments":1,"layout":"page","_id":"cl5i8k7xr00001rpf2nv352pg","content":"<p><a href=\"https://github.com/Abingcbc\"><img src=\"https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white\" alt=\"https://github.com/Abingcbc\" style=\"display:inline\"></a> <a href=\"mailto:abingcbc626@gmail.com\"><img src=\"https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&amp;logo=gmail&amp;logoColor=white\" alt=\"mailto:abingcbc626@gmail.com\" style=\"display:inline\"></a> <a href=\"https://www.linkedin.com/in/bingchang-chen-8b3812183/\"><img src=\"https://img.shields.io/badge/linkedin-%230077B5.svg?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white\" alt=\"https://www.linkedin.com/in/bingchang-chen-8b3812183/\" style=\"display:inline\"></a> <a href=\"https://www.zhihu.com/people/llll-48-29\"><img src=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" alt=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" style=\"display:inline\"></a></p>\n<hr>\n\n<h3 id=\"正在寻找2023年暑期实习-❤️\"><a href=\"#正在寻找2023年暑期实习-❤️\" class=\"headerlink\" title=\"正在寻找2023年暑期实习 ❤️\"></a>正在寻找2023年暑期实习 ❤️</h3><h3 id=\"Open-for-2023-Summer-Internship-❤️\"><a href=\"#Open-for-2023-Summer-Internship-❤️\" class=\"headerlink\" title=\"Open for 2023 Summer Internship ❤️\"></a>Open for 2023 Summer Internship ❤️</h3><hr>\n\n<p>本科就读于同济大学软件学院，目前硕士就读于同济大学设计与创意学院人工智能与数据设计专业 IDvX 实验室 <a href=\"https://idvxlab.com/%E3%80%82\">https://idvxlab.com/。</a></p>\n<h4 id=\"公众号\"><a href=\"#公众号\" class=\"headerlink\" title=\"公众号\"></a>公众号</h4><img src=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" alt=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" style=\"width:300px;margin:0 auto\">\n\n<h4 id=\"Misc\"><a href=\"#Misc\" class=\"headerlink\" title=\"Misc\"></a>Misc</h4><ul>\n<li>ISFP 懒狗</li>\n<li>尼康党</li>\n<li>轻度二次元 (巨人, 鬼灭, JoJo…)</li>\n<li>塞尔达YYDS</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://github.com/Abingcbc\"><img src=\"https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white\" alt=\"https://github.com/Abingcbc\" style=\"display:inline\"></a> <a href=\"mailto:abingcbc626@gmail.com\"><img src=\"https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&amp;logo=gmail&amp;logoColor=white\" alt=\"mailto:abingcbc626@gmail.com\" style=\"display:inline\"></a> <a href=\"https://www.linkedin.com/in/bingchang-chen-8b3812183/\"><img src=\"https://img.shields.io/badge/linkedin-%230077B5.svg?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white\" alt=\"https://www.linkedin.com/in/bingchang-chen-8b3812183/\" style=\"display:inline\"></a> <a href=\"https://www.zhihu.com/people/llll-48-29\"><img src=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" alt=\"https://img.shields.io/badge/zhihu-0078D4.svg?&style=for-the-badge\" style=\"display:inline\"></a></p>\n<hr>\n\n<h3 id=\"正在寻找2023年暑期实习-❤️\"><a href=\"#正在寻找2023年暑期实习-❤️\" class=\"headerlink\" title=\"正在寻找2023年暑期实习 ❤️\"></a>正在寻找2023年暑期实习 ❤️</h3><h3 id=\"Open-for-2023-Summer-Internship-❤️\"><a href=\"#Open-for-2023-Summer-Internship-❤️\" class=\"headerlink\" title=\"Open for 2023 Summer Internship ❤️\"></a>Open for 2023 Summer Internship ❤️</h3><hr>\n\n<p>本科就读于同济大学软件学院，目前硕士就读于同济大学设计与创意学院人工智能与数据设计专业 IDvX 实验室 <a href=\"https://idvxlab.com/%E3%80%82\">https://idvxlab.com/。</a></p>\n<h4 id=\"公众号\"><a href=\"#公众号\" class=\"headerlink\" title=\"公众号\"></a>公众号</h4><img src=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" alt=\"https://blog.abingcbc.cn/asset/gzh.jpeg\" style=\"width:300px;margin:0 auto\">\n\n<h4 id=\"Misc\"><a href=\"#Misc\" class=\"headerlink\" title=\"Misc\"></a>Misc</h4><ul>\n<li>ISFP 懒狗</li>\n<li>尼康党</li>\n<li>轻度二次元 (巨人, 鬼灭, JoJo…)</li>\n<li>塞尔达YYDS</li>\n</ul>\n"},{"title":"友情链接","_content":"## 友情链接\n\n🐈 [卡拉云低代码工具](https://kalacloud.com)","source":"links/index.md","raw":"title: 友情链接\n---\n## 友情链接\n\n🐈 [卡拉云低代码工具](https://kalacloud.com)","date":"2022-07-12T13:54:59.065Z","updated":"2022-07-12T13:54:59.065Z","path":"links/index.html","comments":1,"layout":"page","_id":"cl5i8k7xz00021rpfdzk09lu1","content":"<h2 id=\"友情链接\"><a href=\"#友情链接\" class=\"headerlink\" title=\"友情链接\"></a>友情链接</h2><p>🐈 <a href=\"https://kalacloud.com/\">卡拉云低代码工具</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"友情链接\"><a href=\"#友情链接\" class=\"headerlink\" title=\"友情链接\"></a>友情链接</h2><p>🐈 <a href=\"https://kalacloud.com/\">卡拉云低代码工具</a></p>\n"}],"Post":[{"title":"2021 年终总结 —— 翻天覆地","date":"2022-06-01T00:19:16.000Z","updated":"2022-06-01T00:19:16.000Z","_content":"2021年这一年的时间里，发生了太多的事，如果不写下来的话，在脑海中只会有一些模糊的印象，不清楚自己取得了哪些成绩，又有哪里不尽人意，同时也可以从一个旁观者的视角，来观察自己这一年有没有虚度光阴。所以，就从2021年开始，开始每年对过去的一年进行一个记录吧~\n## 技术\n### K8s\n在年初的时候，阅读了《Kubernetes in Action》这本书，受益匪浅，再加上实习中的实践，对 K8s 的了解更加深入了。K8s 作为目前 Cloud 的实际代名词，在应用开发中已经是无法替代的存在了。作为云原生的基础设施，对于 K8s 的学习，除了掌握其基础的操作，例如 Deployment、Ingress 等等组件的使用，来满足上层的应用开发以外，对于 K8s 本身的学习，也是十分重要的。因为 K8s 本身就是一个庞大复杂的系统，而且经过无数业界大佬的打磨，系统上的设计无疑是很多问题的 best practice。深入理解 K8s 的设计，对于未来设计自己的应用架构肯定会带来很大的帮助。\n### 自动化部署\n我在实习期间负责的主要的一部分内容就是自动化部署。随着现在微服务化的趋势以及系统内第三方依赖组件的数量的增加，部署其实是一个非常麻烦的问题。在公有云的场景下，部分第三方组件或许可以通过外部服务的形式解决。但在私有云的情况下，部署需要同时部署这些第三方组件，无疑给部署增加了很大的复杂度。不仅仅如此，对于任何现代化的软件来说，部署都是需要进行标准化的。通过部署文档，类似手工作坊进行的部署方式，是无法持久的，人因的错误是整个部署过程中很大的不稳定因素。这方面也有很多做的比较好的开源项目，比如说 TiDB 的 TiUP 等等。\n### 可视化\n实话说，在正式进入硕士学习之前，我对于可视化的了解可能都比较浅薄（虽然明知道这是自己硕士的研究方向）。在听完大老板的课之后，对于这个领域有了全新的认知。可视化这个领域虽然很小，但并不是像大众认为的只有简单的图表而已。首先，图表本身的设计有着很多问题值得研究。设计可能是一个主观感性的行为，但我大老板的想法是通过design space等方式，将其转化为理性，有逻辑的行为。我对这一点非常认同，给了我一种茅塞顿开的感觉，让我对之前很多看似感性的事情有了新的理解。其次，可视分析也是可视化中非常重要的一部分。很多问题，通过可视化的形式，就可以很清楚和容易地被分析出来。这一类系统，感觉在 BI 领域应该有很大的发展空间。\n### Casbin\n一直以来想加入一个开源社区，为 developer 们做出一点点自己的贡献。于是通过一个活动加入到了 Casbin社区。Casbin 是一个权限校验框架，通过 well-defined 的 model 结构，支持对各种类型的校验模式（比如RBAC，ABAC等等）。除此之外，我觉得 Casbin 能够获得 10k+ star 的另一个原因是他的生态支持也太丰富了。各种语言，各种前端后端框架，各种数据库，只要是有需求，都会进行支持。其中另一个最重要的应该是 Casdoor，一个第三方统一身份认证框架，与 KeyCloak 相对标。\n\n这两个项目都非常有意义，Casbin 主要是像一个解析器一样，面对不同的 model 和 policy定义，需要提供正确的校验结果，我个人还挺喜欢慢慢推理，寻找哪一步解析错误的过程。而 Casdoor 则像一个业务系统，如何实现更多的 feature，如何提升易用性，与更多的第三方系统进行集成。\n### Other\n除此之前，这一年间也多多少少接触了很多其他技术。实习的时候无所事事，开始写起来了 Augular，体验了一次像 Java 一样的前端的开发模式；暑假的时候参加 GSoC，借机接触了一下 Rust，对这门语言产生了深深的敬畏，但可惜没能深入学习下去；开学以后接触了很多设计领域的coding，做了一些数字媒体类似的课程作业......\n## 生活\n上半年实习的过程中，感觉自己逐渐放开了。在实习期间遇到了很多伙伴，大家一起快乐摸鱼，真的是度过了一段很快乐的时光。最重要的是毕业啦！最高学历终于从高中变成了本科。虽然毕业旅行因为疫情原因没能成行，但暑假在家快乐摸鱼，疯狂锻炼猎龙技术，还差一点把塞尔达的呀哈哈全收集，也算快乐的度过了最后一个暑假了。\n\n如果说上半年是我人生最快乐的时光，那么下半年开学后，就是我目前为止最痛苦的日子了。但无论是前后哪个阶段，我其实都成长了非常多，也算是值得庆幸的事，能够在接受到社会正式的毒打之前，提前成长。\n\n一方面的压力来自于课程，毕竟我是属于跨专业保研到了设创，难免在研究生阶段要接触到设计相关的课程。在组队上，天真地以为老师是技术背景出身的，项目可能也会是，结果就是在课程项目上被设计背景的同学吊打。在 DDL 前疯狂爆肝，但是也换不来特别高的成绩。研究生课程中没能抱上设计大佬们的大腿，没体验一次被带飞的感觉，算是比较可惜的了。\n\n另一方面最大的压力来自于实验室。大老板的痛骂确实是有效的。做事不够深入的问题，在我之前的面试以及实习过程中，都多次被前辈们提到过，但都采用相对和蔼的语气和态度。我也是属实有点抖m了，这种不痛不痒的批评根本不往心里去，也没有发自内心地去纠正自己的问题，非要等到现在的大老板爆骂自己，自己才意识到问题。。。\n\n年末到22年初的状态确实不太好，似乎每年的这段时间都很emo。在年底DDL结束之后，自己的节奏一时间也没调整过来。闲下来以后，脑子昏昏沉沉，没了目标，搞不清楚自己真正想要什么，不讨人喜欢。最后也算造成了不可弥补的错误吧，非常可惜。不过，人各有命吧，也学习到了很多，前后相比，算是成熟了很多了。\n\n拖拖拉拉，写完这篇总结的时候，2022年都已经过去四分之一了。总的来说，2022年可以用“翻天覆地”这个词来概括，在这个痛苦的过程中，也见到了一个不一样的自己。虽然2022年的开端不算顺利，很失败，但这些问题就留到明年的总结中来写吧。最后，希望在2023年总结今年的时候，可以用一个更加开心的关键词吧。","source":"_posts/2021.md","raw":"---\ntitle: 2021 年终总结 —— 翻天覆地\ndate: 2022-06-01 00:19:16\nupdated: 2022-06-01 00:19:16\n---\n2021年这一年的时间里，发生了太多的事，如果不写下来的话，在脑海中只会有一些模糊的印象，不清楚自己取得了哪些成绩，又有哪里不尽人意，同时也可以从一个旁观者的视角，来观察自己这一年有没有虚度光阴。所以，就从2021年开始，开始每年对过去的一年进行一个记录吧~\n## 技术\n### K8s\n在年初的时候，阅读了《Kubernetes in Action》这本书，受益匪浅，再加上实习中的实践，对 K8s 的了解更加深入了。K8s 作为目前 Cloud 的实际代名词，在应用开发中已经是无法替代的存在了。作为云原生的基础设施，对于 K8s 的学习，除了掌握其基础的操作，例如 Deployment、Ingress 等等组件的使用，来满足上层的应用开发以外，对于 K8s 本身的学习，也是十分重要的。因为 K8s 本身就是一个庞大复杂的系统，而且经过无数业界大佬的打磨，系统上的设计无疑是很多问题的 best practice。深入理解 K8s 的设计，对于未来设计自己的应用架构肯定会带来很大的帮助。\n### 自动化部署\n我在实习期间负责的主要的一部分内容就是自动化部署。随着现在微服务化的趋势以及系统内第三方依赖组件的数量的增加，部署其实是一个非常麻烦的问题。在公有云的场景下，部分第三方组件或许可以通过外部服务的形式解决。但在私有云的情况下，部署需要同时部署这些第三方组件，无疑给部署增加了很大的复杂度。不仅仅如此，对于任何现代化的软件来说，部署都是需要进行标准化的。通过部署文档，类似手工作坊进行的部署方式，是无法持久的，人因的错误是整个部署过程中很大的不稳定因素。这方面也有很多做的比较好的开源项目，比如说 TiDB 的 TiUP 等等。\n### 可视化\n实话说，在正式进入硕士学习之前，我对于可视化的了解可能都比较浅薄（虽然明知道这是自己硕士的研究方向）。在听完大老板的课之后，对于这个领域有了全新的认知。可视化这个领域虽然很小，但并不是像大众认为的只有简单的图表而已。首先，图表本身的设计有着很多问题值得研究。设计可能是一个主观感性的行为，但我大老板的想法是通过design space等方式，将其转化为理性，有逻辑的行为。我对这一点非常认同，给了我一种茅塞顿开的感觉，让我对之前很多看似感性的事情有了新的理解。其次，可视分析也是可视化中非常重要的一部分。很多问题，通过可视化的形式，就可以很清楚和容易地被分析出来。这一类系统，感觉在 BI 领域应该有很大的发展空间。\n### Casbin\n一直以来想加入一个开源社区，为 developer 们做出一点点自己的贡献。于是通过一个活动加入到了 Casbin社区。Casbin 是一个权限校验框架，通过 well-defined 的 model 结构，支持对各种类型的校验模式（比如RBAC，ABAC等等）。除此之外，我觉得 Casbin 能够获得 10k+ star 的另一个原因是他的生态支持也太丰富了。各种语言，各种前端后端框架，各种数据库，只要是有需求，都会进行支持。其中另一个最重要的应该是 Casdoor，一个第三方统一身份认证框架，与 KeyCloak 相对标。\n\n这两个项目都非常有意义，Casbin 主要是像一个解析器一样，面对不同的 model 和 policy定义，需要提供正确的校验结果，我个人还挺喜欢慢慢推理，寻找哪一步解析错误的过程。而 Casdoor 则像一个业务系统，如何实现更多的 feature，如何提升易用性，与更多的第三方系统进行集成。\n### Other\n除此之前，这一年间也多多少少接触了很多其他技术。实习的时候无所事事，开始写起来了 Augular，体验了一次像 Java 一样的前端的开发模式；暑假的时候参加 GSoC，借机接触了一下 Rust，对这门语言产生了深深的敬畏，但可惜没能深入学习下去；开学以后接触了很多设计领域的coding，做了一些数字媒体类似的课程作业......\n## 生活\n上半年实习的过程中，感觉自己逐渐放开了。在实习期间遇到了很多伙伴，大家一起快乐摸鱼，真的是度过了一段很快乐的时光。最重要的是毕业啦！最高学历终于从高中变成了本科。虽然毕业旅行因为疫情原因没能成行，但暑假在家快乐摸鱼，疯狂锻炼猎龙技术，还差一点把塞尔达的呀哈哈全收集，也算快乐的度过了最后一个暑假了。\n\n如果说上半年是我人生最快乐的时光，那么下半年开学后，就是我目前为止最痛苦的日子了。但无论是前后哪个阶段，我其实都成长了非常多，也算是值得庆幸的事，能够在接受到社会正式的毒打之前，提前成长。\n\n一方面的压力来自于课程，毕竟我是属于跨专业保研到了设创，难免在研究生阶段要接触到设计相关的课程。在组队上，天真地以为老师是技术背景出身的，项目可能也会是，结果就是在课程项目上被设计背景的同学吊打。在 DDL 前疯狂爆肝，但是也换不来特别高的成绩。研究生课程中没能抱上设计大佬们的大腿，没体验一次被带飞的感觉，算是比较可惜的了。\n\n另一方面最大的压力来自于实验室。大老板的痛骂确实是有效的。做事不够深入的问题，在我之前的面试以及实习过程中，都多次被前辈们提到过，但都采用相对和蔼的语气和态度。我也是属实有点抖m了，这种不痛不痒的批评根本不往心里去，也没有发自内心地去纠正自己的问题，非要等到现在的大老板爆骂自己，自己才意识到问题。。。\n\n年末到22年初的状态确实不太好，似乎每年的这段时间都很emo。在年底DDL结束之后，自己的节奏一时间也没调整过来。闲下来以后，脑子昏昏沉沉，没了目标，搞不清楚自己真正想要什么，不讨人喜欢。最后也算造成了不可弥补的错误吧，非常可惜。不过，人各有命吧，也学习到了很多，前后相比，算是成熟了很多了。\n\n拖拖拉拉，写完这篇总结的时候，2022年都已经过去四分之一了。总的来说，2022年可以用“翻天覆地”这个词来概括，在这个痛苦的过程中，也见到了一个不一样的自己。虽然2022年的开端不算顺利，很失败，但这些问题就留到明年的总结中来写吧。最后，希望在2023年总结今年的时候，可以用一个更加开心的关键词吧。","slug":"2021","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl5i8k7xv00011rpf5tmzbz9z","content":"<p>2021年这一年的时间里，发生了太多的事，如果不写下来的话，在脑海中只会有一些模糊的印象，不清楚自己取得了哪些成绩，又有哪里不尽人意，同时也可以从一个旁观者的视角，来观察自己这一年有没有虚度光阴。所以，就从2021年开始，开始每年对过去的一年进行一个记录吧~</p>\n<h2 id=\"技术\"><a href=\"#技术\" class=\"headerlink\" title=\"技术\"></a>技术</h2><h3 id=\"K8s\"><a href=\"#K8s\" class=\"headerlink\" title=\"K8s\"></a>K8s</h3><p>在年初的时候，阅读了《Kubernetes in Action》这本书，受益匪浅，再加上实习中的实践，对 K8s 的了解更加深入了。K8s 作为目前 Cloud 的实际代名词，在应用开发中已经是无法替代的存在了。作为云原生的基础设施，对于 K8s 的学习，除了掌握其基础的操作，例如 Deployment、Ingress 等等组件的使用，来满足上层的应用开发以外，对于 K8s 本身的学习，也是十分重要的。因为 K8s 本身就是一个庞大复杂的系统，而且经过无数业界大佬的打磨，系统上的设计无疑是很多问题的 best practice。深入理解 K8s 的设计，对于未来设计自己的应用架构肯定会带来很大的帮助。</p>\n<h3 id=\"自动化部署\"><a href=\"#自动化部署\" class=\"headerlink\" title=\"自动化部署\"></a>自动化部署</h3><p>我在实习期间负责的主要的一部分内容就是自动化部署。随着现在微服务化的趋势以及系统内第三方依赖组件的数量的增加，部署其实是一个非常麻烦的问题。在公有云的场景下，部分第三方组件或许可以通过外部服务的形式解决。但在私有云的情况下，部署需要同时部署这些第三方组件，无疑给部署增加了很大的复杂度。不仅仅如此，对于任何现代化的软件来说，部署都是需要进行标准化的。通过部署文档，类似手工作坊进行的部署方式，是无法持久的，人因的错误是整个部署过程中很大的不稳定因素。这方面也有很多做的比较好的开源项目，比如说 TiDB 的 TiUP 等等。</p>\n<h3 id=\"可视化\"><a href=\"#可视化\" class=\"headerlink\" title=\"可视化\"></a>可视化</h3><p>实话说，在正式进入硕士学习之前，我对于可视化的了解可能都比较浅薄（虽然明知道这是自己硕士的研究方向）。在听完大老板的课之后，对于这个领域有了全新的认知。可视化这个领域虽然很小，但并不是像大众认为的只有简单的图表而已。首先，图表本身的设计有着很多问题值得研究。设计可能是一个主观感性的行为，但我大老板的想法是通过design space等方式，将其转化为理性，有逻辑的行为。我对这一点非常认同，给了我一种茅塞顿开的感觉，让我对之前很多看似感性的事情有了新的理解。其次，可视分析也是可视化中非常重要的一部分。很多问题，通过可视化的形式，就可以很清楚和容易地被分析出来。这一类系统，感觉在 BI 领域应该有很大的发展空间。</p>\n<h3 id=\"Casbin\"><a href=\"#Casbin\" class=\"headerlink\" title=\"Casbin\"></a>Casbin</h3><p>一直以来想加入一个开源社区，为 developer 们做出一点点自己的贡献。于是通过一个活动加入到了 Casbin社区。Casbin 是一个权限校验框架，通过 well-defined 的 model 结构，支持对各种类型的校验模式（比如RBAC，ABAC等等）。除此之外，我觉得 Casbin 能够获得 10k+ star 的另一个原因是他的生态支持也太丰富了。各种语言，各种前端后端框架，各种数据库，只要是有需求，都会进行支持。其中另一个最重要的应该是 Casdoor，一个第三方统一身份认证框架，与 KeyCloak 相对标。</p>\n<p>这两个项目都非常有意义，Casbin 主要是像一个解析器一样，面对不同的 model 和 policy定义，需要提供正确的校验结果，我个人还挺喜欢慢慢推理，寻找哪一步解析错误的过程。而 Casdoor 则像一个业务系统，如何实现更多的 feature，如何提升易用性，与更多的第三方系统进行集成。</p>\n<h3 id=\"Other\"><a href=\"#Other\" class=\"headerlink\" title=\"Other\"></a>Other</h3><p>除此之前，这一年间也多多少少接触了很多其他技术。实习的时候无所事事，开始写起来了 Augular，体验了一次像 Java 一样的前端的开发模式；暑假的时候参加 GSoC，借机接触了一下 Rust，对这门语言产生了深深的敬畏，但可惜没能深入学习下去；开学以后接触了很多设计领域的coding，做了一些数字媒体类似的课程作业……</p>\n<h2 id=\"生活\"><a href=\"#生活\" class=\"headerlink\" title=\"生活\"></a>生活</h2><p>上半年实习的过程中，感觉自己逐渐放开了。在实习期间遇到了很多伙伴，大家一起快乐摸鱼，真的是度过了一段很快乐的时光。最重要的是毕业啦！最高学历终于从高中变成了本科。虽然毕业旅行因为疫情原因没能成行，但暑假在家快乐摸鱼，疯狂锻炼猎龙技术，还差一点把塞尔达的呀哈哈全收集，也算快乐的度过了最后一个暑假了。</p>\n<p>如果说上半年是我人生最快乐的时光，那么下半年开学后，就是我目前为止最痛苦的日子了。但无论是前后哪个阶段，我其实都成长了非常多，也算是值得庆幸的事，能够在接受到社会正式的毒打之前，提前成长。</p>\n<p>一方面的压力来自于课程，毕竟我是属于跨专业保研到了设创，难免在研究生阶段要接触到设计相关的课程。在组队上，天真地以为老师是技术背景出身的，项目可能也会是，结果就是在课程项目上被设计背景的同学吊打。在 DDL 前疯狂爆肝，但是也换不来特别高的成绩。研究生课程中没能抱上设计大佬们的大腿，没体验一次被带飞的感觉，算是比较可惜的了。</p>\n<p>另一方面最大的压力来自于实验室。大老板的痛骂确实是有效的。做事不够深入的问题，在我之前的面试以及实习过程中，都多次被前辈们提到过，但都采用相对和蔼的语气和态度。我也是属实有点抖m了，这种不痛不痒的批评根本不往心里去，也没有发自内心地去纠正自己的问题，非要等到现在的大老板爆骂自己，自己才意识到问题。。。</p>\n<p>年末到22年初的状态确实不太好，似乎每年的这段时间都很emo。在年底DDL结束之后，自己的节奏一时间也没调整过来。闲下来以后，脑子昏昏沉沉，没了目标，搞不清楚自己真正想要什么，不讨人喜欢。最后也算造成了不可弥补的错误吧，非常可惜。不过，人各有命吧，也学习到了很多，前后相比，算是成熟了很多了。</p>\n<p>拖拖拉拉，写完这篇总结的时候，2022年都已经过去四分之一了。总的来说，2022年可以用“翻天覆地”这个词来概括，在这个痛苦的过程中，也见到了一个不一样的自己。虽然2022年的开端不算顺利，很失败，但这些问题就留到明年的总结中来写吧。最后，希望在2023年总结今年的时候，可以用一个更加开心的关键词吧。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>2021年这一年的时间里，发生了太多的事，如果不写下来的话，在脑海中只会有一些模糊的印象，不清楚自己取得了哪些成绩，又有哪里不尽人意，同时也可以从一个旁观者的视角，来观察自己这一年有没有虚度光阴。所以，就从2021年开始，开始每年对过去的一年进行一个记录吧~</p>\n<h2 id=\"技术\"><a href=\"#技术\" class=\"headerlink\" title=\"技术\"></a>技术</h2><h3 id=\"K8s\"><a href=\"#K8s\" class=\"headerlink\" title=\"K8s\"></a>K8s</h3><p>在年初的时候，阅读了《Kubernetes in Action》这本书，受益匪浅，再加上实习中的实践，对 K8s 的了解更加深入了。K8s 作为目前 Cloud 的实际代名词，在应用开发中已经是无法替代的存在了。作为云原生的基础设施，对于 K8s 的学习，除了掌握其基础的操作，例如 Deployment、Ingress 等等组件的使用，来满足上层的应用开发以外，对于 K8s 本身的学习，也是十分重要的。因为 K8s 本身就是一个庞大复杂的系统，而且经过无数业界大佬的打磨，系统上的设计无疑是很多问题的 best practice。深入理解 K8s 的设计，对于未来设计自己的应用架构肯定会带来很大的帮助。</p>\n<h3 id=\"自动化部署\"><a href=\"#自动化部署\" class=\"headerlink\" title=\"自动化部署\"></a>自动化部署</h3><p>我在实习期间负责的主要的一部分内容就是自动化部署。随着现在微服务化的趋势以及系统内第三方依赖组件的数量的增加，部署其实是一个非常麻烦的问题。在公有云的场景下，部分第三方组件或许可以通过外部服务的形式解决。但在私有云的情况下，部署需要同时部署这些第三方组件，无疑给部署增加了很大的复杂度。不仅仅如此，对于任何现代化的软件来说，部署都是需要进行标准化的。通过部署文档，类似手工作坊进行的部署方式，是无法持久的，人因的错误是整个部署过程中很大的不稳定因素。这方面也有很多做的比较好的开源项目，比如说 TiDB 的 TiUP 等等。</p>\n<h3 id=\"可视化\"><a href=\"#可视化\" class=\"headerlink\" title=\"可视化\"></a>可视化</h3><p>实话说，在正式进入硕士学习之前，我对于可视化的了解可能都比较浅薄（虽然明知道这是自己硕士的研究方向）。在听完大老板的课之后，对于这个领域有了全新的认知。可视化这个领域虽然很小，但并不是像大众认为的只有简单的图表而已。首先，图表本身的设计有着很多问题值得研究。设计可能是一个主观感性的行为，但我大老板的想法是通过design space等方式，将其转化为理性，有逻辑的行为。我对这一点非常认同，给了我一种茅塞顿开的感觉，让我对之前很多看似感性的事情有了新的理解。其次，可视分析也是可视化中非常重要的一部分。很多问题，通过可视化的形式，就可以很清楚和容易地被分析出来。这一类系统，感觉在 BI 领域应该有很大的发展空间。</p>\n<h3 id=\"Casbin\"><a href=\"#Casbin\" class=\"headerlink\" title=\"Casbin\"></a>Casbin</h3><p>一直以来想加入一个开源社区，为 developer 们做出一点点自己的贡献。于是通过一个活动加入到了 Casbin社区。Casbin 是一个权限校验框架，通过 well-defined 的 model 结构，支持对各种类型的校验模式（比如RBAC，ABAC等等）。除此之外，我觉得 Casbin 能够获得 10k+ star 的另一个原因是他的生态支持也太丰富了。各种语言，各种前端后端框架，各种数据库，只要是有需求，都会进行支持。其中另一个最重要的应该是 Casdoor，一个第三方统一身份认证框架，与 KeyCloak 相对标。</p>\n<p>这两个项目都非常有意义，Casbin 主要是像一个解析器一样，面对不同的 model 和 policy定义，需要提供正确的校验结果，我个人还挺喜欢慢慢推理，寻找哪一步解析错误的过程。而 Casdoor 则像一个业务系统，如何实现更多的 feature，如何提升易用性，与更多的第三方系统进行集成。</p>\n<h3 id=\"Other\"><a href=\"#Other\" class=\"headerlink\" title=\"Other\"></a>Other</h3><p>除此之前，这一年间也多多少少接触了很多其他技术。实习的时候无所事事，开始写起来了 Augular，体验了一次像 Java 一样的前端的开发模式；暑假的时候参加 GSoC，借机接触了一下 Rust，对这门语言产生了深深的敬畏，但可惜没能深入学习下去；开学以后接触了很多设计领域的coding，做了一些数字媒体类似的课程作业……</p>\n<h2 id=\"生活\"><a href=\"#生活\" class=\"headerlink\" title=\"生活\"></a>生活</h2><p>上半年实习的过程中，感觉自己逐渐放开了。在实习期间遇到了很多伙伴，大家一起快乐摸鱼，真的是度过了一段很快乐的时光。最重要的是毕业啦！最高学历终于从高中变成了本科。虽然毕业旅行因为疫情原因没能成行，但暑假在家快乐摸鱼，疯狂锻炼猎龙技术，还差一点把塞尔达的呀哈哈全收集，也算快乐的度过了最后一个暑假了。</p>\n<p>如果说上半年是我人生最快乐的时光，那么下半年开学后，就是我目前为止最痛苦的日子了。但无论是前后哪个阶段，我其实都成长了非常多，也算是值得庆幸的事，能够在接受到社会正式的毒打之前，提前成长。</p>\n<p>一方面的压力来自于课程，毕竟我是属于跨专业保研到了设创，难免在研究生阶段要接触到设计相关的课程。在组队上，天真地以为老师是技术背景出身的，项目可能也会是，结果就是在课程项目上被设计背景的同学吊打。在 DDL 前疯狂爆肝，但是也换不来特别高的成绩。研究生课程中没能抱上设计大佬们的大腿，没体验一次被带飞的感觉，算是比较可惜的了。</p>\n<p>另一方面最大的压力来自于实验室。大老板的痛骂确实是有效的。做事不够深入的问题，在我之前的面试以及实习过程中，都多次被前辈们提到过，但都采用相对和蔼的语气和态度。我也是属实有点抖m了，这种不痛不痒的批评根本不往心里去，也没有发自内心地去纠正自己的问题，非要等到现在的大老板爆骂自己，自己才意识到问题。。。</p>\n<p>年末到22年初的状态确实不太好，似乎每年的这段时间都很emo。在年底DDL结束之后，自己的节奏一时间也没调整过来。闲下来以后，脑子昏昏沉沉，没了目标，搞不清楚自己真正想要什么，不讨人喜欢。最后也算造成了不可弥补的错误吧，非常可惜。不过，人各有命吧，也学习到了很多，前后相比，算是成熟了很多了。</p>\n<p>拖拖拉拉，写完这篇总结的时候，2022年都已经过去四分之一了。总的来说，2022年可以用“翻天覆地”这个词来概括，在这个痛苦的过程中，也见到了一个不一样的自己。虽然2022年的开端不算顺利，很失败，但这些问题就留到明年的总结中来写吧。最后，希望在2023年总结今年的时候，可以用一个更加开心的关键词吧。</p>\n"},{"title":"[BugFix] Spring Cloud IPv6端口问题排坑","date":"2021-03-07T23:59:22.000Z","updated":"2021-03-07T23:59:22.000Z","_content":"\n## 场景\n使用 Spring Cloud Eureka 搭建服务注册中心，使用 Zuul 搭建服务网关，一套比较传统的微服务架构。\n服务注册中心的地址为 http://localhost:8888，Zuul 网关地址为 http://localhost:8080， 另外搭建一个服务名为 metadata-service 的服务，地址为 http://localhost:8088。\n## 问题\n在 metadata-service 中提供一个测试的接口\n```Java\n@RestController\npublic class MetadataController {\n​\n    @GetMapping(value = \"/test\")\n    public int getTest() {\n        return 1;\n    }\n}\n```\n使用 Postman 进行测试，结果发现直接请求 http://localhost:8088/test 即 metadata-service 的地址，可以正常得到结果\n\n![](/asset/spring_ipv6/1.jpg)\n\n而通过网关，使用 Zuul 默认路由规则，调用服务，会出现 404 的错误\n\n![](/asset/spring_ipv6/2.jpg)\n\n## 分析\n首先，我们可以先通过 http://localhost:8888 查看服务是否注册到了服务注册中心\n\n![](/asset/spring_ipv6/3.jpg)\n\n可以看到没有任何问题。\n那么，我们再检查网关有没有获取到 metadata-service 的路由。可以通过 http://localhost:8080/actuator/routes 查看（actuator默认是关闭的，可以通过配置 management.endpoints.web.exposure.include=* 开启）。\n\n![](/asset/spring_ipv6/4.jpg)\n\n同样，我们可以看到没有任何问题。\n那么，就很奇怪了🤨，服务本身没有任何问题，直接调用也可以访问，而通过网关一转发，为什么就 404 了呢？在网上查了一下午，也没有找到有人遇到过类似的问题。。。😱\n问题的关键在我关闭服务后再次请求 http://localhost:8088/test 时终于找到了。正常情况下，关闭了服务后，应该没有返回的 response，但发出请求过后仍然是 404\n\n![](/asset/spring_ipv6/5.jpg)\n\n那么，就很明显了，有另一个进程也在监听 8088 端口 ！！！\n但还是很奇怪，那为什么服务启动的时候没有报端口被占用的错误呢？？？\n重新启动服务，使用 lsof -i tcp:8088 （Mac OS）查看端口占用情况\n\n![](/asset/spring_ipv6/6.jpg)\n\n果然有两个进程同时在监听，而一个是 IPv4，一个是 IPv6的。\n首先，根据这篇文章 https://blog.csdn.net/jiyiqinlovexx/article/details/50959351 的解释，多个进程是完全可以同时监听同一个端口的。\n而从 Java 7 开始，默认使用 IPv6 而不是 IPv4 （https://stackoverflow.com/questions/35470838/localhost-vs-127-0-0-1-in-spring-framework），所以对于 Spring 的 localhost 来说，其实真正使用的 IP 地址是 ::1，而不是 127.0.0.1 。使用 Postman 进行测试，可以发现 http://[::1]:8088/test 得到正常结果，而 http://127.0.0.1:8088/test 则为 404 。这就完美地解释了开启服务与停止服务，返回结果不同的问题，Spring 服务所对应的正是那个 IPv6 的进程。\n那么，为什么网关转发就到了 IPv4 呢？我们再来看一下服务注册中心里的信息\n\n![](/asset/spring_ipv6/7.jpg)\n\n可以看到其实 Eureka 保存的是每个服务的 IP 地址是本机的 IPv4 的内网地址，而不是保存域名，这就是问题的关键。我们可以使用 Postman 发送请求  http://localhost:8080/metadata-service/test 后，使用命令 lsof -i tcp:8088 进行验证。\n\n![](/asset/spring_ipv6/8.jpg)\n\n可以看到的确是向内网 IP 地址，而不是向 localhost 转发请求。\n## 解决方案\n至此，问题的原因已经完全清楚了，果然程序都是 debug de 出来的。\n最简单的方法也很清楚了，换个端口号就 OK 了。\n\n如果本文有错误或者理解不对的地方，欢迎指正！！！😆\n\n那么，占了 8088 端口的 IPv4 进程是哪个程序呢？🤨\n\n\n\n\n\n\n\n。。。。Hadoop 出来挨打！！！😭😭😭","source":"_posts/Spring IPv6.md","raw":"---\ntitle: \"[BugFix] Spring Cloud IPv6端口问题排坑\"\ndate: 2021-03-07 23:59:22\nupdated: 2021-03-07 23:59:22\n---\n\n## 场景\n使用 Spring Cloud Eureka 搭建服务注册中心，使用 Zuul 搭建服务网关，一套比较传统的微服务架构。\n服务注册中心的地址为 http://localhost:8888，Zuul 网关地址为 http://localhost:8080， 另外搭建一个服务名为 metadata-service 的服务，地址为 http://localhost:8088。\n## 问题\n在 metadata-service 中提供一个测试的接口\n```Java\n@RestController\npublic class MetadataController {\n​\n    @GetMapping(value = \"/test\")\n    public int getTest() {\n        return 1;\n    }\n}\n```\n使用 Postman 进行测试，结果发现直接请求 http://localhost:8088/test 即 metadata-service 的地址，可以正常得到结果\n\n![](/asset/spring_ipv6/1.jpg)\n\n而通过网关，使用 Zuul 默认路由规则，调用服务，会出现 404 的错误\n\n![](/asset/spring_ipv6/2.jpg)\n\n## 分析\n首先，我们可以先通过 http://localhost:8888 查看服务是否注册到了服务注册中心\n\n![](/asset/spring_ipv6/3.jpg)\n\n可以看到没有任何问题。\n那么，我们再检查网关有没有获取到 metadata-service 的路由。可以通过 http://localhost:8080/actuator/routes 查看（actuator默认是关闭的，可以通过配置 management.endpoints.web.exposure.include=* 开启）。\n\n![](/asset/spring_ipv6/4.jpg)\n\n同样，我们可以看到没有任何问题。\n那么，就很奇怪了🤨，服务本身没有任何问题，直接调用也可以访问，而通过网关一转发，为什么就 404 了呢？在网上查了一下午，也没有找到有人遇到过类似的问题。。。😱\n问题的关键在我关闭服务后再次请求 http://localhost:8088/test 时终于找到了。正常情况下，关闭了服务后，应该没有返回的 response，但发出请求过后仍然是 404\n\n![](/asset/spring_ipv6/5.jpg)\n\n那么，就很明显了，有另一个进程也在监听 8088 端口 ！！！\n但还是很奇怪，那为什么服务启动的时候没有报端口被占用的错误呢？？？\n重新启动服务，使用 lsof -i tcp:8088 （Mac OS）查看端口占用情况\n\n![](/asset/spring_ipv6/6.jpg)\n\n果然有两个进程同时在监听，而一个是 IPv4，一个是 IPv6的。\n首先，根据这篇文章 https://blog.csdn.net/jiyiqinlovexx/article/details/50959351 的解释，多个进程是完全可以同时监听同一个端口的。\n而从 Java 7 开始，默认使用 IPv6 而不是 IPv4 （https://stackoverflow.com/questions/35470838/localhost-vs-127-0-0-1-in-spring-framework），所以对于 Spring 的 localhost 来说，其实真正使用的 IP 地址是 ::1，而不是 127.0.0.1 。使用 Postman 进行测试，可以发现 http://[::1]:8088/test 得到正常结果，而 http://127.0.0.1:8088/test 则为 404 。这就完美地解释了开启服务与停止服务，返回结果不同的问题，Spring 服务所对应的正是那个 IPv6 的进程。\n那么，为什么网关转发就到了 IPv4 呢？我们再来看一下服务注册中心里的信息\n\n![](/asset/spring_ipv6/7.jpg)\n\n可以看到其实 Eureka 保存的是每个服务的 IP 地址是本机的 IPv4 的内网地址，而不是保存域名，这就是问题的关键。我们可以使用 Postman 发送请求  http://localhost:8080/metadata-service/test 后，使用命令 lsof -i tcp:8088 进行验证。\n\n![](/asset/spring_ipv6/8.jpg)\n\n可以看到的确是向内网 IP 地址，而不是向 localhost 转发请求。\n## 解决方案\n至此，问题的原因已经完全清楚了，果然程序都是 debug de 出来的。\n最简单的方法也很清楚了，换个端口号就 OK 了。\n\n如果本文有错误或者理解不对的地方，欢迎指正！！！😆\n\n那么，占了 8088 端口的 IPv4 进程是哪个程序呢？🤨\n\n\n\n\n\n\n\n。。。。Hadoop 出来挨打！！！😭😭😭","slug":"Spring IPv6","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl5i8k7y000031rpf1zgid45a","content":"<h2 id=\"场景\"><a href=\"#场景\" class=\"headerlink\" title=\"场景\"></a>场景</h2><p>使用 Spring Cloud Eureka 搭建服务注册中心，使用 Zuul 搭建服务网关，一套比较传统的微服务架构。<br>服务注册中心的地址为 <a href=\"http://localhost:8888，Zuul\">http://localhost:8888，Zuul</a> 网关地址为 <a href=\"http://localhost:8080，\">http://localhost:8080，</a> 另外搭建一个服务名为 metadata-service 的服务，地址为 <a href=\"http://localhost:8088。\">http://localhost:8088。</a></p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>在 metadata-service 中提供一个测试的接口</p>\n<pre><code class=\"Java\">@RestController\npublic class MetadataController &#123;\n​\n    @GetMapping(value = &quot;/test&quot;)\n    public int getTest() &#123;\n        return 1;\n    &#125;\n&#125;\n</code></pre>\n<p>使用 Postman 进行测试，结果发现直接请求 <a href=\"http://localhost:8088/test\">http://localhost:8088/test</a> 即 metadata-service 的地址，可以正常得到结果</p>\n<p><img src=\"/asset/spring_ipv6/1.jpg\"></p>\n<p>而通过网关，使用 Zuul 默认路由规则，调用服务，会出现 404 的错误</p>\n<p><img src=\"/asset/spring_ipv6/2.jpg\"></p>\n<h2 id=\"分析\"><a href=\"#分析\" class=\"headerlink\" title=\"分析\"></a>分析</h2><p>首先，我们可以先通过 <a href=\"http://localhost:8888/\">http://localhost:8888</a> 查看服务是否注册到了服务注册中心</p>\n<p><img src=\"/asset/spring_ipv6/3.jpg\"></p>\n<p>可以看到没有任何问题。<br>那么，我们再检查网关有没有获取到 metadata-service 的路由。可以通过 <a href=\"http://localhost:8080/actuator/routes\">http://localhost:8080/actuator/routes</a> 查看（actuator默认是关闭的，可以通过配置 management.endpoints.web.exposure.include=* 开启）。</p>\n<p><img src=\"/asset/spring_ipv6/4.jpg\"></p>\n<p>同样，我们可以看到没有任何问题。<br>那么，就很奇怪了🤨，服务本身没有任何问题，直接调用也可以访问，而通过网关一转发，为什么就 404 了呢？在网上查了一下午，也没有找到有人遇到过类似的问题。。。😱<br>问题的关键在我关闭服务后再次请求 <a href=\"http://localhost:8088/test\">http://localhost:8088/test</a> 时终于找到了。正常情况下，关闭了服务后，应该没有返回的 response，但发出请求过后仍然是 404</p>\n<p><img src=\"/asset/spring_ipv6/5.jpg\"></p>\n<p>那么，就很明显了，有另一个进程也在监听 8088 端口 ！！！<br>但还是很奇怪，那为什么服务启动的时候没有报端口被占用的错误呢？？？<br>重新启动服务，使用 lsof -i tcp:8088 （Mac OS）查看端口占用情况</p>\n<p><img src=\"/asset/spring_ipv6/6.jpg\"></p>\n<p>果然有两个进程同时在监听，而一个是 IPv4，一个是 IPv6的。<br>首先，根据这篇文章 <a href=\"https://blog.csdn.net/jiyiqinlovexx/article/details/50959351\">https://blog.csdn.net/jiyiqinlovexx/article/details/50959351</a> 的解释，多个进程是完全可以同时监听同一个端口的。<br>而从 Java 7 开始，默认使用 IPv6 而不是 IPv4 （<a href=\"https://stackoverflow.com/questions/35470838/localhost-vs-127-0-0-1-in-spring-framework%EF%BC%89%EF%BC%8C%E6%89%80%E4%BB%A5%E5%AF%B9%E4%BA%8E\">https://stackoverflow.com/questions/35470838/localhost-vs-127-0-0-1-in-spring-framework），所以对于</a> Spring 的 localhost 来说，其实真正使用的 IP 地址是 ::1，而不是 127.0.0.1 。使用 Postman 进行测试，可以发现 http://[::1]:8088/test 得到正常结果，而 <a href=\"http://127.0.0.1:8088/test\">http://127.0.0.1:8088/test</a> 则为 404 。这就完美地解释了开启服务与停止服务，返回结果不同的问题，Spring 服务所对应的正是那个 IPv6 的进程。<br>那么，为什么网关转发就到了 IPv4 呢？我们再来看一下服务注册中心里的信息</p>\n<p><img src=\"/asset/spring_ipv6/7.jpg\"></p>\n<p>可以看到其实 Eureka 保存的是每个服务的 IP 地址是本机的 IPv4 的内网地址，而不是保存域名，这就是问题的关键。我们可以使用 Postman 发送请求  <a href=\"http://localhost:8080/metadata-service/test\">http://localhost:8080/metadata-service/test</a> 后，使用命令 lsof -i tcp:8088 进行验证。</p>\n<p><img src=\"/asset/spring_ipv6/8.jpg\"></p>\n<p>可以看到的确是向内网 IP 地址，而不是向 localhost 转发请求。</p>\n<h2 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h2><p>至此，问题的原因已经完全清楚了，果然程序都是 debug de 出来的。<br>最简单的方法也很清楚了，换个端口号就 OK 了。</p>\n<p>如果本文有错误或者理解不对的地方，欢迎指正！！！😆</p>\n<p>那么，占了 8088 端口的 IPv4 进程是哪个程序呢？🤨</p>\n<p>。。。。Hadoop 出来挨打！！！😭😭😭</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"场景\"><a href=\"#场景\" class=\"headerlink\" title=\"场景\"></a>场景</h2><p>使用 Spring Cloud Eureka 搭建服务注册中心，使用 Zuul 搭建服务网关，一套比较传统的微服务架构。<br>服务注册中心的地址为 <a href=\"http://localhost:8888，Zuul\">http://localhost:8888，Zuul</a> 网关地址为 <a href=\"http://localhost:8080，\">http://localhost:8080，</a> 另外搭建一个服务名为 metadata-service 的服务，地址为 <a href=\"http://localhost:8088。\">http://localhost:8088。</a></p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>在 metadata-service 中提供一个测试的接口</p>\n<pre><code class=\"Java\">@RestController\npublic class MetadataController &#123;\n​\n    @GetMapping(value = &quot;/test&quot;)\n    public int getTest() &#123;\n        return 1;\n    &#125;\n&#125;\n</code></pre>\n<p>使用 Postman 进行测试，结果发现直接请求 <a href=\"http://localhost:8088/test\">http://localhost:8088/test</a> 即 metadata-service 的地址，可以正常得到结果</p>\n<p><img src=\"/asset/spring_ipv6/1.jpg\"></p>\n<p>而通过网关，使用 Zuul 默认路由规则，调用服务，会出现 404 的错误</p>\n<p><img src=\"/asset/spring_ipv6/2.jpg\"></p>\n<h2 id=\"分析\"><a href=\"#分析\" class=\"headerlink\" title=\"分析\"></a>分析</h2><p>首先，我们可以先通过 <a href=\"http://localhost:8888/\">http://localhost:8888</a> 查看服务是否注册到了服务注册中心</p>\n<p><img src=\"/asset/spring_ipv6/3.jpg\"></p>\n<p>可以看到没有任何问题。<br>那么，我们再检查网关有没有获取到 metadata-service 的路由。可以通过 <a href=\"http://localhost:8080/actuator/routes\">http://localhost:8080/actuator/routes</a> 查看（actuator默认是关闭的，可以通过配置 management.endpoints.web.exposure.include=* 开启）。</p>\n<p><img src=\"/asset/spring_ipv6/4.jpg\"></p>\n<p>同样，我们可以看到没有任何问题。<br>那么，就很奇怪了🤨，服务本身没有任何问题，直接调用也可以访问，而通过网关一转发，为什么就 404 了呢？在网上查了一下午，也没有找到有人遇到过类似的问题。。。😱<br>问题的关键在我关闭服务后再次请求 <a href=\"http://localhost:8088/test\">http://localhost:8088/test</a> 时终于找到了。正常情况下，关闭了服务后，应该没有返回的 response，但发出请求过后仍然是 404</p>\n<p><img src=\"/asset/spring_ipv6/5.jpg\"></p>\n<p>那么，就很明显了，有另一个进程也在监听 8088 端口 ！！！<br>但还是很奇怪，那为什么服务启动的时候没有报端口被占用的错误呢？？？<br>重新启动服务，使用 lsof -i tcp:8088 （Mac OS）查看端口占用情况</p>\n<p><img src=\"/asset/spring_ipv6/6.jpg\"></p>\n<p>果然有两个进程同时在监听，而一个是 IPv4，一个是 IPv6的。<br>首先，根据这篇文章 <a href=\"https://blog.csdn.net/jiyiqinlovexx/article/details/50959351\">https://blog.csdn.net/jiyiqinlovexx/article/details/50959351</a> 的解释，多个进程是完全可以同时监听同一个端口的。<br>而从 Java 7 开始，默认使用 IPv6 而不是 IPv4 （<a href=\"https://stackoverflow.com/questions/35470838/localhost-vs-127-0-0-1-in-spring-framework%EF%BC%89%EF%BC%8C%E6%89%80%E4%BB%A5%E5%AF%B9%E4%BA%8E\">https://stackoverflow.com/questions/35470838/localhost-vs-127-0-0-1-in-spring-framework），所以对于</a> Spring 的 localhost 来说，其实真正使用的 IP 地址是 ::1，而不是 127.0.0.1 。使用 Postman 进行测试，可以发现 http://[::1]:8088/test 得到正常结果，而 <a href=\"http://127.0.0.1:8088/test\">http://127.0.0.1:8088/test</a> 则为 404 。这就完美地解释了开启服务与停止服务，返回结果不同的问题，Spring 服务所对应的正是那个 IPv6 的进程。<br>那么，为什么网关转发就到了 IPv4 呢？我们再来看一下服务注册中心里的信息</p>\n<p><img src=\"/asset/spring_ipv6/7.jpg\"></p>\n<p>可以看到其实 Eureka 保存的是每个服务的 IP 地址是本机的 IPv4 的内网地址，而不是保存域名，这就是问题的关键。我们可以使用 Postman 发送请求  <a href=\"http://localhost:8080/metadata-service/test\">http://localhost:8080/metadata-service/test</a> 后，使用命令 lsof -i tcp:8088 进行验证。</p>\n<p><img src=\"/asset/spring_ipv6/8.jpg\"></p>\n<p>可以看到的确是向内网 IP 地址，而不是向 localhost 转发请求。</p>\n<h2 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h2><p>至此，问题的原因已经完全清楚了，果然程序都是 debug de 出来的。<br>最简单的方法也很清楚了，换个端口号就 OK 了。</p>\n<p>如果本文有错误或者理解不对的地方，欢迎指正！！！😆</p>\n<p>那么，占了 8088 端口的 IPv4 进程是哪个程序呢？🤨</p>\n<p>。。。。Hadoop 出来挨打！！！😭😭😭</p>\n"},{"title":"[Introduction] Casbin is All You Need —— 访问控制框架 Casbin Ramp Up","date":"2022-06-22T01:05:51.000Z","updated":"2022-06-22T01:05:51.000Z","_content":"\n## TL;DR\n\n- 访问控制框架 Casbin 的原理以及其内部组件的结构\n- 以一个 RBAC 的简单例子介绍 Casbin 的用法\n\n## Casbin是什么？\n### 访问控制\n\n![](/asset/casbin-ramp-up/2022-06-23-17-50-00-image.png)\n\n访问控制，顾名思义，是指判断一条请求是否可以访问受保护的资源的技术。在上图的例子中，我们的后台中有两个资源，Resource1和Resource2。它们可以是服务器、账号、图片、视频等等等等。但是，它们的相同特性是不能被所有用户都访问。比如 Resource1 属于用户 Alice，那么只有 Alice 能够访问它，Bob 则不能。因此，我们就需要对访问请求进行过滤，判断其是否被允许到达目标资源。在上面的例子中，Alice 发起了两个访问请求，分别想要访问 Resource1 和 Resource2。访问控制层需要做的工作就是允许访问 Resource1 的请求通过，而阻拦想要访问 Resource2 的请求，因为 Resource2 属于 Bob，Alice 是无法访问的。\n\n在实际应用中，访问控制问题往往会随着业务而变得非常复杂。而 Casbin [<sup>1</sup>](#casbin) 就是一个强大的、高效的开源访问控制框架。Casbin 在 Github 上已获得超过 10k+ star，并且有着非常完整的生态。基于 Casbin 可以轻松的实现一系列访问控制模型，如 RBAC，ABAC等等。\n\n### 原理——PML\n\nCasbin 的底层原理基于其创建者罗杨博士所发表的一篇论文《PML: An Interpreter-Based Access Control Policy Language for Web Services》[<sup>2</sup>](#pml)\n\n#### 设计目标\n\n这篇论文主要关注于如何解决现实中云服务厂商有关权限校验所遇到的两个问题：\n\n1. 每个云服务厂商都有着自己的一套权限检验规则。这对于在多个云环境都进行部署的用户来说，造成了很大的迁移和维护成本。\n2. 同样的，维护自己的一套权限校验规则对于云服务厂商来说，也是一个挑战。如果云服务厂商缺乏在这方面的相关经验，就很有可能造成安全漏洞。\n\n既然文章的目标本质上是通过通用性来解决问题，作者也考虑了如何实现这个目标，提出了两个 independent 的设计要求：\n\n1. Access Control Model Independent：PML 既需要支持用户可以在多个云服务厂商中使用同一个模型，也需要支持用户在不改变校验代码的同时，可以切换不同的模型。\n2. Implementation Language Independent: PML 的设计不应该依赖于某种编程语言的特性。\n   因此，文中提出了一种新的权限校验语言——PML (PERM Modeling Language)，希望通过一种支持多种权限校验模型的配置语言来弥补这个 gap。\n\n#### 设计实现\n\n在介绍 PML 的设计之前，我们可以先大致了解一下访问控制问题中，所涉及到的一些概念。\n\n![1c2dea1652f67c0b7920b0471a4113afd8d9325a.png](/asset/casbin-ramp-up/overview.png)\n\n一般来说，访问控制会涉及到两个部分：\n\n1. **Model 访问控制模型**。常见的模型有 ACL（Access Control List 访问控制列表），RBAC（Role-Based Access Control 基于角色的访问控制），ABAC（Attribute-Based Access Control 基于属性的访问控制）。对于一个应用来说，Model 的选择是与应用的业务逻辑是密切相关的，因此也是相对静态的。一旦代码编译完成，这部分是不会随着应用的运行而产生变化的。\n2. **Policy 访问控制规则**。Policy 是和 Model 相对应的，每种不同的 Model，都会有不同格式的 Policy。而与 Model 完全相反的是，Policy 是相对动态的。在编写代码的过程中，我们只能去定义 Policy 的格式，而 Policy 的具体内容都是应用运行过程中添加或修改的。例如，有一个新用户注册了我们的应用。那么，我们就需要动态的为其添加一条 Policy。\n\n我们可以将这两部分理解为传统应用中的代码和数据。有了这两部分后，再加上用户特定的校验逻辑，那么就可以完成访问控制任务。\n\n##### PERM 模型\n\n当然，对于现实环境中复杂的情况，简单地将问题建模为这两部分肯定是不够的，因此，论文提出了一个新的元模型 PERM（Policy-Effect-Request-Matcher）。\n\n![6d8fa0a037c3ea185cfadb8d817c41cce0d66d78.png](/asset/casbin-ramp-up/pml.png)\n\nPERM 模型主要包含了 6 个主要的概念：\n\n1. **Request**：访问请求定义。用户真实的访问请求，通常会包含 sub（访问者），obj（被访问的资源）， act（访问时所进行的操作）或其他用户自定义的属性。\n2. **Policy**：访问控制规则定义。定义了需要对访问请求的哪些属性进行校验。\n3. **Policy Rule**：访问控制规则实例。\n4. **Matcher**：如何为一条 Request 匹配到其对应的 Policy Rule。\n5. **Effect**：当一条 Request 匹配到了一条或多条 Policy Rule，如何判断其是否应该被允许。\n6. **Stub Function**：在实际应用中，Request 实例 和 Policy Rule 的匹配往往无法通过简单的 == 等于来解决，例如通配符等等。所以 Stub Function 允许用户自定义一些复杂的匹配方法。\n\n这六个更加详细的建模了访问控制的问题。我们也可以对其简单的分一下类，Request，Policy，Matcher，Effect 和 Stub Function 都是静态的，属于 Model 的一部分。通过这个五项的组合，ACL等常见的模型以及一些用户自定义的规则，都可以很轻易的表示出来。在最后一部分中，会以 RBAC 为例，介绍如何通过 PML 实现这样一个模型。\n\n而 Policy Rule 就属于动态变化的内容。在实际实现中，往往也是像数据一样，存储在数据库当中的。\n\n## 结构\n\n与论文中的实现相比，目前 Casbin 的实现更加强大，支持了更多功能。所以，这里以 Casbin 主库（Go 版本），介绍 Casbin 是如何进行工作的。\n\n![6dbf7fb95022569c5ad99becdcd9090df8a99250.png](/asset/casbin-ramp-up/detail.png)\n\n1. 在应用启动时，Casbin 会读取用户已经定义好的 Model，其中会包含 Request, Policy, Matcher 和 Effector 四个部分的定义。同时，Casbin 会利用 Adapter，从数据源处读取 Policy 实例（也就是上文提到的 Policy Rule）。后文就将 Policy 实例简称为 Policy。\n\n2. 对于 Policy 的存储和读取，Casbin 将其解耦到了独立的 Adapter 模块。通过使用不同的 Adapter（File, MySQL等等），可以从不同的数据源中读取 Policy。对于 Policy 比较多的场景，将所有的 Policy 同时加载进内存，确实会导致一定的性能损失。所以，在加载时，部分 Adapter 也提供 `LoadFilteredPolicy` 的接口，通过只加载 Policy 的一部分子集，减少这部分带来的性能瓶颈。\n\n3. 在一条请求到来时，该请求首先会按照 Model 中的定义进行拆分。接下来，Matcher 会根据 Model 中定义的规则，与 Policy 进行匹配。除了支持 == 强匹配外，Matcher 还支持通过 Function 和 Role Manager 进行模糊匹配。Function 像用户提供了自定义匹配规则的接口。通过向 Matcher 传入自定义函数，Matcher 可以对 Request 与 Policy 之间进行一些复杂的匹配。\n   \n   对于 RBAC 等访问控制模型，除了单纯的用户与权限之间存在关系之外，用户与角色（Role）之间还存在着继承关系。Casbin 中采用了 Role Manager 来为一条 Request 的用户以及其对应角色（包含继承角色）寻找与其相关的 Policy。同时，Role Manager 也支持添加自定义的 Function，来对用户与角色之间进行复杂的匹配。\n\n4. 在实际应用中，一条 Request 可能会匹配到多条 Policy。得到所有的 Policy 后，需要进一步将多条 Policy 的结果进行聚合，得到最终是否允许 Request。Effector 根据 Model 中配置的规则，对所有 Matched Policy 的 effect 项进行进一步的 eval。\n\n5. 在很多场景下，访问控制服务会有多个实例。Casbin 支持对 Policy 进行增量更新，那么，就需要 Dispatcher 维护多个 Casbin 实例的 Policy 之间的一致性。Dispatcher 主要提供两部分的功能，一部分是 Casbin 的 API，另一部分是 Dispatcher 自身的 API，用来实现成员管理等一致性问题，可以通过 Raft 等共识算法实现。\n\n## Usage\n\n在了解了 Casbin 的原理和结构后，我们可以开始利用 Casbin 来进行一些实践。本章以 RBAC 模型为例，构建一个简单的访问控制示例。RBAC (Role-Based Access Control) 模型是基于角色的访问控制模型。在 RBAC 的模型中，用户和资源之间存在着角色（Role），用户可以属于一个或多个角色，角色拥有权限去访问资源。\n\n经过前两章的介绍，我们可以将访问控制分为三个部分：Static，Dynamic 和 User-specific Logic。在使用 Casbin 时，也可以这样进行划分。首先，我们先来定义一个静态的 RBAC Model（model.conf）。\n\n```c#\n[request_definition]\nr = sub, obj, act\n\n[policy_definition]\np = sub, obj, act\n\n[role_definition]\ng = _, _\n\n[policy_effect]\ne = some(where (p.eft == allow))\n\n[matchers]\nm = g(r.sub, p.sub) && r.obj == p.obj && r.act == p.act\n```\n\n在这个模型中，分别定义了五部分内容。\n\n1. **request_definition**，定义了 Request 的结构。这份示例中包含了访问者（sub），被访问者（obj）和操作（act）。\n2. **policy_definition**，定义了 Policy 的结构。通常，由于 Policy 和 Request 之间要进行匹配，所以两者的结构有一定的相似性。\n3. **role_definition**，定义了 Role Manager。`g` 定义了一套 RBAC 系统，换句话说是一组用户角色继承关系的集合。在实际使用中，更类似于一个函数，判断输入的参数是否在这个集合中存在继承关系。\n4. **policy_effect**，定义了如何对多个匹配到的 Policy 做合并。目前，Casbin 支持几个固定语法的合并模式，在官网 [<sup>3</sup>](#policy) 上有着详细的介绍。这些模式的含义也很好理解，模式的语法与自然语言或者 SQL 非常相近。例如，`some(where (p.eft == allow))` 表示的是当任意一个 Policy 的 effect 是 allow，那么合并的结果即为 allow。\n5. **matchers**，定义了如何匹配 Policy 和 Request。 定义公式的语法与常见语言中的布尔表达式相似，通过 `==` 可以将 Policy 和 Request 中的各项进行对比。\n\n#### Policy\n\n```\np, alice, data1, read\np, bob, data2, write\np, data2_admin, data2, read\np, data2_admin, data2, write\ng, alice, data2_admin\n```\n\n接下来，我们可以按照 Model 中定义的 Policy 结构来编写 Policy 实例。例如，第一项 `p, alice, data1, read` 与 `p = sub, obj, act` 相对应，`alice`，`data1`，`read` 与 `sub`，`obj`，`act` 相对应。另外一条比较特殊的实例是最后一项，`g, alice, data2_admin`，定义了用户 `alice` 继承了 `data2_admin` 这一角色。\n\n我们可以将上面的 Policy 保存在文件 policy.csv 中。但一般来说，Policy 储存在数据库等等一些更加 organized 的外部存储中。\n\n#### User Logic\n\nCasbin 几乎支持所有的常见的编程语言，用户使用的逻辑也基本相似，主要通过 Enforcer 类来进行操作。\n\n```go\ne, err := casbin.NewEnforcer(\"model.conf\", \"policy.csv\")\nresult1, _ := e.Enforce(\"alice\", \"data1\", \"read\")\nfmt.Println(result1)\nresult2, _ := e.Enforce(\"alice\", \"data1\", \"write\")\nfmt.Println(result2)\nresult3, _ := e.Enforce(\"alice\", \"data2\", \"read\")\nfmt.Println(result3)\n```\n\n通过 Enforce 方法，开发者输入 Request，就可以得到这条请求是否可以通过。在上述例子中有 3 个 test case，分别验证了合法请求匹配，非法请求匹配，集成角色请求匹配。第一个 test case 对应了 policy.csv 中的第一条 Policy，第二个 test case 则没有 test case。第三个 test case 通过 `g, alice, data2_admin` 将 alice 与 data2_admin 的 Policy 关联起来，然后通过第三条 Policy p, data2_admin, data2, read 验证其为合法请求。\n\n## Reference\n<div id=\"ref1\"/>\n- [1] https://github.com/casbin/casbin\n<div id=\"ref2\"/>\n- [2] https://arxiv.org/pdf/1903.09756.pdf\n<div id=\"ref3\"/>\n- [3] https://casbin.org/docs/en/syntax-for-models#policy-effect\n","source":"_posts/casbin-ramp-up.md","raw":"---\ntitle: \"[Introduction] Casbin is All You Need —— 访问控制框架 Casbin Ramp Up\"\ndate: 2022-06-22 01:05:51\nupdated: 2022-06-22 01:05:51\n---\n\n## TL;DR\n\n- 访问控制框架 Casbin 的原理以及其内部组件的结构\n- 以一个 RBAC 的简单例子介绍 Casbin 的用法\n\n## Casbin是什么？\n### 访问控制\n\n![](/asset/casbin-ramp-up/2022-06-23-17-50-00-image.png)\n\n访问控制，顾名思义，是指判断一条请求是否可以访问受保护的资源的技术。在上图的例子中，我们的后台中有两个资源，Resource1和Resource2。它们可以是服务器、账号、图片、视频等等等等。但是，它们的相同特性是不能被所有用户都访问。比如 Resource1 属于用户 Alice，那么只有 Alice 能够访问它，Bob 则不能。因此，我们就需要对访问请求进行过滤，判断其是否被允许到达目标资源。在上面的例子中，Alice 发起了两个访问请求，分别想要访问 Resource1 和 Resource2。访问控制层需要做的工作就是允许访问 Resource1 的请求通过，而阻拦想要访问 Resource2 的请求，因为 Resource2 属于 Bob，Alice 是无法访问的。\n\n在实际应用中，访问控制问题往往会随着业务而变得非常复杂。而 Casbin [<sup>1</sup>](#casbin) 就是一个强大的、高效的开源访问控制框架。Casbin 在 Github 上已获得超过 10k+ star，并且有着非常完整的生态。基于 Casbin 可以轻松的实现一系列访问控制模型，如 RBAC，ABAC等等。\n\n### 原理——PML\n\nCasbin 的底层原理基于其创建者罗杨博士所发表的一篇论文《PML: An Interpreter-Based Access Control Policy Language for Web Services》[<sup>2</sup>](#pml)\n\n#### 设计目标\n\n这篇论文主要关注于如何解决现实中云服务厂商有关权限校验所遇到的两个问题：\n\n1. 每个云服务厂商都有着自己的一套权限检验规则。这对于在多个云环境都进行部署的用户来说，造成了很大的迁移和维护成本。\n2. 同样的，维护自己的一套权限校验规则对于云服务厂商来说，也是一个挑战。如果云服务厂商缺乏在这方面的相关经验，就很有可能造成安全漏洞。\n\n既然文章的目标本质上是通过通用性来解决问题，作者也考虑了如何实现这个目标，提出了两个 independent 的设计要求：\n\n1. Access Control Model Independent：PML 既需要支持用户可以在多个云服务厂商中使用同一个模型，也需要支持用户在不改变校验代码的同时，可以切换不同的模型。\n2. Implementation Language Independent: PML 的设计不应该依赖于某种编程语言的特性。\n   因此，文中提出了一种新的权限校验语言——PML (PERM Modeling Language)，希望通过一种支持多种权限校验模型的配置语言来弥补这个 gap。\n\n#### 设计实现\n\n在介绍 PML 的设计之前，我们可以先大致了解一下访问控制问题中，所涉及到的一些概念。\n\n![1c2dea1652f67c0b7920b0471a4113afd8d9325a.png](/asset/casbin-ramp-up/overview.png)\n\n一般来说，访问控制会涉及到两个部分：\n\n1. **Model 访问控制模型**。常见的模型有 ACL（Access Control List 访问控制列表），RBAC（Role-Based Access Control 基于角色的访问控制），ABAC（Attribute-Based Access Control 基于属性的访问控制）。对于一个应用来说，Model 的选择是与应用的业务逻辑是密切相关的，因此也是相对静态的。一旦代码编译完成，这部分是不会随着应用的运行而产生变化的。\n2. **Policy 访问控制规则**。Policy 是和 Model 相对应的，每种不同的 Model，都会有不同格式的 Policy。而与 Model 完全相反的是，Policy 是相对动态的。在编写代码的过程中，我们只能去定义 Policy 的格式，而 Policy 的具体内容都是应用运行过程中添加或修改的。例如，有一个新用户注册了我们的应用。那么，我们就需要动态的为其添加一条 Policy。\n\n我们可以将这两部分理解为传统应用中的代码和数据。有了这两部分后，再加上用户特定的校验逻辑，那么就可以完成访问控制任务。\n\n##### PERM 模型\n\n当然，对于现实环境中复杂的情况，简单地将问题建模为这两部分肯定是不够的，因此，论文提出了一个新的元模型 PERM（Policy-Effect-Request-Matcher）。\n\n![6d8fa0a037c3ea185cfadb8d817c41cce0d66d78.png](/asset/casbin-ramp-up/pml.png)\n\nPERM 模型主要包含了 6 个主要的概念：\n\n1. **Request**：访问请求定义。用户真实的访问请求，通常会包含 sub（访问者），obj（被访问的资源）， act（访问时所进行的操作）或其他用户自定义的属性。\n2. **Policy**：访问控制规则定义。定义了需要对访问请求的哪些属性进行校验。\n3. **Policy Rule**：访问控制规则实例。\n4. **Matcher**：如何为一条 Request 匹配到其对应的 Policy Rule。\n5. **Effect**：当一条 Request 匹配到了一条或多条 Policy Rule，如何判断其是否应该被允许。\n6. **Stub Function**：在实际应用中，Request 实例 和 Policy Rule 的匹配往往无法通过简单的 == 等于来解决，例如通配符等等。所以 Stub Function 允许用户自定义一些复杂的匹配方法。\n\n这六个更加详细的建模了访问控制的问题。我们也可以对其简单的分一下类，Request，Policy，Matcher，Effect 和 Stub Function 都是静态的，属于 Model 的一部分。通过这个五项的组合，ACL等常见的模型以及一些用户自定义的规则，都可以很轻易的表示出来。在最后一部分中，会以 RBAC 为例，介绍如何通过 PML 实现这样一个模型。\n\n而 Policy Rule 就属于动态变化的内容。在实际实现中，往往也是像数据一样，存储在数据库当中的。\n\n## 结构\n\n与论文中的实现相比，目前 Casbin 的实现更加强大，支持了更多功能。所以，这里以 Casbin 主库（Go 版本），介绍 Casbin 是如何进行工作的。\n\n![6dbf7fb95022569c5ad99becdcd9090df8a99250.png](/asset/casbin-ramp-up/detail.png)\n\n1. 在应用启动时，Casbin 会读取用户已经定义好的 Model，其中会包含 Request, Policy, Matcher 和 Effector 四个部分的定义。同时，Casbin 会利用 Adapter，从数据源处读取 Policy 实例（也就是上文提到的 Policy Rule）。后文就将 Policy 实例简称为 Policy。\n\n2. 对于 Policy 的存储和读取，Casbin 将其解耦到了独立的 Adapter 模块。通过使用不同的 Adapter（File, MySQL等等），可以从不同的数据源中读取 Policy。对于 Policy 比较多的场景，将所有的 Policy 同时加载进内存，确实会导致一定的性能损失。所以，在加载时，部分 Adapter 也提供 `LoadFilteredPolicy` 的接口，通过只加载 Policy 的一部分子集，减少这部分带来的性能瓶颈。\n\n3. 在一条请求到来时，该请求首先会按照 Model 中的定义进行拆分。接下来，Matcher 会根据 Model 中定义的规则，与 Policy 进行匹配。除了支持 == 强匹配外，Matcher 还支持通过 Function 和 Role Manager 进行模糊匹配。Function 像用户提供了自定义匹配规则的接口。通过向 Matcher 传入自定义函数，Matcher 可以对 Request 与 Policy 之间进行一些复杂的匹配。\n   \n   对于 RBAC 等访问控制模型，除了单纯的用户与权限之间存在关系之外，用户与角色（Role）之间还存在着继承关系。Casbin 中采用了 Role Manager 来为一条 Request 的用户以及其对应角色（包含继承角色）寻找与其相关的 Policy。同时，Role Manager 也支持添加自定义的 Function，来对用户与角色之间进行复杂的匹配。\n\n4. 在实际应用中，一条 Request 可能会匹配到多条 Policy。得到所有的 Policy 后，需要进一步将多条 Policy 的结果进行聚合，得到最终是否允许 Request。Effector 根据 Model 中配置的规则，对所有 Matched Policy 的 effect 项进行进一步的 eval。\n\n5. 在很多场景下，访问控制服务会有多个实例。Casbin 支持对 Policy 进行增量更新，那么，就需要 Dispatcher 维护多个 Casbin 实例的 Policy 之间的一致性。Dispatcher 主要提供两部分的功能，一部分是 Casbin 的 API，另一部分是 Dispatcher 自身的 API，用来实现成员管理等一致性问题，可以通过 Raft 等共识算法实现。\n\n## Usage\n\n在了解了 Casbin 的原理和结构后，我们可以开始利用 Casbin 来进行一些实践。本章以 RBAC 模型为例，构建一个简单的访问控制示例。RBAC (Role-Based Access Control) 模型是基于角色的访问控制模型。在 RBAC 的模型中，用户和资源之间存在着角色（Role），用户可以属于一个或多个角色，角色拥有权限去访问资源。\n\n经过前两章的介绍，我们可以将访问控制分为三个部分：Static，Dynamic 和 User-specific Logic。在使用 Casbin 时，也可以这样进行划分。首先，我们先来定义一个静态的 RBAC Model（model.conf）。\n\n```c#\n[request_definition]\nr = sub, obj, act\n\n[policy_definition]\np = sub, obj, act\n\n[role_definition]\ng = _, _\n\n[policy_effect]\ne = some(where (p.eft == allow))\n\n[matchers]\nm = g(r.sub, p.sub) && r.obj == p.obj && r.act == p.act\n```\n\n在这个模型中，分别定义了五部分内容。\n\n1. **request_definition**，定义了 Request 的结构。这份示例中包含了访问者（sub），被访问者（obj）和操作（act）。\n2. **policy_definition**，定义了 Policy 的结构。通常，由于 Policy 和 Request 之间要进行匹配，所以两者的结构有一定的相似性。\n3. **role_definition**，定义了 Role Manager。`g` 定义了一套 RBAC 系统，换句话说是一组用户角色继承关系的集合。在实际使用中，更类似于一个函数，判断输入的参数是否在这个集合中存在继承关系。\n4. **policy_effect**，定义了如何对多个匹配到的 Policy 做合并。目前，Casbin 支持几个固定语法的合并模式，在官网 [<sup>3</sup>](#policy) 上有着详细的介绍。这些模式的含义也很好理解，模式的语法与自然语言或者 SQL 非常相近。例如，`some(where (p.eft == allow))` 表示的是当任意一个 Policy 的 effect 是 allow，那么合并的结果即为 allow。\n5. **matchers**，定义了如何匹配 Policy 和 Request。 定义公式的语法与常见语言中的布尔表达式相似，通过 `==` 可以将 Policy 和 Request 中的各项进行对比。\n\n#### Policy\n\n```\np, alice, data1, read\np, bob, data2, write\np, data2_admin, data2, read\np, data2_admin, data2, write\ng, alice, data2_admin\n```\n\n接下来，我们可以按照 Model 中定义的 Policy 结构来编写 Policy 实例。例如，第一项 `p, alice, data1, read` 与 `p = sub, obj, act` 相对应，`alice`，`data1`，`read` 与 `sub`，`obj`，`act` 相对应。另外一条比较特殊的实例是最后一项，`g, alice, data2_admin`，定义了用户 `alice` 继承了 `data2_admin` 这一角色。\n\n我们可以将上面的 Policy 保存在文件 policy.csv 中。但一般来说，Policy 储存在数据库等等一些更加 organized 的外部存储中。\n\n#### User Logic\n\nCasbin 几乎支持所有的常见的编程语言，用户使用的逻辑也基本相似，主要通过 Enforcer 类来进行操作。\n\n```go\ne, err := casbin.NewEnforcer(\"model.conf\", \"policy.csv\")\nresult1, _ := e.Enforce(\"alice\", \"data1\", \"read\")\nfmt.Println(result1)\nresult2, _ := e.Enforce(\"alice\", \"data1\", \"write\")\nfmt.Println(result2)\nresult3, _ := e.Enforce(\"alice\", \"data2\", \"read\")\nfmt.Println(result3)\n```\n\n通过 Enforce 方法，开发者输入 Request，就可以得到这条请求是否可以通过。在上述例子中有 3 个 test case，分别验证了合法请求匹配，非法请求匹配，集成角色请求匹配。第一个 test case 对应了 policy.csv 中的第一条 Policy，第二个 test case 则没有 test case。第三个 test case 通过 `g, alice, data2_admin` 将 alice 与 data2_admin 的 Policy 关联起来，然后通过第三条 Policy p, data2_admin, data2, read 验证其为合法请求。\n\n## Reference\n<div id=\"ref1\"/>\n- [1] https://github.com/casbin/casbin\n<div id=\"ref2\"/>\n- [2] https://arxiv.org/pdf/1903.09756.pdf\n<div id=\"ref3\"/>\n- [3] https://casbin.org/docs/en/syntax-for-models#policy-effect\n","slug":"casbin-ramp-up","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl5i8k7y100041rpfczvk31af","content":"<h2 id=\"TL-DR\"><a href=\"#TL-DR\" class=\"headerlink\" title=\"TL;DR\"></a>TL;DR</h2><ul>\n<li>访问控制框架 Casbin 的原理以及其内部组件的结构</li>\n<li>以一个 RBAC 的简单例子介绍 Casbin 的用法</li>\n</ul>\n<h2 id=\"Casbin是什么？\"><a href=\"#Casbin是什么？\" class=\"headerlink\" title=\"Casbin是什么？\"></a>Casbin是什么？</h2><h3 id=\"访问控制\"><a href=\"#访问控制\" class=\"headerlink\" title=\"访问控制\"></a>访问控制</h3><p><img src=\"/asset/casbin-ramp-up/2022-06-23-17-50-00-image.png\"></p>\n<p>访问控制，顾名思义，是指判断一条请求是否可以访问受保护的资源的技术。在上图的例子中，我们的后台中有两个资源，Resource1和Resource2。它们可以是服务器、账号、图片、视频等等等等。但是，它们的相同特性是不能被所有用户都访问。比如 Resource1 属于用户 Alice，那么只有 Alice 能够访问它，Bob 则不能。因此，我们就需要对访问请求进行过滤，判断其是否被允许到达目标资源。在上面的例子中，Alice 发起了两个访问请求，分别想要访问 Resource1 和 Resource2。访问控制层需要做的工作就是允许访问 Resource1 的请求通过，而阻拦想要访问 Resource2 的请求，因为 Resource2 属于 Bob，Alice 是无法访问的。</p>\n<p>在实际应用中，访问控制问题往往会随着业务而变得非常复杂。而 Casbin <a href=\"#casbin\"><sup>1</sup></a> 就是一个强大的、高效的开源访问控制框架。Casbin 在 Github 上已获得超过 10k+ star，并且有着非常完整的生态。基于 Casbin 可以轻松的实现一系列访问控制模型，如 RBAC，ABAC等等。</p>\n<h3 id=\"原理——PML\"><a href=\"#原理——PML\" class=\"headerlink\" title=\"原理——PML\"></a>原理——PML</h3><p>Casbin 的底层原理基于其创建者罗杨博士所发表的一篇论文《PML: An Interpreter-Based Access Control Policy Language for Web Services》<a href=\"#pml\"><sup>2</sup></a></p>\n<h4 id=\"设计目标\"><a href=\"#设计目标\" class=\"headerlink\" title=\"设计目标\"></a>设计目标</h4><p>这篇论文主要关注于如何解决现实中云服务厂商有关权限校验所遇到的两个问题：</p>\n<ol>\n<li>每个云服务厂商都有着自己的一套权限检验规则。这对于在多个云环境都进行部署的用户来说，造成了很大的迁移和维护成本。</li>\n<li>同样的，维护自己的一套权限校验规则对于云服务厂商来说，也是一个挑战。如果云服务厂商缺乏在这方面的相关经验，就很有可能造成安全漏洞。</li>\n</ol>\n<p>既然文章的目标本质上是通过通用性来解决问题，作者也考虑了如何实现这个目标，提出了两个 independent 的设计要求：</p>\n<ol>\n<li>Access Control Model Independent：PML 既需要支持用户可以在多个云服务厂商中使用同一个模型，也需要支持用户在不改变校验代码的同时，可以切换不同的模型。</li>\n<li>Implementation Language Independent: PML 的设计不应该依赖于某种编程语言的特性。<br>因此，文中提出了一种新的权限校验语言——PML (PERM Modeling Language)，希望通过一种支持多种权限校验模型的配置语言来弥补这个 gap。</li>\n</ol>\n<h4 id=\"设计实现\"><a href=\"#设计实现\" class=\"headerlink\" title=\"设计实现\"></a>设计实现</h4><p>在介绍 PML 的设计之前，我们可以先大致了解一下访问控制问题中，所涉及到的一些概念。</p>\n<p><img src=\"/asset/casbin-ramp-up/overview.png\" alt=\"1c2dea1652f67c0b7920b0471a4113afd8d9325a.png\"></p>\n<p>一般来说，访问控制会涉及到两个部分：</p>\n<ol>\n<li><strong>Model 访问控制模型</strong>。常见的模型有 ACL（Access Control List 访问控制列表），RBAC（Role-Based Access Control 基于角色的访问控制），ABAC（Attribute-Based Access Control 基于属性的访问控制）。对于一个应用来说，Model 的选择是与应用的业务逻辑是密切相关的，因此也是相对静态的。一旦代码编译完成，这部分是不会随着应用的运行而产生变化的。</li>\n<li><strong>Policy 访问控制规则</strong>。Policy 是和 Model 相对应的，每种不同的 Model，都会有不同格式的 Policy。而与 Model 完全相反的是，Policy 是相对动态的。在编写代码的过程中，我们只能去定义 Policy 的格式，而 Policy 的具体内容都是应用运行过程中添加或修改的。例如，有一个新用户注册了我们的应用。那么，我们就需要动态的为其添加一条 Policy。</li>\n</ol>\n<p>我们可以将这两部分理解为传统应用中的代码和数据。有了这两部分后，再加上用户特定的校验逻辑，那么就可以完成访问控制任务。</p>\n<h5 id=\"PERM-模型\"><a href=\"#PERM-模型\" class=\"headerlink\" title=\"PERM 模型\"></a>PERM 模型</h5><p>当然，对于现实环境中复杂的情况，简单地将问题建模为这两部分肯定是不够的，因此，论文提出了一个新的元模型 PERM（Policy-Effect-Request-Matcher）。</p>\n<p><img src=\"/asset/casbin-ramp-up/pml.png\" alt=\"6d8fa0a037c3ea185cfadb8d817c41cce0d66d78.png\"></p>\n<p>PERM 模型主要包含了 6 个主要的概念：</p>\n<ol>\n<li><strong>Request</strong>：访问请求定义。用户真实的访问请求，通常会包含 sub（访问者），obj（被访问的资源）， act（访问时所进行的操作）或其他用户自定义的属性。</li>\n<li><strong>Policy</strong>：访问控制规则定义。定义了需要对访问请求的哪些属性进行校验。</li>\n<li><strong>Policy Rule</strong>：访问控制规则实例。</li>\n<li><strong>Matcher</strong>：如何为一条 Request 匹配到其对应的 Policy Rule。</li>\n<li><strong>Effect</strong>：当一条 Request 匹配到了一条或多条 Policy Rule，如何判断其是否应该被允许。</li>\n<li><strong>Stub Function</strong>：在实际应用中，Request 实例 和 Policy Rule 的匹配往往无法通过简单的 == 等于来解决，例如通配符等等。所以 Stub Function 允许用户自定义一些复杂的匹配方法。</li>\n</ol>\n<p>这六个更加详细的建模了访问控制的问题。我们也可以对其简单的分一下类，Request，Policy，Matcher，Effect 和 Stub Function 都是静态的，属于 Model 的一部分。通过这个五项的组合，ACL等常见的模型以及一些用户自定义的规则，都可以很轻易的表示出来。在最后一部分中，会以 RBAC 为例，介绍如何通过 PML 实现这样一个模型。</p>\n<p>而 Policy Rule 就属于动态变化的内容。在实际实现中，往往也是像数据一样，存储在数据库当中的。</p>\n<h2 id=\"结构\"><a href=\"#结构\" class=\"headerlink\" title=\"结构\"></a>结构</h2><p>与论文中的实现相比，目前 Casbin 的实现更加强大，支持了更多功能。所以，这里以 Casbin 主库（Go 版本），介绍 Casbin 是如何进行工作的。</p>\n<p><img src=\"/asset/casbin-ramp-up/detail.png\" alt=\"6dbf7fb95022569c5ad99becdcd9090df8a99250.png\"></p>\n<ol>\n<li><p>在应用启动时，Casbin 会读取用户已经定义好的 Model，其中会包含 Request, Policy, Matcher 和 Effector 四个部分的定义。同时，Casbin 会利用 Adapter，从数据源处读取 Policy 实例（也就是上文提到的 Policy Rule）。后文就将 Policy 实例简称为 Policy。</p>\n</li>\n<li><p>对于 Policy 的存储和读取，Casbin 将其解耦到了独立的 Adapter 模块。通过使用不同的 Adapter（File, MySQL等等），可以从不同的数据源中读取 Policy。对于 Policy 比较多的场景，将所有的 Policy 同时加载进内存，确实会导致一定的性能损失。所以，在加载时，部分 Adapter 也提供 <code>LoadFilteredPolicy</code> 的接口，通过只加载 Policy 的一部分子集，减少这部分带来的性能瓶颈。</p>\n</li>\n<li><p>在一条请求到来时，该请求首先会按照 Model 中的定义进行拆分。接下来，Matcher 会根据 Model 中定义的规则，与 Policy 进行匹配。除了支持 == 强匹配外，Matcher 还支持通过 Function 和 Role Manager 进行模糊匹配。Function 像用户提供了自定义匹配规则的接口。通过向 Matcher 传入自定义函数，Matcher 可以对 Request 与 Policy 之间进行一些复杂的匹配。</p>\n<p>对于 RBAC 等访问控制模型，除了单纯的用户与权限之间存在关系之外，用户与角色（Role）之间还存在着继承关系。Casbin 中采用了 Role Manager 来为一条 Request 的用户以及其对应角色（包含继承角色）寻找与其相关的 Policy。同时，Role Manager 也支持添加自定义的 Function，来对用户与角色之间进行复杂的匹配。</p>\n</li>\n<li><p>在实际应用中，一条 Request 可能会匹配到多条 Policy。得到所有的 Policy 后，需要进一步将多条 Policy 的结果进行聚合，得到最终是否允许 Request。Effector 根据 Model 中配置的规则，对所有 Matched Policy 的 effect 项进行进一步的 eval。</p>\n</li>\n<li><p>在很多场景下，访问控制服务会有多个实例。Casbin 支持对 Policy 进行增量更新，那么，就需要 Dispatcher 维护多个 Casbin 实例的 Policy 之间的一致性。Dispatcher 主要提供两部分的功能，一部分是 Casbin 的 API，另一部分是 Dispatcher 自身的 API，用来实现成员管理等一致性问题，可以通过 Raft 等共识算法实现。</p>\n</li>\n</ol>\n<h2 id=\"Usage\"><a href=\"#Usage\" class=\"headerlink\" title=\"Usage\"></a>Usage</h2><p>在了解了 Casbin 的原理和结构后，我们可以开始利用 Casbin 来进行一些实践。本章以 RBAC 模型为例，构建一个简单的访问控制示例。RBAC (Role-Based Access Control) 模型是基于角色的访问控制模型。在 RBAC 的模型中，用户和资源之间存在着角色（Role），用户可以属于一个或多个角色，角色拥有权限去访问资源。</p>\n<p>经过前两章的介绍，我们可以将访问控制分为三个部分：Static，Dynamic 和 User-specific Logic。在使用 Casbin 时，也可以这样进行划分。首先，我们先来定义一个静态的 RBAC Model（model.conf）。</p>\n<pre><code class=\"c#\">[request_definition]\nr = sub, obj, act\n\n[policy_definition]\np = sub, obj, act\n\n[role_definition]\ng = _, _\n\n[policy_effect]\ne = some(where (p.eft == allow))\n\n[matchers]\nm = g(r.sub, p.sub) &amp;&amp; r.obj == p.obj &amp;&amp; r.act == p.act\n</code></pre>\n<p>在这个模型中，分别定义了五部分内容。</p>\n<ol>\n<li><strong>request_definition</strong>，定义了 Request 的结构。这份示例中包含了访问者（sub），被访问者（obj）和操作（act）。</li>\n<li><strong>policy_definition</strong>，定义了 Policy 的结构。通常，由于 Policy 和 Request 之间要进行匹配，所以两者的结构有一定的相似性。</li>\n<li><strong>role_definition</strong>，定义了 Role Manager。<code>g</code> 定义了一套 RBAC 系统，换句话说是一组用户角色继承关系的集合。在实际使用中，更类似于一个函数，判断输入的参数是否在这个集合中存在继承关系。</li>\n<li><strong>policy_effect</strong>，定义了如何对多个匹配到的 Policy 做合并。目前，Casbin 支持几个固定语法的合并模式，在官网 <a href=\"#policy\"><sup>3</sup></a> 上有着详细的介绍。这些模式的含义也很好理解，模式的语法与自然语言或者 SQL 非常相近。例如，<code>some(where (p.eft == allow))</code> 表示的是当任意一个 Policy 的 effect 是 allow，那么合并的结果即为 allow。</li>\n<li><strong>matchers</strong>，定义了如何匹配 Policy 和 Request。 定义公式的语法与常见语言中的布尔表达式相似，通过 <code>==</code> 可以将 Policy 和 Request 中的各项进行对比。</li>\n</ol>\n<h4 id=\"Policy\"><a href=\"#Policy\" class=\"headerlink\" title=\"Policy\"></a>Policy</h4><pre><code>p, alice, data1, read\np, bob, data2, write\np, data2_admin, data2, read\np, data2_admin, data2, write\ng, alice, data2_admin\n</code></pre>\n<p>接下来，我们可以按照 Model 中定义的 Policy 结构来编写 Policy 实例。例如，第一项 <code>p, alice, data1, read</code> 与 <code>p = sub, obj, act</code> 相对应，<code>alice</code>，<code>data1</code>，<code>read</code> 与 <code>sub</code>，<code>obj</code>，<code>act</code> 相对应。另外一条比较特殊的实例是最后一项，<code>g, alice, data2_admin</code>，定义了用户 <code>alice</code> 继承了 <code>data2_admin</code> 这一角色。</p>\n<p>我们可以将上面的 Policy 保存在文件 policy.csv 中。但一般来说，Policy 储存在数据库等等一些更加 organized 的外部存储中。</p>\n<h4 id=\"User-Logic\"><a href=\"#User-Logic\" class=\"headerlink\" title=\"User Logic\"></a>User Logic</h4><p>Casbin 几乎支持所有的常见的编程语言，用户使用的逻辑也基本相似，主要通过 Enforcer 类来进行操作。</p>\n<pre><code class=\"go\">e, err := casbin.NewEnforcer(&quot;model.conf&quot;, &quot;policy.csv&quot;)\nresult1, _ := e.Enforce(&quot;alice&quot;, &quot;data1&quot;, &quot;read&quot;)\nfmt.Println(result1)\nresult2, _ := e.Enforce(&quot;alice&quot;, &quot;data1&quot;, &quot;write&quot;)\nfmt.Println(result2)\nresult3, _ := e.Enforce(&quot;alice&quot;, &quot;data2&quot;, &quot;read&quot;)\nfmt.Println(result3)\n</code></pre>\n<p>通过 Enforce 方法，开发者输入 Request，就可以得到这条请求是否可以通过。在上述例子中有 3 个 test case，分别验证了合法请求匹配，非法请求匹配，集成角色请求匹配。第一个 test case 对应了 policy.csv 中的第一条 Policy，第二个 test case 则没有 test case。第三个 test case 通过 <code>g, alice, data2_admin</code> 将 alice 与 data2_admin 的 Policy 关联起来，然后通过第三条 Policy p, data2_admin, data2, read 验证其为合法请求。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><div id=\"ref1\"/>\n- [1] https://github.com/casbin/casbin\n<div id=\"ref2\"/>\n- [2] https://arxiv.org/pdf/1903.09756.pdf\n<div id=\"ref3\"/>\n- [3] https://casbin.org/docs/en/syntax-for-models#policy-effect\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"TL-DR\"><a href=\"#TL-DR\" class=\"headerlink\" title=\"TL;DR\"></a>TL;DR</h2><ul>\n<li>访问控制框架 Casbin 的原理以及其内部组件的结构</li>\n<li>以一个 RBAC 的简单例子介绍 Casbin 的用法</li>\n</ul>\n<h2 id=\"Casbin是什么？\"><a href=\"#Casbin是什么？\" class=\"headerlink\" title=\"Casbin是什么？\"></a>Casbin是什么？</h2><h3 id=\"访问控制\"><a href=\"#访问控制\" class=\"headerlink\" title=\"访问控制\"></a>访问控制</h3><p><img src=\"/asset/casbin-ramp-up/2022-06-23-17-50-00-image.png\"></p>\n<p>访问控制，顾名思义，是指判断一条请求是否可以访问受保护的资源的技术。在上图的例子中，我们的后台中有两个资源，Resource1和Resource2。它们可以是服务器、账号、图片、视频等等等等。但是，它们的相同特性是不能被所有用户都访问。比如 Resource1 属于用户 Alice，那么只有 Alice 能够访问它，Bob 则不能。因此，我们就需要对访问请求进行过滤，判断其是否被允许到达目标资源。在上面的例子中，Alice 发起了两个访问请求，分别想要访问 Resource1 和 Resource2。访问控制层需要做的工作就是允许访问 Resource1 的请求通过，而阻拦想要访问 Resource2 的请求，因为 Resource2 属于 Bob，Alice 是无法访问的。</p>\n<p>在实际应用中，访问控制问题往往会随着业务而变得非常复杂。而 Casbin <a href=\"#casbin\"><sup>1</sup></a> 就是一个强大的、高效的开源访问控制框架。Casbin 在 Github 上已获得超过 10k+ star，并且有着非常完整的生态。基于 Casbin 可以轻松的实现一系列访问控制模型，如 RBAC，ABAC等等。</p>\n<h3 id=\"原理——PML\"><a href=\"#原理——PML\" class=\"headerlink\" title=\"原理——PML\"></a>原理——PML</h3><p>Casbin 的底层原理基于其创建者罗杨博士所发表的一篇论文《PML: An Interpreter-Based Access Control Policy Language for Web Services》<a href=\"#pml\"><sup>2</sup></a></p>\n<h4 id=\"设计目标\"><a href=\"#设计目标\" class=\"headerlink\" title=\"设计目标\"></a>设计目标</h4><p>这篇论文主要关注于如何解决现实中云服务厂商有关权限校验所遇到的两个问题：</p>\n<ol>\n<li>每个云服务厂商都有着自己的一套权限检验规则。这对于在多个云环境都进行部署的用户来说，造成了很大的迁移和维护成本。</li>\n<li>同样的，维护自己的一套权限校验规则对于云服务厂商来说，也是一个挑战。如果云服务厂商缺乏在这方面的相关经验，就很有可能造成安全漏洞。</li>\n</ol>\n<p>既然文章的目标本质上是通过通用性来解决问题，作者也考虑了如何实现这个目标，提出了两个 independent 的设计要求：</p>\n<ol>\n<li>Access Control Model Independent：PML 既需要支持用户可以在多个云服务厂商中使用同一个模型，也需要支持用户在不改变校验代码的同时，可以切换不同的模型。</li>\n<li>Implementation Language Independent: PML 的设计不应该依赖于某种编程语言的特性。<br>因此，文中提出了一种新的权限校验语言——PML (PERM Modeling Language)，希望通过一种支持多种权限校验模型的配置语言来弥补这个 gap。</li>\n</ol>\n<h4 id=\"设计实现\"><a href=\"#设计实现\" class=\"headerlink\" title=\"设计实现\"></a>设计实现</h4><p>在介绍 PML 的设计之前，我们可以先大致了解一下访问控制问题中，所涉及到的一些概念。</p>\n<p><img src=\"/asset/casbin-ramp-up/overview.png\" alt=\"1c2dea1652f67c0b7920b0471a4113afd8d9325a.png\"></p>\n<p>一般来说，访问控制会涉及到两个部分：</p>\n<ol>\n<li><strong>Model 访问控制模型</strong>。常见的模型有 ACL（Access Control List 访问控制列表），RBAC（Role-Based Access Control 基于角色的访问控制），ABAC（Attribute-Based Access Control 基于属性的访问控制）。对于一个应用来说，Model 的选择是与应用的业务逻辑是密切相关的，因此也是相对静态的。一旦代码编译完成，这部分是不会随着应用的运行而产生变化的。</li>\n<li><strong>Policy 访问控制规则</strong>。Policy 是和 Model 相对应的，每种不同的 Model，都会有不同格式的 Policy。而与 Model 完全相反的是，Policy 是相对动态的。在编写代码的过程中，我们只能去定义 Policy 的格式，而 Policy 的具体内容都是应用运行过程中添加或修改的。例如，有一个新用户注册了我们的应用。那么，我们就需要动态的为其添加一条 Policy。</li>\n</ol>\n<p>我们可以将这两部分理解为传统应用中的代码和数据。有了这两部分后，再加上用户特定的校验逻辑，那么就可以完成访问控制任务。</p>\n<h5 id=\"PERM-模型\"><a href=\"#PERM-模型\" class=\"headerlink\" title=\"PERM 模型\"></a>PERM 模型</h5><p>当然，对于现实环境中复杂的情况，简单地将问题建模为这两部分肯定是不够的，因此，论文提出了一个新的元模型 PERM（Policy-Effect-Request-Matcher）。</p>\n<p><img src=\"/asset/casbin-ramp-up/pml.png\" alt=\"6d8fa0a037c3ea185cfadb8d817c41cce0d66d78.png\"></p>\n<p>PERM 模型主要包含了 6 个主要的概念：</p>\n<ol>\n<li><strong>Request</strong>：访问请求定义。用户真实的访问请求，通常会包含 sub（访问者），obj（被访问的资源）， act（访问时所进行的操作）或其他用户自定义的属性。</li>\n<li><strong>Policy</strong>：访问控制规则定义。定义了需要对访问请求的哪些属性进行校验。</li>\n<li><strong>Policy Rule</strong>：访问控制规则实例。</li>\n<li><strong>Matcher</strong>：如何为一条 Request 匹配到其对应的 Policy Rule。</li>\n<li><strong>Effect</strong>：当一条 Request 匹配到了一条或多条 Policy Rule，如何判断其是否应该被允许。</li>\n<li><strong>Stub Function</strong>：在实际应用中，Request 实例 和 Policy Rule 的匹配往往无法通过简单的 == 等于来解决，例如通配符等等。所以 Stub Function 允许用户自定义一些复杂的匹配方法。</li>\n</ol>\n<p>这六个更加详细的建模了访问控制的问题。我们也可以对其简单的分一下类，Request，Policy，Matcher，Effect 和 Stub Function 都是静态的，属于 Model 的一部分。通过这个五项的组合，ACL等常见的模型以及一些用户自定义的规则，都可以很轻易的表示出来。在最后一部分中，会以 RBAC 为例，介绍如何通过 PML 实现这样一个模型。</p>\n<p>而 Policy Rule 就属于动态变化的内容。在实际实现中，往往也是像数据一样，存储在数据库当中的。</p>\n<h2 id=\"结构\"><a href=\"#结构\" class=\"headerlink\" title=\"结构\"></a>结构</h2><p>与论文中的实现相比，目前 Casbin 的实现更加强大，支持了更多功能。所以，这里以 Casbin 主库（Go 版本），介绍 Casbin 是如何进行工作的。</p>\n<p><img src=\"/asset/casbin-ramp-up/detail.png\" alt=\"6dbf7fb95022569c5ad99becdcd9090df8a99250.png\"></p>\n<ol>\n<li><p>在应用启动时，Casbin 会读取用户已经定义好的 Model，其中会包含 Request, Policy, Matcher 和 Effector 四个部分的定义。同时，Casbin 会利用 Adapter，从数据源处读取 Policy 实例（也就是上文提到的 Policy Rule）。后文就将 Policy 实例简称为 Policy。</p>\n</li>\n<li><p>对于 Policy 的存储和读取，Casbin 将其解耦到了独立的 Adapter 模块。通过使用不同的 Adapter（File, MySQL等等），可以从不同的数据源中读取 Policy。对于 Policy 比较多的场景，将所有的 Policy 同时加载进内存，确实会导致一定的性能损失。所以，在加载时，部分 Adapter 也提供 <code>LoadFilteredPolicy</code> 的接口，通过只加载 Policy 的一部分子集，减少这部分带来的性能瓶颈。</p>\n</li>\n<li><p>在一条请求到来时，该请求首先会按照 Model 中的定义进行拆分。接下来，Matcher 会根据 Model 中定义的规则，与 Policy 进行匹配。除了支持 == 强匹配外，Matcher 还支持通过 Function 和 Role Manager 进行模糊匹配。Function 像用户提供了自定义匹配规则的接口。通过向 Matcher 传入自定义函数，Matcher 可以对 Request 与 Policy 之间进行一些复杂的匹配。</p>\n<p>对于 RBAC 等访问控制模型，除了单纯的用户与权限之间存在关系之外，用户与角色（Role）之间还存在着继承关系。Casbin 中采用了 Role Manager 来为一条 Request 的用户以及其对应角色（包含继承角色）寻找与其相关的 Policy。同时，Role Manager 也支持添加自定义的 Function，来对用户与角色之间进行复杂的匹配。</p>\n</li>\n<li><p>在实际应用中，一条 Request 可能会匹配到多条 Policy。得到所有的 Policy 后，需要进一步将多条 Policy 的结果进行聚合，得到最终是否允许 Request。Effector 根据 Model 中配置的规则，对所有 Matched Policy 的 effect 项进行进一步的 eval。</p>\n</li>\n<li><p>在很多场景下，访问控制服务会有多个实例。Casbin 支持对 Policy 进行增量更新，那么，就需要 Dispatcher 维护多个 Casbin 实例的 Policy 之间的一致性。Dispatcher 主要提供两部分的功能，一部分是 Casbin 的 API，另一部分是 Dispatcher 自身的 API，用来实现成员管理等一致性问题，可以通过 Raft 等共识算法实现。</p>\n</li>\n</ol>\n<h2 id=\"Usage\"><a href=\"#Usage\" class=\"headerlink\" title=\"Usage\"></a>Usage</h2><p>在了解了 Casbin 的原理和结构后，我们可以开始利用 Casbin 来进行一些实践。本章以 RBAC 模型为例，构建一个简单的访问控制示例。RBAC (Role-Based Access Control) 模型是基于角色的访问控制模型。在 RBAC 的模型中，用户和资源之间存在着角色（Role），用户可以属于一个或多个角色，角色拥有权限去访问资源。</p>\n<p>经过前两章的介绍，我们可以将访问控制分为三个部分：Static，Dynamic 和 User-specific Logic。在使用 Casbin 时，也可以这样进行划分。首先，我们先来定义一个静态的 RBAC Model（model.conf）。</p>\n<pre><code class=\"c#\">[request_definition]\nr = sub, obj, act\n\n[policy_definition]\np = sub, obj, act\n\n[role_definition]\ng = _, _\n\n[policy_effect]\ne = some(where (p.eft == allow))\n\n[matchers]\nm = g(r.sub, p.sub) &amp;&amp; r.obj == p.obj &amp;&amp; r.act == p.act\n</code></pre>\n<p>在这个模型中，分别定义了五部分内容。</p>\n<ol>\n<li><strong>request_definition</strong>，定义了 Request 的结构。这份示例中包含了访问者（sub），被访问者（obj）和操作（act）。</li>\n<li><strong>policy_definition</strong>，定义了 Policy 的结构。通常，由于 Policy 和 Request 之间要进行匹配，所以两者的结构有一定的相似性。</li>\n<li><strong>role_definition</strong>，定义了 Role Manager。<code>g</code> 定义了一套 RBAC 系统，换句话说是一组用户角色继承关系的集合。在实际使用中，更类似于一个函数，判断输入的参数是否在这个集合中存在继承关系。</li>\n<li><strong>policy_effect</strong>，定义了如何对多个匹配到的 Policy 做合并。目前，Casbin 支持几个固定语法的合并模式，在官网 <a href=\"#policy\"><sup>3</sup></a> 上有着详细的介绍。这些模式的含义也很好理解，模式的语法与自然语言或者 SQL 非常相近。例如，<code>some(where (p.eft == allow))</code> 表示的是当任意一个 Policy 的 effect 是 allow，那么合并的结果即为 allow。</li>\n<li><strong>matchers</strong>，定义了如何匹配 Policy 和 Request。 定义公式的语法与常见语言中的布尔表达式相似，通过 <code>==</code> 可以将 Policy 和 Request 中的各项进行对比。</li>\n</ol>\n<h4 id=\"Policy\"><a href=\"#Policy\" class=\"headerlink\" title=\"Policy\"></a>Policy</h4><pre><code>p, alice, data1, read\np, bob, data2, write\np, data2_admin, data2, read\np, data2_admin, data2, write\ng, alice, data2_admin\n</code></pre>\n<p>接下来，我们可以按照 Model 中定义的 Policy 结构来编写 Policy 实例。例如，第一项 <code>p, alice, data1, read</code> 与 <code>p = sub, obj, act</code> 相对应，<code>alice</code>，<code>data1</code>，<code>read</code> 与 <code>sub</code>，<code>obj</code>，<code>act</code> 相对应。另外一条比较特殊的实例是最后一项，<code>g, alice, data2_admin</code>，定义了用户 <code>alice</code> 继承了 <code>data2_admin</code> 这一角色。</p>\n<p>我们可以将上面的 Policy 保存在文件 policy.csv 中。但一般来说，Policy 储存在数据库等等一些更加 organized 的外部存储中。</p>\n<h4 id=\"User-Logic\"><a href=\"#User-Logic\" class=\"headerlink\" title=\"User Logic\"></a>User Logic</h4><p>Casbin 几乎支持所有的常见的编程语言，用户使用的逻辑也基本相似，主要通过 Enforcer 类来进行操作。</p>\n<pre><code class=\"go\">e, err := casbin.NewEnforcer(&quot;model.conf&quot;, &quot;policy.csv&quot;)\nresult1, _ := e.Enforce(&quot;alice&quot;, &quot;data1&quot;, &quot;read&quot;)\nfmt.Println(result1)\nresult2, _ := e.Enforce(&quot;alice&quot;, &quot;data1&quot;, &quot;write&quot;)\nfmt.Println(result2)\nresult3, _ := e.Enforce(&quot;alice&quot;, &quot;data2&quot;, &quot;read&quot;)\nfmt.Println(result3)\n</code></pre>\n<p>通过 Enforce 方法，开发者输入 Request，就可以得到这条请求是否可以通过。在上述例子中有 3 个 test case，分别验证了合法请求匹配，非法请求匹配，集成角色请求匹配。第一个 test case 对应了 policy.csv 中的第一条 Policy，第二个 test case 则没有 test case。第三个 test case 通过 <code>g, alice, data2_admin</code> 将 alice 与 data2_admin 的 Policy 关联起来，然后通过第三条 Policy p, data2_admin, data2, read 验证其为合法请求。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><div id=\"ref1\"/>\n- [1] https://github.com/casbin/casbin\n<div id=\"ref2\"/>\n- [2] https://arxiv.org/pdf/1903.09756.pdf\n<div id=\"ref3\"/>\n- [3] https://casbin.org/docs/en/syntax-for-models#policy-effect\n"},{"title":"[BugFix] Kubernetes Ingress Nginx DNS 报错日志 Bug Fix","date":"2021-03-13T12:27:49.000Z","updated":"2021-03-13T12:27:49.000Z","_content":"## 问题\n目前，当指定访问集群外部地址为 IP 时，ingress-nginx controller 的日志中存在大量的 DNS 报错的垃圾日志。虽然不影响正常运行（猜测可能会导致性能波动，对比见最后），但是查看 Nginx 日志 debug 时效率严重降低。\n\n![](/asset/ingress-nginx-bug-fix/error.png)\n\n## 原因\n详细的讨论见：\n[https://github.com/coredns/coredns/issues/2324](https://github.com/coredns/coredns/issues/2324)\n\n简略总结下，导致访问外部 IP，Nginx 报 DNS 解析错误的原因在于 Kubernetes 自身的bug，缺少了一个验证。\n\n出现问题的情况是通过 ExternalName 类型的 Service 访问外部服务的。定义的 yaml 类似于下面这种：\n```yaml\nkind: Service\napiVersion: v1\nmetadata:\n  name: demo2\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: xxx.xxx.xxx.xxx\n```\n对于 Kubernetes 的设计来说，ExternalName 就是一个域名。K8s 官方是这样介绍的 \n\n> ExternalName: Maps the service to the contents of the externalName field (e.g. foo.bar.example.com), by returning aCNAMErecord with its value. No proxying of any kind is set up.\n\n> Note:ExternalName accepts an IPv4 address string, but as a DNS names comprised of digits, not as an IP address. ExternalNames that resemble IPv4 addresses are not resolved by CoreDNS or ingress-nginx because ExternalName is intended to specify a canonical DNS name.\n\n从实现上来看，ExternalName 类型的 Service 其实就是在 CoreDNS 里的一条 CNAME 记录。 CNAME 是一条域名指向另一个域名的记录，在 K8s 中，这条 record 记载的就是 Service 名字指向 ExtenalName 的一个映射。\n\n但是，当 ExternalName 类型的 Service 中设定的是 IP 时，K8s 并没有对其进行判断，仍然允许其正常创建。\n\n同时，Nginx 本身存在着一个轮询机制，会不断的向 DNS 服务拉取记录进行缓存。\n[https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua](https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua)\n\n在每次拉取缓存时会发生以下的过程：\n1. Nginx 从 CoreDNS 拉取到了一个 CNAME 记录，例如：demo2 -> xxx.xxx.xxx.xxx\n2. 接着，Nginx 尝试解析 xxx.xxx.xxx.xxx 这个域名，CoreDNS 自然是对这个长成 IP 样子的域名解析不出来的，于是解析失败，导致报错\n\n------\n\n至于为什么 DNS 解析失败之后，Nginx 仍然能够成功转发请求，原因是 ingress-nginx controller 在实现上并没有对这个进行区分。\n\n首先，先简单介绍下 controller 的原理。Ingress-nginx controller 一直监听着 k8s 系统中的 ingress 资源。当有新的 ingress 创建时，controller 会开始更新 Nginx 的配置文件，向其中添加转发规则，并重启 Nginx。\n\n下面是 controller 解析指向 ExternalName 的 ingress，然后创建 upstream 的逻辑\n[https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52](https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52)\n\n![](/asset/ingress-nginx-bug-fix/nginx-code.png)\n\n可以看到，controller 是没有强制解析 ExternalName 成域名的，所以写进 nginx.conf 的 upstream 也是 ip 形式，这样 nginx 会自然地将 ExternalName 解析成 IP，从而可以正常工作。\n\n## 解决方法\n在上面提到的 Github Issue 的讨论中，有大佬已经给出了解决方法，就是通过 Service without selectors 的方式。\nhttps://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors\n\n常见的 K8s Service 都是通过标签选择器，选择一系列 Pod 作为后端，K8s endpoint controller 会自动根据 Service 的声明去为 Service 的每个端口创建一个 endpoint。Endpoint 是 K8s 中实际进行服务路由的资源。\n\n而创建 Service without selectors，就需要我们手动去创建一个与 Service 同名的 endpoint。这样就不需要指定 Service 为 ExternalName 的类型，CoreDNS 中就会将其视作一条 A 记录，而不是一条 CNAME 记录。Nginx 拉取 DNS 缓存时也不会把 IP 当做域名了。\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: demo2\n  namespace: default\nspec:\n  clusterIP: None\n  ports:\n  - name: grpc\n    port: 32443\n    protocol: TCP\n---\nkind: Endpoints\napiVersion: v1\nmetadata:\n  name: demo2\n  namespace: default\nsubsets:\n  - addresses:\n      - ip: xxx.xxx.xxx.xxx\n    ports:\n      - port: 32443\n        name: grpc\n        protocol: TCP\n```\n\n## 对比\n在两个对等的集群发生通信时，demo1 修复，demo2不修复，对比两侧的 CPU 使用情况\n\ndemo1：\n![](/asset/ingress-nginx-bug-fix/demo1.png)\n\ndemo2：\n![](/asset/ingress-nginx-bug-fix/demo2.png)\n\ndemo2 大约比 demo1 消耗 CPU 多 0.020 个核。虽然这个报错会稍微增加一点 CPU 的使用量，但并不多。\n","source":"_posts/ingress-nginx-bug-fix.md","raw":"---\ntitle: \"[BugFix] Kubernetes Ingress Nginx DNS 报错日志 Bug Fix\"\ndate: 2021-03-13 12:27:49\nupdated: 2021-03-13 12:27:49\n---\n## 问题\n目前，当指定访问集群外部地址为 IP 时，ingress-nginx controller 的日志中存在大量的 DNS 报错的垃圾日志。虽然不影响正常运行（猜测可能会导致性能波动，对比见最后），但是查看 Nginx 日志 debug 时效率严重降低。\n\n![](/asset/ingress-nginx-bug-fix/error.png)\n\n## 原因\n详细的讨论见：\n[https://github.com/coredns/coredns/issues/2324](https://github.com/coredns/coredns/issues/2324)\n\n简略总结下，导致访问外部 IP，Nginx 报 DNS 解析错误的原因在于 Kubernetes 自身的bug，缺少了一个验证。\n\n出现问题的情况是通过 ExternalName 类型的 Service 访问外部服务的。定义的 yaml 类似于下面这种：\n```yaml\nkind: Service\napiVersion: v1\nmetadata:\n  name: demo2\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: xxx.xxx.xxx.xxx\n```\n对于 Kubernetes 的设计来说，ExternalName 就是一个域名。K8s 官方是这样介绍的 \n\n> ExternalName: Maps the service to the contents of the externalName field (e.g. foo.bar.example.com), by returning aCNAMErecord with its value. No proxying of any kind is set up.\n\n> Note:ExternalName accepts an IPv4 address string, but as a DNS names comprised of digits, not as an IP address. ExternalNames that resemble IPv4 addresses are not resolved by CoreDNS or ingress-nginx because ExternalName is intended to specify a canonical DNS name.\n\n从实现上来看，ExternalName 类型的 Service 其实就是在 CoreDNS 里的一条 CNAME 记录。 CNAME 是一条域名指向另一个域名的记录，在 K8s 中，这条 record 记载的就是 Service 名字指向 ExtenalName 的一个映射。\n\n但是，当 ExternalName 类型的 Service 中设定的是 IP 时，K8s 并没有对其进行判断，仍然允许其正常创建。\n\n同时，Nginx 本身存在着一个轮询机制，会不断的向 DNS 服务拉取记录进行缓存。\n[https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua](https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua)\n\n在每次拉取缓存时会发生以下的过程：\n1. Nginx 从 CoreDNS 拉取到了一个 CNAME 记录，例如：demo2 -> xxx.xxx.xxx.xxx\n2. 接着，Nginx 尝试解析 xxx.xxx.xxx.xxx 这个域名，CoreDNS 自然是对这个长成 IP 样子的域名解析不出来的，于是解析失败，导致报错\n\n------\n\n至于为什么 DNS 解析失败之后，Nginx 仍然能够成功转发请求，原因是 ingress-nginx controller 在实现上并没有对这个进行区分。\n\n首先，先简单介绍下 controller 的原理。Ingress-nginx controller 一直监听着 k8s 系统中的 ingress 资源。当有新的 ingress 创建时，controller 会开始更新 Nginx 的配置文件，向其中添加转发规则，并重启 Nginx。\n\n下面是 controller 解析指向 ExternalName 的 ingress，然后创建 upstream 的逻辑\n[https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52](https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52)\n\n![](/asset/ingress-nginx-bug-fix/nginx-code.png)\n\n可以看到，controller 是没有强制解析 ExternalName 成域名的，所以写进 nginx.conf 的 upstream 也是 ip 形式，这样 nginx 会自然地将 ExternalName 解析成 IP，从而可以正常工作。\n\n## 解决方法\n在上面提到的 Github Issue 的讨论中，有大佬已经给出了解决方法，就是通过 Service without selectors 的方式。\nhttps://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors\n\n常见的 K8s Service 都是通过标签选择器，选择一系列 Pod 作为后端，K8s endpoint controller 会自动根据 Service 的声明去为 Service 的每个端口创建一个 endpoint。Endpoint 是 K8s 中实际进行服务路由的资源。\n\n而创建 Service without selectors，就需要我们手动去创建一个与 Service 同名的 endpoint。这样就不需要指定 Service 为 ExternalName 的类型，CoreDNS 中就会将其视作一条 A 记录，而不是一条 CNAME 记录。Nginx 拉取 DNS 缓存时也不会把 IP 当做域名了。\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: demo2\n  namespace: default\nspec:\n  clusterIP: None\n  ports:\n  - name: grpc\n    port: 32443\n    protocol: TCP\n---\nkind: Endpoints\napiVersion: v1\nmetadata:\n  name: demo2\n  namespace: default\nsubsets:\n  - addresses:\n      - ip: xxx.xxx.xxx.xxx\n    ports:\n      - port: 32443\n        name: grpc\n        protocol: TCP\n```\n\n## 对比\n在两个对等的集群发生通信时，demo1 修复，demo2不修复，对比两侧的 CPU 使用情况\n\ndemo1：\n![](/asset/ingress-nginx-bug-fix/demo1.png)\n\ndemo2：\n![](/asset/ingress-nginx-bug-fix/demo2.png)\n\ndemo2 大约比 demo1 消耗 CPU 多 0.020 个核。虽然这个报错会稍微增加一点 CPU 的使用量，但并不多。\n","slug":"ingress-nginx-bug-fix","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl5i8k7y300051rpf87vag2ku","content":"<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>目前，当指定访问集群外部地址为 IP 时，ingress-nginx controller 的日志中存在大量的 DNS 报错的垃圾日志。虽然不影响正常运行（猜测可能会导致性能波动，对比见最后），但是查看 Nginx 日志 debug 时效率严重降低。</p>\n<p><img src=\"/asset/ingress-nginx-bug-fix/error.png\"></p>\n<h2 id=\"原因\"><a href=\"#原因\" class=\"headerlink\" title=\"原因\"></a>原因</h2><p>详细的讨论见：<br><a href=\"https://github.com/coredns/coredns/issues/2324\">https://github.com/coredns/coredns/issues/2324</a></p>\n<p>简略总结下，导致访问外部 IP，Nginx 报 DNS 解析错误的原因在于 Kubernetes 自身的bug，缺少了一个验证。</p>\n<p>出现问题的情况是通过 ExternalName 类型的 Service 访问外部服务的。定义的 yaml 类似于下面这种：</p>\n<pre><code class=\"yaml\">kind: Service\napiVersion: v1\nmetadata:\n  name: demo2\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: xxx.xxx.xxx.xxx\n</code></pre>\n<p>对于 Kubernetes 的设计来说，ExternalName 就是一个域名。K8s 官方是这样介绍的 </p>\n<blockquote>\n<p>ExternalName: Maps the service to the contents of the externalName field (e.g. foo.bar.example.com), by returning aCNAMErecord with its value. No proxying of any kind is set up.</p>\n</blockquote>\n<blockquote>\n<p>Note:ExternalName accepts an IPv4 address string, but as a DNS names comprised of digits, not as an IP address. ExternalNames that resemble IPv4 addresses are not resolved by CoreDNS or ingress-nginx because ExternalName is intended to specify a canonical DNS name.</p>\n</blockquote>\n<p>从实现上来看，ExternalName 类型的 Service 其实就是在 CoreDNS 里的一条 CNAME 记录。 CNAME 是一条域名指向另一个域名的记录，在 K8s 中，这条 record 记载的就是 Service 名字指向 ExtenalName 的一个映射。</p>\n<p>但是，当 ExternalName 类型的 Service 中设定的是 IP 时，K8s 并没有对其进行判断，仍然允许其正常创建。</p>\n<p>同时，Nginx 本身存在着一个轮询机制，会不断的向 DNS 服务拉取记录进行缓存。<br><a href=\"https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua\">https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua</a></p>\n<p>在每次拉取缓存时会发生以下的过程：</p>\n<ol>\n<li>Nginx 从 CoreDNS 拉取到了一个 CNAME 记录，例如：demo2 -&gt; xxx.xxx.xxx.xxx</li>\n<li>接着，Nginx 尝试解析 xxx.xxx.xxx.xxx 这个域名，CoreDNS 自然是对这个长成 IP 样子的域名解析不出来的，于是解析失败，导致报错</li>\n</ol>\n<hr>\n<p>至于为什么 DNS 解析失败之后，Nginx 仍然能够成功转发请求，原因是 ingress-nginx controller 在实现上并没有对这个进行区分。</p>\n<p>首先，先简单介绍下 controller 的原理。Ingress-nginx controller 一直监听着 k8s 系统中的 ingress 资源。当有新的 ingress 创建时，controller 会开始更新 Nginx 的配置文件，向其中添加转发规则，并重启 Nginx。</p>\n<p>下面是 controller 解析指向 ExternalName 的 ingress，然后创建 upstream 的逻辑<br><a href=\"https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52\">https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52</a></p>\n<p><img src=\"/asset/ingress-nginx-bug-fix/nginx-code.png\"></p>\n<p>可以看到，controller 是没有强制解析 ExternalName 成域名的，所以写进 nginx.conf 的 upstream 也是 ip 形式，这样 nginx 会自然地将 ExternalName 解析成 IP，从而可以正常工作。</p>\n<h2 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h2><p>在上面提到的 Github Issue 的讨论中，有大佬已经给出了解决方法，就是通过 Service without selectors 的方式。<br><a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors\">https://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors</a></p>\n<p>常见的 K8s Service 都是通过标签选择器，选择一系列 Pod 作为后端，K8s endpoint controller 会自动根据 Service 的声明去为 Service 的每个端口创建一个 endpoint。Endpoint 是 K8s 中实际进行服务路由的资源。</p>\n<p>而创建 Service without selectors，就需要我们手动去创建一个与 Service 同名的 endpoint。这样就不需要指定 Service 为 ExternalName 的类型，CoreDNS 中就会将其视作一条 A 记录，而不是一条 CNAME 记录。Nginx 拉取 DNS 缓存时也不会把 IP 当做域名了。</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: demo2\n  namespace: default\nspec:\n  clusterIP: None\n  ports:\n  - name: grpc\n    port: 32443\n    protocol: TCP\n---\nkind: Endpoints\napiVersion: v1\nmetadata:\n  name: demo2\n  namespace: default\nsubsets:\n  - addresses:\n      - ip: xxx.xxx.xxx.xxx\n    ports:\n      - port: 32443\n        name: grpc\n        protocol: TCP\n</code></pre>\n<h2 id=\"对比\"><a href=\"#对比\" class=\"headerlink\" title=\"对比\"></a>对比</h2><p>在两个对等的集群发生通信时，demo1 修复，demo2不修复，对比两侧的 CPU 使用情况</p>\n<p>demo1：<br><img src=\"/asset/ingress-nginx-bug-fix/demo1.png\"></p>\n<p>demo2：<br><img src=\"/asset/ingress-nginx-bug-fix/demo2.png\"></p>\n<p>demo2 大约比 demo1 消耗 CPU 多 0.020 个核。虽然这个报错会稍微增加一点 CPU 的使用量，但并不多。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>目前，当指定访问集群外部地址为 IP 时，ingress-nginx controller 的日志中存在大量的 DNS 报错的垃圾日志。虽然不影响正常运行（猜测可能会导致性能波动，对比见最后），但是查看 Nginx 日志 debug 时效率严重降低。</p>\n<p><img src=\"/asset/ingress-nginx-bug-fix/error.png\"></p>\n<h2 id=\"原因\"><a href=\"#原因\" class=\"headerlink\" title=\"原因\"></a>原因</h2><p>详细的讨论见：<br><a href=\"https://github.com/coredns/coredns/issues/2324\">https://github.com/coredns/coredns/issues/2324</a></p>\n<p>简略总结下，导致访问外部 IP，Nginx 报 DNS 解析错误的原因在于 Kubernetes 自身的bug，缺少了一个验证。</p>\n<p>出现问题的情况是通过 ExternalName 类型的 Service 访问外部服务的。定义的 yaml 类似于下面这种：</p>\n<pre><code class=\"yaml\">kind: Service\napiVersion: v1\nmetadata:\n  name: demo2\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: xxx.xxx.xxx.xxx\n</code></pre>\n<p>对于 Kubernetes 的设计来说，ExternalName 就是一个域名。K8s 官方是这样介绍的 </p>\n<blockquote>\n<p>ExternalName: Maps the service to the contents of the externalName field (e.g. foo.bar.example.com), by returning aCNAMErecord with its value. No proxying of any kind is set up.</p>\n</blockquote>\n<blockquote>\n<p>Note:ExternalName accepts an IPv4 address string, but as a DNS names comprised of digits, not as an IP address. ExternalNames that resemble IPv4 addresses are not resolved by CoreDNS or ingress-nginx because ExternalName is intended to specify a canonical DNS name.</p>\n</blockquote>\n<p>从实现上来看，ExternalName 类型的 Service 其实就是在 CoreDNS 里的一条 CNAME 记录。 CNAME 是一条域名指向另一个域名的记录，在 K8s 中，这条 record 记载的就是 Service 名字指向 ExtenalName 的一个映射。</p>\n<p>但是，当 ExternalName 类型的 Service 中设定的是 IP 时，K8s 并没有对其进行判断，仍然允许其正常创建。</p>\n<p>同时，Nginx 本身存在着一个轮询机制，会不断的向 DNS 服务拉取记录进行缓存。<br><a href=\"https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua\">https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util/dns.lua</a></p>\n<p>在每次拉取缓存时会发生以下的过程：</p>\n<ol>\n<li>Nginx 从 CoreDNS 拉取到了一个 CNAME 记录，例如：demo2 -&gt; xxx.xxx.xxx.xxx</li>\n<li>接着，Nginx 尝试解析 xxx.xxx.xxx.xxx 这个域名，CoreDNS 自然是对这个长成 IP 样子的域名解析不出来的，于是解析失败，导致报错</li>\n</ol>\n<hr>\n<p>至于为什么 DNS 解析失败之后，Nginx 仍然能够成功转发请求，原因是 ingress-nginx controller 在实现上并没有对这个进行区分。</p>\n<p>首先，先简单介绍下 controller 的原理。Ingress-nginx controller 一直监听着 k8s 系统中的 ingress 资源。当有新的 ingress 创建时，controller 会开始更新 Nginx 的配置文件，向其中添加转发规则，并重启 Nginx。</p>\n<p>下面是 controller 解析指向 ExternalName 的 ingress，然后创建 upstream 的逻辑<br><a href=\"https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52\">https://github.com/kubernetes/ingress-nginx/blob/5f1a37a624ca38e8cccc87cb7a36d7dbcbe70b01/internal/ingress/controller/endpoints.go#L52</a></p>\n<p><img src=\"/asset/ingress-nginx-bug-fix/nginx-code.png\"></p>\n<p>可以看到，controller 是没有强制解析 ExternalName 成域名的，所以写进 nginx.conf 的 upstream 也是 ip 形式，这样 nginx 会自然地将 ExternalName 解析成 IP，从而可以正常工作。</p>\n<h2 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h2><p>在上面提到的 Github Issue 的讨论中，有大佬已经给出了解决方法，就是通过 Service without selectors 的方式。<br><a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors\">https://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors</a></p>\n<p>常见的 K8s Service 都是通过标签选择器，选择一系列 Pod 作为后端，K8s endpoint controller 会自动根据 Service 的声明去为 Service 的每个端口创建一个 endpoint。Endpoint 是 K8s 中实际进行服务路由的资源。</p>\n<p>而创建 Service without selectors，就需要我们手动去创建一个与 Service 同名的 endpoint。这样就不需要指定 Service 为 ExternalName 的类型，CoreDNS 中就会将其视作一条 A 记录，而不是一条 CNAME 记录。Nginx 拉取 DNS 缓存时也不会把 IP 当做域名了。</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: demo2\n  namespace: default\nspec:\n  clusterIP: None\n  ports:\n  - name: grpc\n    port: 32443\n    protocol: TCP\n---\nkind: Endpoints\napiVersion: v1\nmetadata:\n  name: demo2\n  namespace: default\nsubsets:\n  - addresses:\n      - ip: xxx.xxx.xxx.xxx\n    ports:\n      - port: 32443\n        name: grpc\n        protocol: TCP\n</code></pre>\n<h2 id=\"对比\"><a href=\"#对比\" class=\"headerlink\" title=\"对比\"></a>对比</h2><p>在两个对等的集群发生通信时，demo1 修复，demo2不修复，对比两侧的 CPU 使用情况</p>\n<p>demo1：<br><img src=\"/asset/ingress-nginx-bug-fix/demo1.png\"></p>\n<p>demo2：<br><img src=\"/asset/ingress-nginx-bug-fix/demo2.png\"></p>\n<p>demo2 大约比 demo1 消耗 CPU 多 0.020 个核。虽然这个报错会稍微增加一点 CPU 的使用量，但并不多。</p>\n"},{"title":"[Introduction] Kubernetes CronJob","date":"2021-03-08T00:06:14.000Z","updated":"2021-03-08T00:06:14.000Z","_content":"本文主要介绍下 K8s 的 CronJob，还有其中的一些小坑。\n\n## 概念\nCronjob 是 K8s 定时通过 cronjob controller 定时去创建 Job 实现的。\n创建 Cronjob 的一个例子：\n```yaml\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"*/1 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox\n            imagePullPolicy: IfNotPresent\n            args:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n```\n\n## Cronjob controller 工作原理\nstartingDeadlineSeconds 是一个很重要的参数，其配置了一个周期创建job时，多长时间算作失败。\n1. Controller 每10秒轮询一次 cronjob\n2. 对于每个 cronjob，计算从上次被调度 lastScheduleTime 到现在错过了多少次调度。如果大于100次，则将其状态置为 FailedNeedsStart\n3. 对于其他 cronjob 计算当前是否还在其 lastScheduleTime + startingDeadlineSeconds 内，如果在，则进行调度。如果不在，则发送一条 event\n\"Missed starting window for {cronjob name}. Missed scheduled time to start a job {scheduledTime}\"\n\n## Tips\n1. 时间\n因为 Cronjob 实际上是通过 controller 去管理的，所以其时间是 kube-controller-manager 的时间。\n2. 命名\nCronjob 的命名要遵循 DNS subdomain name，并且不能超过 52 个字符。因为 Cronjob Controller 会在其创建的 Job 后再拼接 11 个字符 （ K8s 对 Job 命名的限制是 63 个字符）\n3. 幂等性\n根据配置的重启和健康规则的不同，K8s 启动 Cronjob 时只能保证 about 一次，有时可能会启动多次，有时可能会没有启动，所以需要 cronjob 保证幂等性。\n以下是 cronjob 可能会发生的两种异常情况：\n  1. 触发多次\nconcurrencyPolicy 配置为 Allow，并且 startingDeadlineSeconds 不设置或者设置为很大。这样，controller 在多次轮询中可能都会查看到 cronjob 符合再次运行的条件，从而创建多个 job。\n  2. 触发0次\nconcurrencyPolicy 配置为 Forbid，这样 controller 就会等到上一次cronjob结束之后才会进行下一次调度。\n4. 自定义 Crontroller\n从 kubernetes 1.20 开始支持\n5. 删除运行成功的 Job\n配置 https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/\n  successfulJobsHistoryLimit: 0\n  failedJobsHistoryLimit: 0\n\n","source":"_posts/k8s-cronjob.md","raw":"---\ntitle: \"[Introduction] Kubernetes CronJob\"\ndate: 2021-03-08 00:06:14\nupdated: 2021-03-08 00:06:14\n---\n本文主要介绍下 K8s 的 CronJob，还有其中的一些小坑。\n\n## 概念\nCronjob 是 K8s 定时通过 cronjob controller 定时去创建 Job 实现的。\n创建 Cronjob 的一个例子：\n```yaml\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"*/1 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox\n            imagePullPolicy: IfNotPresent\n            args:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n```\n\n## Cronjob controller 工作原理\nstartingDeadlineSeconds 是一个很重要的参数，其配置了一个周期创建job时，多长时间算作失败。\n1. Controller 每10秒轮询一次 cronjob\n2. 对于每个 cronjob，计算从上次被调度 lastScheduleTime 到现在错过了多少次调度。如果大于100次，则将其状态置为 FailedNeedsStart\n3. 对于其他 cronjob 计算当前是否还在其 lastScheduleTime + startingDeadlineSeconds 内，如果在，则进行调度。如果不在，则发送一条 event\n\"Missed starting window for {cronjob name}. Missed scheduled time to start a job {scheduledTime}\"\n\n## Tips\n1. 时间\n因为 Cronjob 实际上是通过 controller 去管理的，所以其时间是 kube-controller-manager 的时间。\n2. 命名\nCronjob 的命名要遵循 DNS subdomain name，并且不能超过 52 个字符。因为 Cronjob Controller 会在其创建的 Job 后再拼接 11 个字符 （ K8s 对 Job 命名的限制是 63 个字符）\n3. 幂等性\n根据配置的重启和健康规则的不同，K8s 启动 Cronjob 时只能保证 about 一次，有时可能会启动多次，有时可能会没有启动，所以需要 cronjob 保证幂等性。\n以下是 cronjob 可能会发生的两种异常情况：\n  1. 触发多次\nconcurrencyPolicy 配置为 Allow，并且 startingDeadlineSeconds 不设置或者设置为很大。这样，controller 在多次轮询中可能都会查看到 cronjob 符合再次运行的条件，从而创建多个 job。\n  2. 触发0次\nconcurrencyPolicy 配置为 Forbid，这样 controller 就会等到上一次cronjob结束之后才会进行下一次调度。\n4. 自定义 Crontroller\n从 kubernetes 1.20 开始支持\n5. 删除运行成功的 Job\n配置 https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/\n  successfulJobsHistoryLimit: 0\n  failedJobsHistoryLimit: 0\n\n","slug":"k8s-cronjob","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl5i8k7y400061rpf8gtl8e4u","content":"<p>本文主要介绍下 K8s 的 CronJob，还有其中的一些小坑。</p>\n<h2 id=\"概念\"><a href=\"#概念\" class=\"headerlink\" title=\"概念\"></a>概念</h2><p>Cronjob 是 K8s 定时通过 cronjob controller 定时去创建 Job 实现的。<br>创建 Cronjob 的一个例子：</p>\n<pre><code class=\"yaml\">apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: &quot;*/1 * * * *&quot;\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox\n            imagePullPolicy: IfNotPresent\n            args:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n</code></pre>\n<h2 id=\"Cronjob-controller-工作原理\"><a href=\"#Cronjob-controller-工作原理\" class=\"headerlink\" title=\"Cronjob controller 工作原理\"></a>Cronjob controller 工作原理</h2><p>startingDeadlineSeconds 是一个很重要的参数，其配置了一个周期创建job时，多长时间算作失败。</p>\n<ol>\n<li>Controller 每10秒轮询一次 cronjob</li>\n<li>对于每个 cronjob，计算从上次被调度 lastScheduleTime 到现在错过了多少次调度。如果大于100次，则将其状态置为 FailedNeedsStart</li>\n<li>对于其他 cronjob 计算当前是否还在其 lastScheduleTime + startingDeadlineSeconds 内，如果在，则进行调度。如果不在，则发送一条 event<br>“Missed starting window for {cronjob name}. Missed scheduled time to start a job {scheduledTime}”</li>\n</ol>\n<h2 id=\"Tips\"><a href=\"#Tips\" class=\"headerlink\" title=\"Tips\"></a>Tips</h2><ol>\n<li>时间<br>因为 Cronjob 实际上是通过 controller 去管理的，所以其时间是 kube-controller-manager 的时间。</li>\n<li>命名<br>Cronjob 的命名要遵循 DNS subdomain name，并且不能超过 52 个字符。因为 Cronjob Controller 会在其创建的 Job 后再拼接 11 个字符 （ K8s 对 Job 命名的限制是 63 个字符）</li>\n<li>幂等性<br>根据配置的重启和健康规则的不同，K8s 启动 Cronjob 时只能保证 about 一次，有时可能会启动多次，有时可能会没有启动，所以需要 cronjob 保证幂等性。<br>以下是 cronjob 可能会发生的两种异常情况：</li>\n<li>触发多次<br>concurrencyPolicy 配置为 Allow，并且 startingDeadlineSeconds 不设置或者设置为很大。这样，controller 在多次轮询中可能都会查看到 cronjob 符合再次运行的条件，从而创建多个 job。</li>\n<li>触发0次<br>concurrencyPolicy 配置为 Forbid，这样 controller 就会等到上一次cronjob结束之后才会进行下一次调度。</li>\n<li>自定义 Crontroller<br>从 kubernetes 1.20 开始支持</li>\n<li>删除运行成功的 Job<br>配置 <a href=\"https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/\">https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/</a><br>successfulJobsHistoryLimit: 0<br>failedJobsHistoryLimit: 0</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>本文主要介绍下 K8s 的 CronJob，还有其中的一些小坑。</p>\n<h2 id=\"概念\"><a href=\"#概念\" class=\"headerlink\" title=\"概念\"></a>概念</h2><p>Cronjob 是 K8s 定时通过 cronjob controller 定时去创建 Job 实现的。<br>创建 Cronjob 的一个例子：</p>\n<pre><code class=\"yaml\">apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: &quot;*/1 * * * *&quot;\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox\n            imagePullPolicy: IfNotPresent\n            args:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n</code></pre>\n<h2 id=\"Cronjob-controller-工作原理\"><a href=\"#Cronjob-controller-工作原理\" class=\"headerlink\" title=\"Cronjob controller 工作原理\"></a>Cronjob controller 工作原理</h2><p>startingDeadlineSeconds 是一个很重要的参数，其配置了一个周期创建job时，多长时间算作失败。</p>\n<ol>\n<li>Controller 每10秒轮询一次 cronjob</li>\n<li>对于每个 cronjob，计算从上次被调度 lastScheduleTime 到现在错过了多少次调度。如果大于100次，则将其状态置为 FailedNeedsStart</li>\n<li>对于其他 cronjob 计算当前是否还在其 lastScheduleTime + startingDeadlineSeconds 内，如果在，则进行调度。如果不在，则发送一条 event<br>“Missed starting window for {cronjob name}. Missed scheduled time to start a job {scheduledTime}”</li>\n</ol>\n<h2 id=\"Tips\"><a href=\"#Tips\" class=\"headerlink\" title=\"Tips\"></a>Tips</h2><ol>\n<li>时间<br>因为 Cronjob 实际上是通过 controller 去管理的，所以其时间是 kube-controller-manager 的时间。</li>\n<li>命名<br>Cronjob 的命名要遵循 DNS subdomain name，并且不能超过 52 个字符。因为 Cronjob Controller 会在其创建的 Job 后再拼接 11 个字符 （ K8s 对 Job 命名的限制是 63 个字符）</li>\n<li>幂等性<br>根据配置的重启和健康规则的不同，K8s 启动 Cronjob 时只能保证 about 一次，有时可能会启动多次，有时可能会没有启动，所以需要 cronjob 保证幂等性。<br>以下是 cronjob 可能会发生的两种异常情况：</li>\n<li>触发多次<br>concurrencyPolicy 配置为 Allow，并且 startingDeadlineSeconds 不设置或者设置为很大。这样，controller 在多次轮询中可能都会查看到 cronjob 符合再次运行的条件，从而创建多个 job。</li>\n<li>触发0次<br>concurrencyPolicy 配置为 Forbid，这样 controller 就会等到上一次cronjob结束之后才会进行下一次调度。</li>\n<li>自定义 Crontroller<br>从 kubernetes 1.20 开始支持</li>\n<li>删除运行成功的 Job<br>配置 <a href=\"https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/\">https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/</a><br>successfulJobsHistoryLimit: 0<br>failedJobsHistoryLimit: 0</li>\n</ol>\n"},{"title":"[Introduction] Kubernetes Ramp Up","date":"2021-03-15T15:39:36.000Z","updated":"2021-03-15T15:39:36.000Z","_content":"## 目标\n- 介绍 K8s，Docker 概念以及原理\n- 从 0 开始部署一个简单完整的服务\n\n## Docker是什么？\nDocker是由Google推出的Go语言进行开发实现，基于Linux内核的 <font color=red>namespace</font>，对<font color=red>进程</font>进行封装<font color=red>隔离</font>，属于操作系统层面的容器化技术。\n\n![](/asset/k8s-ramp-up/1.png)\n\n### 三大核心概念\n镜像（Image）\n\n容器（Container）\n\n仓库（Repository）\n\n从代码的角度来看，镜像就像一个类；容器是对象实例，运行时在系统中会有许多容器；仓库主要用于存储和维护这些镜像。\n\n### 为什么使用 Docker？\n- 配置环境\n开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性\n- 应用隔离\n机器上可能同时运行多个服务。如果服务之间没有隔离，一个服务出现异常，往往可能会导致其他服务也挂掉。同时，不同服务所依赖的环境也可能发生冲突。\n\n### 原理\n首先，要了解一下进程的命名空间。Linux 系统中的所有进程按照惯例是通过PID标识的，这意味着内核必须管理一个全局的PID列表。而且，所有调用者通过uname系统调用返回的系统相关信息（包括系统名称和有关内核的一些信息）都是相同的。\n\nLinux 的命名空间从内核层面上进行了虚拟化，对所有的全局资源进行一个抽象。本质上，建立了系统的不同视图。每一项全局资源都必须包装到命名空间的数据结构中，只有资源和包含资源的命名空间构成的二元组仍然是全局唯一的。不仅仅是 PID，Linux 通过同样的方法对其他资源也做了虚拟化处理。命名空间共有以下6种：\n\n![](/asset/k8s-ramp-up/2.png)\n\n借助 Linux 的命名空间，Docker 对进程进行隔离，可以从进程树的角度理解。\n\n![](/asset/k8s-ramp-up/3.png)\n\n每次在执行 `docker start` 或 `docker run` 的时候，其实是由 docker 的 daemon 进程 docker containerd，调用 Linux 系统调用 `clone()` 去创建新的进程。而创建进程的过程中就为新创建的进程分配了新的 Linux 命名空间。可以简单阅读一下 docker 的开源代码\n\n<pre><code class=\"go\">// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L17\n// 创建容器的函数，其中又调用了设置\nfunc (daemon *Daemon) ContainerStart(name string, hostConfig *containertypes.HostConfig, checkpoint string, checkpointDir string) error\n\n// https://github.com/moby/moby/blob/470ae8422fc6f1845288eb7572253b08f1e6edf8/daemon/oci_linux.go#L212\n// 设置 Namespace\nfunc setNamespace(s *specs.Spec, ns specs.LinuxNamespace) {\n   for i, n := range s.Linux.Namespaces {\n      if n.Type == ns.Type {\n         s.Linux.Namespaces[i] = ns\n         return\n      }\n   }\n   s.Linux.Namespaces = append(s.Linux.Namespaces, ns)\n}\n\n// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L198\n// 创建新的进程\npid, err := daemon.containerd.Start(context.Background(), \n                                    container.ID, \n                                    checkpointDir,    \n                                    container.StreamConfig.Stdin() != nil | | container.Config.Tty, \n                                    container.InitializeStdio)\n</code></pre>\n\n### 如何安装？\n[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)\n\n## Kubernetes是什么？\nKubernetes 是 Google 于 2014 年基于其内部 Brog 系统开源的一个容器编排管理系统，可使用声明式的配置（以 yaml 文件的形式）自动地执行容器化应用程序的管理，包括部署、伸缩、负载均衡、回滚等。\n\n为什么叫 K8s？因为 K<font color=red>ubernete</font>s，中间是8个字母。\n\nkubernetes 提供的功能：\n- 自动发布与伸缩：可以通过声明式的配置文件定义想要部署的容器\n- 滚动升级与灰度发布：采用逐步替换的策略实现滚动升级\n- 服务发现与负载均衡：Kubernetes 通过 DNS 名称或 IP 地址暴露容器的访问方式，并且可在同一容器组内实现负载分发与均衡\n- 存储编排：Kubernetes 可以自动挂载指定的存储系统，如 local storage/nfs / 云存储等\n- 故障恢复：Kubernetes 自动重启已经停机的容器，替换不满足健康检查的容器\n- 密钥与配置管理：Kubernetes 可以存储与管理敏感信息，如 Docker Registry 的登录凭证，密码，ssh 密钥等\n\n### 为什么使用 K8s？\n大型单体应用被逐渐拆分成小的、可独立运行的组件。随着部署组件的增多和数据中心的增长，配置、管理和运维变得很困难。(微服务）\n\nK8s 的定义就是容器编排和管理引擎，解决了这些问题。\n\n### 如何安装？\n由难到易(๑•̀ㅂ•́)و✧\n- Kubeadm: https://kubernetes.io/docs/reference/setup-tools/kubeadm/\n- MiniKube: Local kubernetes https://minikube.sigs.k8s.io/docs/start/\n- Kind: Kubernetes in Docker https://github.com/kubernetes-sigs/kind\n- Docker-desktop（仅限 Mac）: 一键开启\n![](/asset/k8s-ramp-up/4.png)\n\n\n其他版本的类 K8s 系统：\n- K3s: https://github.com/k3s-io/k3s\n- K0s: https://github.com/k0sproject/k0s\n\n## Kubernetes 架构\n\n![](/asset/k8s-ramp-up/5.png)\n\n### master\n\nMaster 负责管理服务来对整个系统进行管理与控制，包括\n- apiserver：作为整个系统的对外接口，提供一套 Restful API 供客户端调用，任何的资源请求 / 调用操作都是通过 kube-apiserver 提供的接口进行, 如 kubectl、kubernetes dashboard 等管理工具就是通过 apiserver 来实现对集群的管理\n- kube-scheduler：资源调度器，负责将容器组分配到哪些节点上\n- kube-controller-manager：管理控制器，集群中处理常规任务的后台线程，包括节点控制器（负责监听节点停机的事件并作出对应响应）、endpoint-controller（刷新服务与容器组的关联信息）、replication-controller（维护容器组的副本数为指定的数值）、Service Account & Token 控制器（负责为新的命名空间创建默认的 Service Account 以及 API Access Token）\n- etcd：数据存储，存储集群所有的配置信息\n- coredns：实现集群内部通过服务名称进行容器组访问的功能\n\n### worker\n\nWorker 负载执行 Master 分配的任务，包括\n- kubelet：是工作节点上执行操作的代理程序，负责容器的生命周期管理，定期执行容器健康检查，并上报容器的运行状态\n- kube-proxy：是一个具有负载均衡能力的简单的网络访问代理，负责将访问某个服务的请求分配到工作节点的具体某个容器上（kube-proxy 也运行于 master node 上）\n- Docker Daemon：Kubernetes 其实不局限于 Docker（即将取消），它支持任何实现了 Kubernetes 容器引擎接口的容器引擎，如 containerd、rktlet\n\n### 网络通信\n网络通信组件只需要符合 CNI （Container Network Interface）接口规范，主要作用在于给各个容器分配集群内 IP，使得其内网 IP 能够集群内唯一，并且可以互相访问，目前常用的有 Flannel，Calico等网络组件。\n\n简单介绍下比较常用的 Flannel 的原理。Flannel 运行在第3层网络层，基于 IPv4，创建一个大型内部网络，跨越集群中每个节点。每个节点组成一个子网，每个容器在内网中有唯一的IP。\n\n首先，Flannel 会为每台节点分配一个子网段。Flanneld 在 Docker 容器启动时修改其启动参数，将其 IP 限制在当前的子网段内，具体 IP 的分配仍是由 docker 进行。Flannel通过Etcd服务维护了一张节点间的路由表，详细记录了各节点子网网段，保证不同节点的子网网段不会重复。\n\n数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，flanneld服务监听在网卡的另外一端。\n\n源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。\n\n![](/asset/k8s-ramp-up/6.png)\n\n## 快速上手 K8s 概念\n\n一些推荐的 K8s 概念介绍：\n- 微软的 50天 K8s 教程中（https://azure.microsoft.com/en-us/resources/kubernetes-learning-path/）通过动物园的形式介绍了一些 K8s 概念 http://aka.ms/k8s/LearnwithPhippy\n- 综述PPT：https://www2.slideshare.net/BobKillen/kubernetes-a-comprehensive-overview-updated?from_action=save\n\nK8s 中的概念极多，比较零碎，这里通过一个简单的小例子，尽可能覆盖多的 K8s 概念。\n\n## 概览\n\n例子使用一个开源的 fortune-teller 镜像（`quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1`） ，每次请求容器内的服务，服务会返回一句名言。希望在 MacOS 的环境下，展示一个应用在 K8s 中运行的全流程。\n\n准备环境\n为了不影响大家本地的环境，这里使用 Kind 创建出一个独立的 K8s 集群，方便统一版本并且可以在完成快速清理掉。(Docker 双重隔离）\n\n1. 安装 Kind 以及 gRpc 测试工具\n```bash\nbrew install kind\nbrew install grpcurl\n```\n\n2. 拉取镜像\n```bash\ndocker pull kindest/node:v1.16.15\ndocker pull quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n```\n\n3. 创建 K8s 集群，因为 Kind 是在 Docker 容器里面创建的 K8s，所以宿主机访问，需要把端口暴露出来。Kind 会默认把 K8s apiserver 的端口暴露出来，用来给 kubectl 命令使用。但为了之后的测试，我们提前把几个端口在创建的时候就暴露出来。\n\nKind 同样支持通过yaml 的形式创建集群\n```yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 32080\n    hostPort: 32080\n  - containerPort: 32443\n    hostPort: 32443\n```\n\n```bash\nkind create cluster --name=fortune-teller --image=kindest/node:v1.16.15 --config kind-config.yaml\n```\n\n### 运行 Docker 版本\n1. 启动容器\n```bash\ndocker run -p 50051:50051 quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n```\n\n`-p` 将容器内的 50051 端口映射到宿主机的 50051 端口\n\n2. 测试应用是否正常运行，第一次运行时可能需要给 grpcurl 开启权限\n```bash\ngrpcurl -plaintext 127.0.0.1:50051 build.stack.fortune.FortuneTeller/Predict\n```\n\n应用在收到请求以后，会返回一句名言\n\n![](/asset/k8s-ramp-up/7.png)\n\n### 将应用从 Docker 迁移到 K8s 中\n\n与 Docker 中容器概念相对应的，K8s 中也有着容器的概念。对于虚拟化的容器来说，最佳实践是一个容器一个应用，但当一个服务需要多个应用组合完成时，简单的将多个应用部署到一个容器内，就破坏了应用之间的隔离性，所以 K8s 对于容器进行了一层封装，形成了 Pod 的概念。\n\n#### Pod\n\nPod 是 Kubernetes 创建或部署的最小基本单元。一个 Pod 封装一个或多个应用容器、存储资源、一个独立的网络 IP 以及管理控制容器运行方式的策略选项。Pod 中的每个容器共享网络命名空间（包括 IP 与端口），Pod 内的容器可以使用 localhost 相互通信。Pod 可以指定一组共享存储卷 Volumes，Pod 中所有容器都可以访问共享的 Volumes。 \n\n通过 Pod，用户就可以非常方便地控制容器之间的隔离性。\n\n有了 Pod 作为基础以后，K8s 就要实现它最重要的功能，对容器的编排管理。当服务需要扩容时，K8s 需要能够快速复制 Pod，当 Pod 挂掉了，K8s 需要能够自动重启。所以 K8s 由此衍生出了 ReplicaSet 的概念。\n\n#### ReplicaSet\n\nReplicaSet 确保在任何时候都有按配置的 Pod 副本数在运行，通过标签选择器的方式对 Pod 进行筛选和管理。在旧的版本中还有一个 ReplicaController 的概念，RC 与 RS 两者功能完全相同，区别仅仅在于 RS 对于 Pod 的标签选择器更加强大。\n\n开头提到了 K8s 使用声明式的配置自动去管理容器，而 ReplicaSet 的内容却太过具体，涉及到了 Pod 的具体维护细节。所以 K8s 在 ReplicaSet 之上又衍生出声明式配置容器的概念，Deployment。\n\n#### Deployment\n\nDeployment 为 Pod 与 ReplicaSet 提供了声明式的定义，描述你想要的目标状态是什么，Deployment controller 就会帮你将 Pod 与 ReplicaSet 的实际状态改变到你想要的目标状态。\n\n以 fortune-teller 为例子，可以编写一份下面这样的 Deployment 配置文件\n```yaml\napiVersion: apps/v1 # k8s api版本\nkind: Deployment # 资源类型\nmetadata:\n  name: fortune-teller-app # deployment 名字\n  namespace: default\nspec:\n  replicas: 1 # Pod 副本数量\n  selector:\n    matchLabels:\n      k8s-app: fortune-teller-app # 管理标签中包含 k8s-app: fortune-teller-app 的 Pod\n  template: # Pod 模板\n    metadata:\n      labels:\n        k8s-app: fortune-teller-app # Pod 标签\n    spec: # Pod 配置\n      containers:\n      - image: quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n        imagePullPolicy: IfNotPresent\n        name: fortune-teller-app\n        ports:\n        - containerPort: 50051\n          name: grpc\n          protocol: TCP\n```\n\n将上面的内容保存到一份 yaml 文件中，执行以下命令，让 K8s 执行 yaml\n```bash\nkubectl apply -f deployment.yaml\n```\n\n通过以下命令，我们就可以看到刚刚创建的 deployment\n```bash\nkubectl get deployement\n```\n\n![](/asset/k8s-ramp-up/8.png)\n\n此时，K8s 已经自动根据 deployment 中配置的 Pod 模板和配置，创建了 Pod。通过以下命令，我们就可以看到 K8s 自动创建的 Pod\n```bash\nkubectl get pods\n```\n![](/asset/k8s-ramp-up/9.png)\n\n因为 K8s 采用声明式的配置去管理 Pod，所以我们可以动态地去修改 deployment 的配置，K8s 会自动根据新的配置去管理 Pod。\n```bash\nkubectl edit deployment fortune-teller-app\n```\n\n我们把配置文件中的副本数量修改为 2\n\n![](/asset/k8s-ramp-up/10.png)\n\n保存退出后，我们再次执行 kubectl get pods ，我们就可以看到 K8s 根据新的配置，创建了一个新的 Pod\n\n![](/asset/k8s-ramp-up/11.png)\n\n现在我们就有了两个 fortune-teller 的服务。在真实环境中，Pod 的调度由 K8s 进行管理，某个时刻服务可能在 Node1 上，而另一时刻服务可能就被调度到了 Node2 上。所以，访问具体 Pod 是一种不稳定的服务访问方法，而且目前大多数的后端服务都是无状态的服务，直接访问 Pod 也导致不能进行负载均衡。所以，K8s 在此基础上衍生出 Service 的概念。\n\n#### Service\n\nService 可以看做一组提供相同服务的 Pod 的对外访问接口。Kubernetes 提供三种类型的 Service：\n- NodePort： 集群外部可以通过 Node IP 与 Node Port 来访问具体某个 Pod，每台机器上都会暴露同样的端口\n- ClusterIP：指通过集群的内部 IP 暴露服务，服务只能够在集群内部可以访问，这也是默认的 ServiceType\n- ExternalName：不指向 Pod，指向外部服务\nService 和 Deployment 是一对比较容易混淆的概念，两者都是对一组 Pod 进行管理，但它们两者之间的关系可以用下面这张图来概括\n\n![](/asset/k8s-ramp-up/12.png)\n\nService 是面向服务调用者，也就是外部访问 K8s。而 Deployment 是面向 K8s 底层引擎的，面向内部管理者。\n\nService 的配置文件格式与 Deployment 很类似\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: fortune-teller-service\n  namespace: default\nspec:\n  ports:\n  - name: grpc\n    port: 50051\n    protocol: TCP\n    targetPort: 50051\n  selector:\n    k8s-app: fortune-teller-app\n  type: ClusterIP\n```\n\n同样的，我们通过 kubectl apply -f service.yaml 命令，可以创建 Service。通过 kubectl get service 可以查看到刚刚创建的 Service。\n\n![](/asset/k8s-ramp-up/13.png)\n\n接下来，我们登陆到一个 Pod 里去测试一下是否可以访问服务。\nNetshoot 镜像中包含了一些网络测试的工具，我们可以直接进入一个 netshoot 容器内测试。采用 deployment 的方式创建 Pod\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netshoot\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: netshoot\n  template:\n    metadata:\n      labels:\n        k8s-app: netshoot\n    spec:\n      containers:\n      - args:\n        - 1000d\n        command:\n        - /bin/sleep\n        image: nicolaka/netshoot\n        name: netshoot\n```\n\n与 Docker 的命令类似，使用命令 `kubectl cp grpcurl_1.6.0_linux_x86_64.tar.gz <pod name>:/` 复制工具到容器内。\n\n复制成功后，使用命令 kubectl exec -it <pod name> bash 可以进入到容器内。\n\n解压\n```bash\ncd /\ntar -zxf grpcurl_1.6.0_linux_x86_64.tar.gz\n```\n\n然后我们可以测试是否可以从 K8s 集群内访问 Service。对于 K8s 集群内部的服务，K8s 有自己的 DNS 组件，所以可以直接通过服务名访问。\n```bash\n./grpcurl -plaintext fortune-teller-service:50051 build.stack.fortune.FortuneTeller/Predict\n```\n\n![](/asset/k8s-ramp-up/14.png)\n\n验证服务可以从集群内访问之后，我们就需要解决如何从集群外访问服务的问题，毕竟大多数服务是面向 K8s 集群外的用户的。其实目前我们已经了解了一种解决方案，就是使用 Nodeport 类型的 Service。但采用这种方法有几个缺点：\n1. 每个端口只能是一种服务\n2. 端口范围只能是 30000-32767\n3. 如果节点 的 IP 地址发生变化，调用方需要能够察觉。\n所以，K8s 为服务的外部访问路由提供了新的类型 Ingress。\n\n#### Ingress\nIngress 其实是一种类似于路由表一样的配置，实际的路由工作需要 Ingress Controller 执行。K8s 本身并没有提供 Ingress Controller，目前常用的是通过 Nginx 实现的版本 https://github.com/kubernetes/ingress-nginx 。可以使用上面压缩包中的 ingress-controller.yaml 安装\n\n![](/asset/k8s-ramp-up/15.png)\n\nIngree nginx controller 通过宿主机暴露给外部访问的端口是随机的，所以我们修改 yaml，改成我们在一开始创建集群时映射的端口。\n\n通过 `kubectl get svc -n ingress-nginx` 我们就可以看到，暴露的是两个随机分配的端口\n\n![](/asset/k8s-ramp-up/16.png)\n\n我们手动将其改成 32080 和 32443。\n\n![](/asset/k8s-ramp-up/17.png)\n\nIngress nginx controller 同样是通过标签选择器的方式管理 Ingress。\n\n下面是一个简单的 Ingress，将发往 Host fortune.bytedance.com 的请求路由到 Service fortune-teller-service。\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: GRPC\n  name: fortune-ingress\n  namespace: default\nspec:\n  rules:\n  - host: fortune.bytedance.com\n    http:\n      paths:\n      - backend:\n          serviceName: fortune-teller-service\n          servicePort: grpc\n```\n\n安装 Ingress\n```bash\nkubectl apply -f ingress.yaml\nkubectl get ingress\n```\n\n![](/asset/k8s-ramp-up/18.png)\n\nIngress nginx controller 对于 gRpc 默认只支持 SSL 的形式，而Fedlearner 中的 controller 做了一些定制化操作，使得通过 80 端口，只使用 HTTP2 也可以转发 gRpc。详情可以参考 https://github.com/bytedance/ingress-nginx/pull/2/files\n\n使用下面这个命令，我们可以测试一下从外部访问服务\n```bash\ngrpcurl -insecure -servername 'fortune.bytedance.com' 0.0.0.0:32443 build.stack.fortune.FortuneTeller/Predict\n```\n\n![](/asset/k8s-ramp-up/19.png)\n\n刚才也提到了，gRpc 通常是使用 SSL 进行加密的，SSL 的关键在于公钥，私钥以及证书的验证。通过文件系统的方式确实可以处理证书的问题，但 K8s 抽象出 Secret 这种资源，大大提高了对这类文件的管理和复用能力。\n\n#### Secret\n\nSecret 解决了密码、token、密钥等敏感数据的存储问题，主要分为三种类型：\n- Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的 / run/secrets/kubernetes.io/serviceaccount 目录中\n- Opaque ：Base64 编码格式的 Secret，用来存储密码、密钥等\n- kubernetes.io/dockerconfigjson ：用来存储 docker registry 的认证信息\n\n接下来，我们就来创建一个 Opaque 类型的 Secret，使得 ingress nginx controller 支持服务端的 SSL。\n由于篇幅有限，这里简单介绍下证书相关的概念：\n- CA：证书授权中心(certificate authority)，用来签发私钥，并验证公钥，私钥的合法性\n- 私钥，公钥：私钥用于加密，公钥用于解密\n\nSecret 支持不编写 yaml，直接从文件中创建 Secret\n```bash\nkubectl create secret generic fortune-teller-ssl-verify \\\n  --from-file=ca.crt=CA.pem \\\n  --from-file=tls.crt=server-public.pem \\\n  --from-file=tls.key=server-private.key\n```\n\n修改 Ingress，使其使用新创建的 Secret 提供服务侧的 SSL。\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: GRPC\n  name: fortune-ingress\n  namespace: default\nspec:\n  rules:\n  - host: fortune.test.com\n    http:\n      paths:\n      - backend:\n          serviceName: fortune-teller-service\n          servicePort: grpc\n  tls:\n  - hosts:\n    - fortune.test.com\n    secretName: fortune-teller-ssl-verify\n```\n\n因为我们使用的是自签名的证书，不被公共的 CA 所信任，所以在发送请求是需要手动指定自己所信任的 CA。\n\n```bash\ngrpcurl -cacert CA.pem \\\n  -servername 'fortune.test.com' \\\n  127.0.0.1:32443 \\\n  build.stack.fortune.FortuneTeller/Predict\n```\n\n![](/asset/k8s-ramp-up/20.png)\n\n## 参考\n- https://draveness.me/docker/\n- https://www.yuque.com/kshare/2020/fe3b6f86-3b9b-48da-9770-897d838cbf41?language=zh-cn#Namespace\n- https://network.51cto.com/art/201907/598970.htm\n- https://blog.csdn.net/gatieme/article/details/51383322\n\n","source":"_posts/k8s-ramp-up.md","raw":"---\ntitle: \"[Introduction] Kubernetes Ramp Up\"\ndate: 2021-03-15 15:39:36\nupdated: 2021-03-15 15:39:36\n---\n## 目标\n- 介绍 K8s，Docker 概念以及原理\n- 从 0 开始部署一个简单完整的服务\n\n## Docker是什么？\nDocker是由Google推出的Go语言进行开发实现，基于Linux内核的 <font color=red>namespace</font>，对<font color=red>进程</font>进行封装<font color=red>隔离</font>，属于操作系统层面的容器化技术。\n\n![](/asset/k8s-ramp-up/1.png)\n\n### 三大核心概念\n镜像（Image）\n\n容器（Container）\n\n仓库（Repository）\n\n从代码的角度来看，镜像就像一个类；容器是对象实例，运行时在系统中会有许多容器；仓库主要用于存储和维护这些镜像。\n\n### 为什么使用 Docker？\n- 配置环境\n开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性\n- 应用隔离\n机器上可能同时运行多个服务。如果服务之间没有隔离，一个服务出现异常，往往可能会导致其他服务也挂掉。同时，不同服务所依赖的环境也可能发生冲突。\n\n### 原理\n首先，要了解一下进程的命名空间。Linux 系统中的所有进程按照惯例是通过PID标识的，这意味着内核必须管理一个全局的PID列表。而且，所有调用者通过uname系统调用返回的系统相关信息（包括系统名称和有关内核的一些信息）都是相同的。\n\nLinux 的命名空间从内核层面上进行了虚拟化，对所有的全局资源进行一个抽象。本质上，建立了系统的不同视图。每一项全局资源都必须包装到命名空间的数据结构中，只有资源和包含资源的命名空间构成的二元组仍然是全局唯一的。不仅仅是 PID，Linux 通过同样的方法对其他资源也做了虚拟化处理。命名空间共有以下6种：\n\n![](/asset/k8s-ramp-up/2.png)\n\n借助 Linux 的命名空间，Docker 对进程进行隔离，可以从进程树的角度理解。\n\n![](/asset/k8s-ramp-up/3.png)\n\n每次在执行 `docker start` 或 `docker run` 的时候，其实是由 docker 的 daemon 进程 docker containerd，调用 Linux 系统调用 `clone()` 去创建新的进程。而创建进程的过程中就为新创建的进程分配了新的 Linux 命名空间。可以简单阅读一下 docker 的开源代码\n\n<pre><code class=\"go\">// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L17\n// 创建容器的函数，其中又调用了设置\nfunc (daemon *Daemon) ContainerStart(name string, hostConfig *containertypes.HostConfig, checkpoint string, checkpointDir string) error\n\n// https://github.com/moby/moby/blob/470ae8422fc6f1845288eb7572253b08f1e6edf8/daemon/oci_linux.go#L212\n// 设置 Namespace\nfunc setNamespace(s *specs.Spec, ns specs.LinuxNamespace) {\n   for i, n := range s.Linux.Namespaces {\n      if n.Type == ns.Type {\n         s.Linux.Namespaces[i] = ns\n         return\n      }\n   }\n   s.Linux.Namespaces = append(s.Linux.Namespaces, ns)\n}\n\n// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L198\n// 创建新的进程\npid, err := daemon.containerd.Start(context.Background(), \n                                    container.ID, \n                                    checkpointDir,    \n                                    container.StreamConfig.Stdin() != nil | | container.Config.Tty, \n                                    container.InitializeStdio)\n</code></pre>\n\n### 如何安装？\n[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)\n\n## Kubernetes是什么？\nKubernetes 是 Google 于 2014 年基于其内部 Brog 系统开源的一个容器编排管理系统，可使用声明式的配置（以 yaml 文件的形式）自动地执行容器化应用程序的管理，包括部署、伸缩、负载均衡、回滚等。\n\n为什么叫 K8s？因为 K<font color=red>ubernete</font>s，中间是8个字母。\n\nkubernetes 提供的功能：\n- 自动发布与伸缩：可以通过声明式的配置文件定义想要部署的容器\n- 滚动升级与灰度发布：采用逐步替换的策略实现滚动升级\n- 服务发现与负载均衡：Kubernetes 通过 DNS 名称或 IP 地址暴露容器的访问方式，并且可在同一容器组内实现负载分发与均衡\n- 存储编排：Kubernetes 可以自动挂载指定的存储系统，如 local storage/nfs / 云存储等\n- 故障恢复：Kubernetes 自动重启已经停机的容器，替换不满足健康检查的容器\n- 密钥与配置管理：Kubernetes 可以存储与管理敏感信息，如 Docker Registry 的登录凭证，密码，ssh 密钥等\n\n### 为什么使用 K8s？\n大型单体应用被逐渐拆分成小的、可独立运行的组件。随着部署组件的增多和数据中心的增长，配置、管理和运维变得很困难。(微服务）\n\nK8s 的定义就是容器编排和管理引擎，解决了这些问题。\n\n### 如何安装？\n由难到易(๑•̀ㅂ•́)و✧\n- Kubeadm: https://kubernetes.io/docs/reference/setup-tools/kubeadm/\n- MiniKube: Local kubernetes https://minikube.sigs.k8s.io/docs/start/\n- Kind: Kubernetes in Docker https://github.com/kubernetes-sigs/kind\n- Docker-desktop（仅限 Mac）: 一键开启\n![](/asset/k8s-ramp-up/4.png)\n\n\n其他版本的类 K8s 系统：\n- K3s: https://github.com/k3s-io/k3s\n- K0s: https://github.com/k0sproject/k0s\n\n## Kubernetes 架构\n\n![](/asset/k8s-ramp-up/5.png)\n\n### master\n\nMaster 负责管理服务来对整个系统进行管理与控制，包括\n- apiserver：作为整个系统的对外接口，提供一套 Restful API 供客户端调用，任何的资源请求 / 调用操作都是通过 kube-apiserver 提供的接口进行, 如 kubectl、kubernetes dashboard 等管理工具就是通过 apiserver 来实现对集群的管理\n- kube-scheduler：资源调度器，负责将容器组分配到哪些节点上\n- kube-controller-manager：管理控制器，集群中处理常规任务的后台线程，包括节点控制器（负责监听节点停机的事件并作出对应响应）、endpoint-controller（刷新服务与容器组的关联信息）、replication-controller（维护容器组的副本数为指定的数值）、Service Account & Token 控制器（负责为新的命名空间创建默认的 Service Account 以及 API Access Token）\n- etcd：数据存储，存储集群所有的配置信息\n- coredns：实现集群内部通过服务名称进行容器组访问的功能\n\n### worker\n\nWorker 负载执行 Master 分配的任务，包括\n- kubelet：是工作节点上执行操作的代理程序，负责容器的生命周期管理，定期执行容器健康检查，并上报容器的运行状态\n- kube-proxy：是一个具有负载均衡能力的简单的网络访问代理，负责将访问某个服务的请求分配到工作节点的具体某个容器上（kube-proxy 也运行于 master node 上）\n- Docker Daemon：Kubernetes 其实不局限于 Docker（即将取消），它支持任何实现了 Kubernetes 容器引擎接口的容器引擎，如 containerd、rktlet\n\n### 网络通信\n网络通信组件只需要符合 CNI （Container Network Interface）接口规范，主要作用在于给各个容器分配集群内 IP，使得其内网 IP 能够集群内唯一，并且可以互相访问，目前常用的有 Flannel，Calico等网络组件。\n\n简单介绍下比较常用的 Flannel 的原理。Flannel 运行在第3层网络层，基于 IPv4，创建一个大型内部网络，跨越集群中每个节点。每个节点组成一个子网，每个容器在内网中有唯一的IP。\n\n首先，Flannel 会为每台节点分配一个子网段。Flanneld 在 Docker 容器启动时修改其启动参数，将其 IP 限制在当前的子网段内，具体 IP 的分配仍是由 docker 进行。Flannel通过Etcd服务维护了一张节点间的路由表，详细记录了各节点子网网段，保证不同节点的子网网段不会重复。\n\n数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，flanneld服务监听在网卡的另外一端。\n\n源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。\n\n![](/asset/k8s-ramp-up/6.png)\n\n## 快速上手 K8s 概念\n\n一些推荐的 K8s 概念介绍：\n- 微软的 50天 K8s 教程中（https://azure.microsoft.com/en-us/resources/kubernetes-learning-path/）通过动物园的形式介绍了一些 K8s 概念 http://aka.ms/k8s/LearnwithPhippy\n- 综述PPT：https://www2.slideshare.net/BobKillen/kubernetes-a-comprehensive-overview-updated?from_action=save\n\nK8s 中的概念极多，比较零碎，这里通过一个简单的小例子，尽可能覆盖多的 K8s 概念。\n\n## 概览\n\n例子使用一个开源的 fortune-teller 镜像（`quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1`） ，每次请求容器内的服务，服务会返回一句名言。希望在 MacOS 的环境下，展示一个应用在 K8s 中运行的全流程。\n\n准备环境\n为了不影响大家本地的环境，这里使用 Kind 创建出一个独立的 K8s 集群，方便统一版本并且可以在完成快速清理掉。(Docker 双重隔离）\n\n1. 安装 Kind 以及 gRpc 测试工具\n```bash\nbrew install kind\nbrew install grpcurl\n```\n\n2. 拉取镜像\n```bash\ndocker pull kindest/node:v1.16.15\ndocker pull quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n```\n\n3. 创建 K8s 集群，因为 Kind 是在 Docker 容器里面创建的 K8s，所以宿主机访问，需要把端口暴露出来。Kind 会默认把 K8s apiserver 的端口暴露出来，用来给 kubectl 命令使用。但为了之后的测试，我们提前把几个端口在创建的时候就暴露出来。\n\nKind 同样支持通过yaml 的形式创建集群\n```yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 32080\n    hostPort: 32080\n  - containerPort: 32443\n    hostPort: 32443\n```\n\n```bash\nkind create cluster --name=fortune-teller --image=kindest/node:v1.16.15 --config kind-config.yaml\n```\n\n### 运行 Docker 版本\n1. 启动容器\n```bash\ndocker run -p 50051:50051 quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n```\n\n`-p` 将容器内的 50051 端口映射到宿主机的 50051 端口\n\n2. 测试应用是否正常运行，第一次运行时可能需要给 grpcurl 开启权限\n```bash\ngrpcurl -plaintext 127.0.0.1:50051 build.stack.fortune.FortuneTeller/Predict\n```\n\n应用在收到请求以后，会返回一句名言\n\n![](/asset/k8s-ramp-up/7.png)\n\n### 将应用从 Docker 迁移到 K8s 中\n\n与 Docker 中容器概念相对应的，K8s 中也有着容器的概念。对于虚拟化的容器来说，最佳实践是一个容器一个应用，但当一个服务需要多个应用组合完成时，简单的将多个应用部署到一个容器内，就破坏了应用之间的隔离性，所以 K8s 对于容器进行了一层封装，形成了 Pod 的概念。\n\n#### Pod\n\nPod 是 Kubernetes 创建或部署的最小基本单元。一个 Pod 封装一个或多个应用容器、存储资源、一个独立的网络 IP 以及管理控制容器运行方式的策略选项。Pod 中的每个容器共享网络命名空间（包括 IP 与端口），Pod 内的容器可以使用 localhost 相互通信。Pod 可以指定一组共享存储卷 Volumes，Pod 中所有容器都可以访问共享的 Volumes。 \n\n通过 Pod，用户就可以非常方便地控制容器之间的隔离性。\n\n有了 Pod 作为基础以后，K8s 就要实现它最重要的功能，对容器的编排管理。当服务需要扩容时，K8s 需要能够快速复制 Pod，当 Pod 挂掉了，K8s 需要能够自动重启。所以 K8s 由此衍生出了 ReplicaSet 的概念。\n\n#### ReplicaSet\n\nReplicaSet 确保在任何时候都有按配置的 Pod 副本数在运行，通过标签选择器的方式对 Pod 进行筛选和管理。在旧的版本中还有一个 ReplicaController 的概念，RC 与 RS 两者功能完全相同，区别仅仅在于 RS 对于 Pod 的标签选择器更加强大。\n\n开头提到了 K8s 使用声明式的配置自动去管理容器，而 ReplicaSet 的内容却太过具体，涉及到了 Pod 的具体维护细节。所以 K8s 在 ReplicaSet 之上又衍生出声明式配置容器的概念，Deployment。\n\n#### Deployment\n\nDeployment 为 Pod 与 ReplicaSet 提供了声明式的定义，描述你想要的目标状态是什么，Deployment controller 就会帮你将 Pod 与 ReplicaSet 的实际状态改变到你想要的目标状态。\n\n以 fortune-teller 为例子，可以编写一份下面这样的 Deployment 配置文件\n```yaml\napiVersion: apps/v1 # k8s api版本\nkind: Deployment # 资源类型\nmetadata:\n  name: fortune-teller-app # deployment 名字\n  namespace: default\nspec:\n  replicas: 1 # Pod 副本数量\n  selector:\n    matchLabels:\n      k8s-app: fortune-teller-app # 管理标签中包含 k8s-app: fortune-teller-app 的 Pod\n  template: # Pod 模板\n    metadata:\n      labels:\n        k8s-app: fortune-teller-app # Pod 标签\n    spec: # Pod 配置\n      containers:\n      - image: quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n        imagePullPolicy: IfNotPresent\n        name: fortune-teller-app\n        ports:\n        - containerPort: 50051\n          name: grpc\n          protocol: TCP\n```\n\n将上面的内容保存到一份 yaml 文件中，执行以下命令，让 K8s 执行 yaml\n```bash\nkubectl apply -f deployment.yaml\n```\n\n通过以下命令，我们就可以看到刚刚创建的 deployment\n```bash\nkubectl get deployement\n```\n\n![](/asset/k8s-ramp-up/8.png)\n\n此时，K8s 已经自动根据 deployment 中配置的 Pod 模板和配置，创建了 Pod。通过以下命令，我们就可以看到 K8s 自动创建的 Pod\n```bash\nkubectl get pods\n```\n![](/asset/k8s-ramp-up/9.png)\n\n因为 K8s 采用声明式的配置去管理 Pod，所以我们可以动态地去修改 deployment 的配置，K8s 会自动根据新的配置去管理 Pod。\n```bash\nkubectl edit deployment fortune-teller-app\n```\n\n我们把配置文件中的副本数量修改为 2\n\n![](/asset/k8s-ramp-up/10.png)\n\n保存退出后，我们再次执行 kubectl get pods ，我们就可以看到 K8s 根据新的配置，创建了一个新的 Pod\n\n![](/asset/k8s-ramp-up/11.png)\n\n现在我们就有了两个 fortune-teller 的服务。在真实环境中，Pod 的调度由 K8s 进行管理，某个时刻服务可能在 Node1 上，而另一时刻服务可能就被调度到了 Node2 上。所以，访问具体 Pod 是一种不稳定的服务访问方法，而且目前大多数的后端服务都是无状态的服务，直接访问 Pod 也导致不能进行负载均衡。所以，K8s 在此基础上衍生出 Service 的概念。\n\n#### Service\n\nService 可以看做一组提供相同服务的 Pod 的对外访问接口。Kubernetes 提供三种类型的 Service：\n- NodePort： 集群外部可以通过 Node IP 与 Node Port 来访问具体某个 Pod，每台机器上都会暴露同样的端口\n- ClusterIP：指通过集群的内部 IP 暴露服务，服务只能够在集群内部可以访问，这也是默认的 ServiceType\n- ExternalName：不指向 Pod，指向外部服务\nService 和 Deployment 是一对比较容易混淆的概念，两者都是对一组 Pod 进行管理，但它们两者之间的关系可以用下面这张图来概括\n\n![](/asset/k8s-ramp-up/12.png)\n\nService 是面向服务调用者，也就是外部访问 K8s。而 Deployment 是面向 K8s 底层引擎的，面向内部管理者。\n\nService 的配置文件格式与 Deployment 很类似\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: fortune-teller-service\n  namespace: default\nspec:\n  ports:\n  - name: grpc\n    port: 50051\n    protocol: TCP\n    targetPort: 50051\n  selector:\n    k8s-app: fortune-teller-app\n  type: ClusterIP\n```\n\n同样的，我们通过 kubectl apply -f service.yaml 命令，可以创建 Service。通过 kubectl get service 可以查看到刚刚创建的 Service。\n\n![](/asset/k8s-ramp-up/13.png)\n\n接下来，我们登陆到一个 Pod 里去测试一下是否可以访问服务。\nNetshoot 镜像中包含了一些网络测试的工具，我们可以直接进入一个 netshoot 容器内测试。采用 deployment 的方式创建 Pod\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netshoot\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: netshoot\n  template:\n    metadata:\n      labels:\n        k8s-app: netshoot\n    spec:\n      containers:\n      - args:\n        - 1000d\n        command:\n        - /bin/sleep\n        image: nicolaka/netshoot\n        name: netshoot\n```\n\n与 Docker 的命令类似，使用命令 `kubectl cp grpcurl_1.6.0_linux_x86_64.tar.gz <pod name>:/` 复制工具到容器内。\n\n复制成功后，使用命令 kubectl exec -it <pod name> bash 可以进入到容器内。\n\n解压\n```bash\ncd /\ntar -zxf grpcurl_1.6.0_linux_x86_64.tar.gz\n```\n\n然后我们可以测试是否可以从 K8s 集群内访问 Service。对于 K8s 集群内部的服务，K8s 有自己的 DNS 组件，所以可以直接通过服务名访问。\n```bash\n./grpcurl -plaintext fortune-teller-service:50051 build.stack.fortune.FortuneTeller/Predict\n```\n\n![](/asset/k8s-ramp-up/14.png)\n\n验证服务可以从集群内访问之后，我们就需要解决如何从集群外访问服务的问题，毕竟大多数服务是面向 K8s 集群外的用户的。其实目前我们已经了解了一种解决方案，就是使用 Nodeport 类型的 Service。但采用这种方法有几个缺点：\n1. 每个端口只能是一种服务\n2. 端口范围只能是 30000-32767\n3. 如果节点 的 IP 地址发生变化，调用方需要能够察觉。\n所以，K8s 为服务的外部访问路由提供了新的类型 Ingress。\n\n#### Ingress\nIngress 其实是一种类似于路由表一样的配置，实际的路由工作需要 Ingress Controller 执行。K8s 本身并没有提供 Ingress Controller，目前常用的是通过 Nginx 实现的版本 https://github.com/kubernetes/ingress-nginx 。可以使用上面压缩包中的 ingress-controller.yaml 安装\n\n![](/asset/k8s-ramp-up/15.png)\n\nIngree nginx controller 通过宿主机暴露给外部访问的端口是随机的，所以我们修改 yaml，改成我们在一开始创建集群时映射的端口。\n\n通过 `kubectl get svc -n ingress-nginx` 我们就可以看到，暴露的是两个随机分配的端口\n\n![](/asset/k8s-ramp-up/16.png)\n\n我们手动将其改成 32080 和 32443。\n\n![](/asset/k8s-ramp-up/17.png)\n\nIngress nginx controller 同样是通过标签选择器的方式管理 Ingress。\n\n下面是一个简单的 Ingress，将发往 Host fortune.bytedance.com 的请求路由到 Service fortune-teller-service。\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: GRPC\n  name: fortune-ingress\n  namespace: default\nspec:\n  rules:\n  - host: fortune.bytedance.com\n    http:\n      paths:\n      - backend:\n          serviceName: fortune-teller-service\n          servicePort: grpc\n```\n\n安装 Ingress\n```bash\nkubectl apply -f ingress.yaml\nkubectl get ingress\n```\n\n![](/asset/k8s-ramp-up/18.png)\n\nIngress nginx controller 对于 gRpc 默认只支持 SSL 的形式，而Fedlearner 中的 controller 做了一些定制化操作，使得通过 80 端口，只使用 HTTP2 也可以转发 gRpc。详情可以参考 https://github.com/bytedance/ingress-nginx/pull/2/files\n\n使用下面这个命令，我们可以测试一下从外部访问服务\n```bash\ngrpcurl -insecure -servername 'fortune.bytedance.com' 0.0.0.0:32443 build.stack.fortune.FortuneTeller/Predict\n```\n\n![](/asset/k8s-ramp-up/19.png)\n\n刚才也提到了，gRpc 通常是使用 SSL 进行加密的，SSL 的关键在于公钥，私钥以及证书的验证。通过文件系统的方式确实可以处理证书的问题，但 K8s 抽象出 Secret 这种资源，大大提高了对这类文件的管理和复用能力。\n\n#### Secret\n\nSecret 解决了密码、token、密钥等敏感数据的存储问题，主要分为三种类型：\n- Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的 / run/secrets/kubernetes.io/serviceaccount 目录中\n- Opaque ：Base64 编码格式的 Secret，用来存储密码、密钥等\n- kubernetes.io/dockerconfigjson ：用来存储 docker registry 的认证信息\n\n接下来，我们就来创建一个 Opaque 类型的 Secret，使得 ingress nginx controller 支持服务端的 SSL。\n由于篇幅有限，这里简单介绍下证书相关的概念：\n- CA：证书授权中心(certificate authority)，用来签发私钥，并验证公钥，私钥的合法性\n- 私钥，公钥：私钥用于加密，公钥用于解密\n\nSecret 支持不编写 yaml，直接从文件中创建 Secret\n```bash\nkubectl create secret generic fortune-teller-ssl-verify \\\n  --from-file=ca.crt=CA.pem \\\n  --from-file=tls.crt=server-public.pem \\\n  --from-file=tls.key=server-private.key\n```\n\n修改 Ingress，使其使用新创建的 Secret 提供服务侧的 SSL。\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: GRPC\n  name: fortune-ingress\n  namespace: default\nspec:\n  rules:\n  - host: fortune.test.com\n    http:\n      paths:\n      - backend:\n          serviceName: fortune-teller-service\n          servicePort: grpc\n  tls:\n  - hosts:\n    - fortune.test.com\n    secretName: fortune-teller-ssl-verify\n```\n\n因为我们使用的是自签名的证书，不被公共的 CA 所信任，所以在发送请求是需要手动指定自己所信任的 CA。\n\n```bash\ngrpcurl -cacert CA.pem \\\n  -servername 'fortune.test.com' \\\n  127.0.0.1:32443 \\\n  build.stack.fortune.FortuneTeller/Predict\n```\n\n![](/asset/k8s-ramp-up/20.png)\n\n## 参考\n- https://draveness.me/docker/\n- https://www.yuque.com/kshare/2020/fe3b6f86-3b9b-48da-9770-897d838cbf41?language=zh-cn#Namespace\n- https://network.51cto.com/art/201907/598970.htm\n- https://blog.csdn.net/gatieme/article/details/51383322\n\n","slug":"k8s-ramp-up","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl5i8k7y500071rpf8od71152","content":"<h2 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h2><ul>\n<li>介绍 K8s，Docker 概念以及原理</li>\n<li>从 0 开始部署一个简单完整的服务</li>\n</ul>\n<h2 id=\"Docker是什么？\"><a href=\"#Docker是什么？\" class=\"headerlink\" title=\"Docker是什么？\"></a>Docker是什么？</h2><p>Docker是由Google推出的Go语言进行开发实现，基于Linux内核的 <font color=red>namespace</font>，对<font color=red>进程</font>进行封装<font color=red>隔离</font>，属于操作系统层面的容器化技术。</p>\n<p><img src=\"/asset/k8s-ramp-up/1.png\"></p>\n<h3 id=\"三大核心概念\"><a href=\"#三大核心概念\" class=\"headerlink\" title=\"三大核心概念\"></a>三大核心概念</h3><p>镜像（Image）</p>\n<p>容器（Container）</p>\n<p>仓库（Repository）</p>\n<p>从代码的角度来看，镜像就像一个类；容器是对象实例，运行时在系统中会有许多容器；仓库主要用于存储和维护这些镜像。</p>\n<h3 id=\"为什么使用-Docker？\"><a href=\"#为什么使用-Docker？\" class=\"headerlink\" title=\"为什么使用 Docker？\"></a>为什么使用 Docker？</h3><ul>\n<li>配置环境<br>开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性</li>\n<li>应用隔离<br>机器上可能同时运行多个服务。如果服务之间没有隔离，一个服务出现异常，往往可能会导致其他服务也挂掉。同时，不同服务所依赖的环境也可能发生冲突。</li>\n</ul>\n<h3 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h3><p>首先，要了解一下进程的命名空间。Linux 系统中的所有进程按照惯例是通过PID标识的，这意味着内核必须管理一个全局的PID列表。而且，所有调用者通过uname系统调用返回的系统相关信息（包括系统名称和有关内核的一些信息）都是相同的。</p>\n<p>Linux 的命名空间从内核层面上进行了虚拟化，对所有的全局资源进行一个抽象。本质上，建立了系统的不同视图。每一项全局资源都必须包装到命名空间的数据结构中，只有资源和包含资源的命名空间构成的二元组仍然是全局唯一的。不仅仅是 PID，Linux 通过同样的方法对其他资源也做了虚拟化处理。命名空间共有以下6种：</p>\n<p><img src=\"/asset/k8s-ramp-up/2.png\"></p>\n<p>借助 Linux 的命名空间，Docker 对进程进行隔离，可以从进程树的角度理解。</p>\n<p><img src=\"/asset/k8s-ramp-up/3.png\"></p>\n<p>每次在执行 <code>docker start</code> 或 <code>docker run</code> 的时候，其实是由 docker 的 daemon 进程 docker containerd，调用 Linux 系统调用 <code>clone()</code> 去创建新的进程。而创建进程的过程中就为新创建的进程分配了新的 Linux 命名空间。可以简单阅读一下 docker 的开源代码</p>\n<pre><code class=\"go\">// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L17\n// 创建容器的函数，其中又调用了设置\nfunc (daemon *Daemon) ContainerStart(name string, hostConfig *containertypes.HostConfig, checkpoint string, checkpointDir string) error\n\n// https://github.com/moby/moby/blob/470ae8422fc6f1845288eb7572253b08f1e6edf8/daemon/oci_linux.go#L212\n// 设置 Namespace\nfunc setNamespace(s *specs.Spec, ns specs.LinuxNamespace) &#123;\n   for i, n := range s.Linux.Namespaces &#123;\n      if n.Type == ns.Type &#123;\n         s.Linux.Namespaces[i] = ns\n         return\n      &#125;\n   &#125;\n   s.Linux.Namespaces = append(s.Linux.Namespaces, ns)\n&#125;\n\n// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L198\n// 创建新的进程\npid, err := daemon.containerd.Start(context.Background(), \n                                    container.ID, \n                                    checkpointDir,    \n                                    container.StreamConfig.Stdin() != nil | | container.Config.Tty, \n                                    container.InitializeStdio)\n</code></pre>\n\n<h3 id=\"如何安装？\"><a href=\"#如何安装？\" class=\"headerlink\" title=\"如何安装？\"></a>如何安装？</h3><p><a href=\"https://docs.docker.com/get-docker/\">https://docs.docker.com/get-docker/</a></p>\n<h2 id=\"Kubernetes是什么？\"><a href=\"#Kubernetes是什么？\" class=\"headerlink\" title=\"Kubernetes是什么？\"></a>Kubernetes是什么？</h2><p>Kubernetes 是 Google 于 2014 年基于其内部 Brog 系统开源的一个容器编排管理系统，可使用声明式的配置（以 yaml 文件的形式）自动地执行容器化应用程序的管理，包括部署、伸缩、负载均衡、回滚等。</p>\n<p>为什么叫 K8s？因为 K<font color=red>ubernete</font>s，中间是8个字母。</p>\n<p>kubernetes 提供的功能：</p>\n<ul>\n<li>自动发布与伸缩：可以通过声明式的配置文件定义想要部署的容器</li>\n<li>滚动升级与灰度发布：采用逐步替换的策略实现滚动升级</li>\n<li>服务发现与负载均衡：Kubernetes 通过 DNS 名称或 IP 地址暴露容器的访问方式，并且可在同一容器组内实现负载分发与均衡</li>\n<li>存储编排：Kubernetes 可以自动挂载指定的存储系统，如 local storage/nfs / 云存储等</li>\n<li>故障恢复：Kubernetes 自动重启已经停机的容器，替换不满足健康检查的容器</li>\n<li>密钥与配置管理：Kubernetes 可以存储与管理敏感信息，如 Docker Registry 的登录凭证，密码，ssh 密钥等</li>\n</ul>\n<h3 id=\"为什么使用-K8s？\"><a href=\"#为什么使用-K8s？\" class=\"headerlink\" title=\"为什么使用 K8s？\"></a>为什么使用 K8s？</h3><p>大型单体应用被逐渐拆分成小的、可独立运行的组件。随着部署组件的增多和数据中心的增长，配置、管理和运维变得很困难。(微服务）</p>\n<p>K8s 的定义就是容器编排和管理引擎，解决了这些问题。</p>\n<h3 id=\"如何安装？-1\"><a href=\"#如何安装？-1\" class=\"headerlink\" title=\"如何安装？\"></a>如何安装？</h3><p>由难到易(๑•̀ㅂ•́)و✧</p>\n<ul>\n<li>Kubeadm: <a href=\"https://kubernetes.io/docs/reference/setup-tools/kubeadm/\">https://kubernetes.io/docs/reference/setup-tools/kubeadm/</a></li>\n<li>MiniKube: Local kubernetes <a href=\"https://minikube.sigs.k8s.io/docs/start/\">https://minikube.sigs.k8s.io/docs/start/</a></li>\n<li>Kind: Kubernetes in Docker <a href=\"https://github.com/kubernetes-sigs/kind\">https://github.com/kubernetes-sigs/kind</a></li>\n<li>Docker-desktop（仅限 Mac）: 一键开启<br><img src=\"/asset/k8s-ramp-up/4.png\"></li>\n</ul>\n<p>其他版本的类 K8s 系统：</p>\n<ul>\n<li>K3s: <a href=\"https://github.com/k3s-io/k3s\">https://github.com/k3s-io/k3s</a></li>\n<li>K0s: <a href=\"https://github.com/k0sproject/k0s\">https://github.com/k0sproject/k0s</a></li>\n</ul>\n<h2 id=\"Kubernetes-架构\"><a href=\"#Kubernetes-架构\" class=\"headerlink\" title=\"Kubernetes 架构\"></a>Kubernetes 架构</h2><p><img src=\"/asset/k8s-ramp-up/5.png\"></p>\n<h3 id=\"master\"><a href=\"#master\" class=\"headerlink\" title=\"master\"></a>master</h3><p>Master 负责管理服务来对整个系统进行管理与控制，包括</p>\n<ul>\n<li>apiserver：作为整个系统的对外接口，提供一套 Restful API 供客户端调用，任何的资源请求 / 调用操作都是通过 kube-apiserver 提供的接口进行, 如 kubectl、kubernetes dashboard 等管理工具就是通过 apiserver 来实现对集群的管理</li>\n<li>kube-scheduler：资源调度器，负责将容器组分配到哪些节点上</li>\n<li>kube-controller-manager：管理控制器，集群中处理常规任务的后台线程，包括节点控制器（负责监听节点停机的事件并作出对应响应）、endpoint-controller（刷新服务与容器组的关联信息）、replication-controller（维护容器组的副本数为指定的数值）、Service Account &amp; Token 控制器（负责为新的命名空间创建默认的 Service Account 以及 API Access Token）</li>\n<li>etcd：数据存储，存储集群所有的配置信息</li>\n<li>coredns：实现集群内部通过服务名称进行容器组访问的功能</li>\n</ul>\n<h3 id=\"worker\"><a href=\"#worker\" class=\"headerlink\" title=\"worker\"></a>worker</h3><p>Worker 负载执行 Master 分配的任务，包括</p>\n<ul>\n<li>kubelet：是工作节点上执行操作的代理程序，负责容器的生命周期管理，定期执行容器健康检查，并上报容器的运行状态</li>\n<li>kube-proxy：是一个具有负载均衡能力的简单的网络访问代理，负责将访问某个服务的请求分配到工作节点的具体某个容器上（kube-proxy 也运行于 master node 上）</li>\n<li>Docker Daemon：Kubernetes 其实不局限于 Docker（即将取消），它支持任何实现了 Kubernetes 容器引擎接口的容器引擎，如 containerd、rktlet</li>\n</ul>\n<h3 id=\"网络通信\"><a href=\"#网络通信\" class=\"headerlink\" title=\"网络通信\"></a>网络通信</h3><p>网络通信组件只需要符合 CNI （Container Network Interface）接口规范，主要作用在于给各个容器分配集群内 IP，使得其内网 IP 能够集群内唯一，并且可以互相访问，目前常用的有 Flannel，Calico等网络组件。</p>\n<p>简单介绍下比较常用的 Flannel 的原理。Flannel 运行在第3层网络层，基于 IPv4，创建一个大型内部网络，跨越集群中每个节点。每个节点组成一个子网，每个容器在内网中有唯一的IP。</p>\n<p>首先，Flannel 会为每台节点分配一个子网段。Flanneld 在 Docker 容器启动时修改其启动参数，将其 IP 限制在当前的子网段内，具体 IP 的分配仍是由 docker 进行。Flannel通过Etcd服务维护了一张节点间的路由表，详细记录了各节点子网网段，保证不同节点的子网网段不会重复。</p>\n<p>数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，flanneld服务监听在网卡的另外一端。</p>\n<p>源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。</p>\n<p><img src=\"/asset/k8s-ramp-up/6.png\"></p>\n<h2 id=\"快速上手-K8s-概念\"><a href=\"#快速上手-K8s-概念\" class=\"headerlink\" title=\"快速上手 K8s 概念\"></a>快速上手 K8s 概念</h2><p>一些推荐的 K8s 概念介绍：</p>\n<ul>\n<li>微软的 50天 K8s 教程中（<a href=\"https://azure.microsoft.com/en-us/resources/kubernetes-learning-path/%EF%BC%89%E9%80%9A%E8%BF%87%E5%8A%A8%E7%89%A9%E5%9B%AD%E7%9A%84%E5%BD%A2%E5%BC%8F%E4%BB%8B%E7%BB%8D%E4%BA%86%E4%B8%80%E4%BA%9B\">https://azure.microsoft.com/en-us/resources/kubernetes-learning-path/）通过动物园的形式介绍了一些</a> K8s 概念 <a href=\"http://aka.ms/k8s/LearnwithPhippy\">http://aka.ms/k8s/LearnwithPhippy</a></li>\n<li>综述PPT：<a href=\"https://www2.slideshare.net/BobKillen/kubernetes-a-comprehensive-overview-updated?from_action=save\">https://www2.slideshare.net/BobKillen/kubernetes-a-comprehensive-overview-updated?from_action=save</a></li>\n</ul>\n<p>K8s 中的概念极多，比较零碎，这里通过一个简单的小例子，尽可能覆盖多的 K8s 概念。</p>\n<h2 id=\"概览\"><a href=\"#概览\" class=\"headerlink\" title=\"概览\"></a>概览</h2><p>例子使用一个开源的 fortune-teller 镜像（<code>quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1</code>） ，每次请求容器内的服务，服务会返回一句名言。希望在 MacOS 的环境下，展示一个应用在 K8s 中运行的全流程。</p>\n<p>准备环境<br>为了不影响大家本地的环境，这里使用 Kind 创建出一个独立的 K8s 集群，方便统一版本并且可以在完成快速清理掉。(Docker 双重隔离）</p>\n<ol>\n<li><p>安装 Kind 以及 gRpc 测试工具</p>\n<pre><code class=\"bash\">brew install kind\nbrew install grpcurl\n</code></pre>\n</li>\n<li><p>拉取镜像</p>\n<pre><code class=\"bash\">docker pull kindest/node:v1.16.15\ndocker pull quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n</code></pre>\n</li>\n<li><p>创建 K8s 集群，因为 Kind 是在 Docker 容器里面创建的 K8s，所以宿主机访问，需要把端口暴露出来。Kind 会默认把 K8s apiserver 的端口暴露出来，用来给 kubectl 命令使用。但为了之后的测试，我们提前把几个端口在创建的时候就暴露出来。</p>\n</li>\n</ol>\n<p>Kind 同样支持通过yaml 的形式创建集群</p>\n<pre><code class=\"yaml\">kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 32080\n    hostPort: 32080\n  - containerPort: 32443\n    hostPort: 32443\n</code></pre>\n<pre><code class=\"bash\">kind create cluster --name=fortune-teller --image=kindest/node:v1.16.15 --config kind-config.yaml\n</code></pre>\n<h3 id=\"运行-Docker-版本\"><a href=\"#运行-Docker-版本\" class=\"headerlink\" title=\"运行 Docker 版本\"></a>运行 Docker 版本</h3><ol>\n<li>启动容器<pre><code class=\"bash\">docker run -p 50051:50051 quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n</code></pre>\n</li>\n</ol>\n<p><code>-p</code> 将容器内的 50051 端口映射到宿主机的 50051 端口</p>\n<ol start=\"2\">\n<li>测试应用是否正常运行，第一次运行时可能需要给 grpcurl 开启权限<pre><code class=\"bash\">grpcurl -plaintext 127.0.0.1:50051 build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n</li>\n</ol>\n<p>应用在收到请求以后，会返回一句名言</p>\n<p><img src=\"/asset/k8s-ramp-up/7.png\"></p>\n<h3 id=\"将应用从-Docker-迁移到-K8s-中\"><a href=\"#将应用从-Docker-迁移到-K8s-中\" class=\"headerlink\" title=\"将应用从 Docker 迁移到 K8s 中\"></a>将应用从 Docker 迁移到 K8s 中</h3><p>与 Docker 中容器概念相对应的，K8s 中也有着容器的概念。对于虚拟化的容器来说，最佳实践是一个容器一个应用，但当一个服务需要多个应用组合完成时，简单的将多个应用部署到一个容器内，就破坏了应用之间的隔离性，所以 K8s 对于容器进行了一层封装，形成了 Pod 的概念。</p>\n<h4 id=\"Pod\"><a href=\"#Pod\" class=\"headerlink\" title=\"Pod\"></a>Pod</h4><p>Pod 是 Kubernetes 创建或部署的最小基本单元。一个 Pod 封装一个或多个应用容器、存储资源、一个独立的网络 IP 以及管理控制容器运行方式的策略选项。Pod 中的每个容器共享网络命名空间（包括 IP 与端口），Pod 内的容器可以使用 localhost 相互通信。Pod 可以指定一组共享存储卷 Volumes，Pod 中所有容器都可以访问共享的 Volumes。 </p>\n<p>通过 Pod，用户就可以非常方便地控制容器之间的隔离性。</p>\n<p>有了 Pod 作为基础以后，K8s 就要实现它最重要的功能，对容器的编排管理。当服务需要扩容时，K8s 需要能够快速复制 Pod，当 Pod 挂掉了，K8s 需要能够自动重启。所以 K8s 由此衍生出了 ReplicaSet 的概念。</p>\n<h4 id=\"ReplicaSet\"><a href=\"#ReplicaSet\" class=\"headerlink\" title=\"ReplicaSet\"></a>ReplicaSet</h4><p>ReplicaSet 确保在任何时候都有按配置的 Pod 副本数在运行，通过标签选择器的方式对 Pod 进行筛选和管理。在旧的版本中还有一个 ReplicaController 的概念，RC 与 RS 两者功能完全相同，区别仅仅在于 RS 对于 Pod 的标签选择器更加强大。</p>\n<p>开头提到了 K8s 使用声明式的配置自动去管理容器，而 ReplicaSet 的内容却太过具体，涉及到了 Pod 的具体维护细节。所以 K8s 在 ReplicaSet 之上又衍生出声明式配置容器的概念，Deployment。</p>\n<h4 id=\"Deployment\"><a href=\"#Deployment\" class=\"headerlink\" title=\"Deployment\"></a>Deployment</h4><p>Deployment 为 Pod 与 ReplicaSet 提供了声明式的定义，描述你想要的目标状态是什么，Deployment controller 就会帮你将 Pod 与 ReplicaSet 的实际状态改变到你想要的目标状态。</p>\n<p>以 fortune-teller 为例子，可以编写一份下面这样的 Deployment 配置文件</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1 # k8s api版本\nkind: Deployment # 资源类型\nmetadata:\n  name: fortune-teller-app # deployment 名字\n  namespace: default\nspec:\n  replicas: 1 # Pod 副本数量\n  selector:\n    matchLabels:\n      k8s-app: fortune-teller-app # 管理标签中包含 k8s-app: fortune-teller-app 的 Pod\n  template: # Pod 模板\n    metadata:\n      labels:\n        k8s-app: fortune-teller-app # Pod 标签\n    spec: # Pod 配置\n      containers:\n      - image: quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n        imagePullPolicy: IfNotPresent\n        name: fortune-teller-app\n        ports:\n        - containerPort: 50051\n          name: grpc\n          protocol: TCP\n</code></pre>\n<p>将上面的内容保存到一份 yaml 文件中，执行以下命令，让 K8s 执行 yaml</p>\n<pre><code class=\"bash\">kubectl apply -f deployment.yaml\n</code></pre>\n<p>通过以下命令，我们就可以看到刚刚创建的 deployment</p>\n<pre><code class=\"bash\">kubectl get deployement\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/8.png\"></p>\n<p>此时，K8s 已经自动根据 deployment 中配置的 Pod 模板和配置，创建了 Pod。通过以下命令，我们就可以看到 K8s 自动创建的 Pod</p>\n<pre><code class=\"bash\">kubectl get pods\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/9.png\"></p>\n<p>因为 K8s 采用声明式的配置去管理 Pod，所以我们可以动态地去修改 deployment 的配置，K8s 会自动根据新的配置去管理 Pod。</p>\n<pre><code class=\"bash\">kubectl edit deployment fortune-teller-app\n</code></pre>\n<p>我们把配置文件中的副本数量修改为 2</p>\n<p><img src=\"/asset/k8s-ramp-up/10.png\"></p>\n<p>保存退出后，我们再次执行 kubectl get pods ，我们就可以看到 K8s 根据新的配置，创建了一个新的 Pod</p>\n<p><img src=\"/asset/k8s-ramp-up/11.png\"></p>\n<p>现在我们就有了两个 fortune-teller 的服务。在真实环境中，Pod 的调度由 K8s 进行管理，某个时刻服务可能在 Node1 上，而另一时刻服务可能就被调度到了 Node2 上。所以，访问具体 Pod 是一种不稳定的服务访问方法，而且目前大多数的后端服务都是无状态的服务，直接访问 Pod 也导致不能进行负载均衡。所以，K8s 在此基础上衍生出 Service 的概念。</p>\n<h4 id=\"Service\"><a href=\"#Service\" class=\"headerlink\" title=\"Service\"></a>Service</h4><p>Service 可以看做一组提供相同服务的 Pod 的对外访问接口。Kubernetes 提供三种类型的 Service：</p>\n<ul>\n<li>NodePort： 集群外部可以通过 Node IP 与 Node Port 来访问具体某个 Pod，每台机器上都会暴露同样的端口</li>\n<li>ClusterIP：指通过集群的内部 IP 暴露服务，服务只能够在集群内部可以访问，这也是默认的 ServiceType</li>\n<li>ExternalName：不指向 Pod，指向外部服务<br>Service 和 Deployment 是一对比较容易混淆的概念，两者都是对一组 Pod 进行管理，但它们两者之间的关系可以用下面这张图来概括</li>\n</ul>\n<p><img src=\"/asset/k8s-ramp-up/12.png\"></p>\n<p>Service 是面向服务调用者，也就是外部访问 K8s。而 Deployment 是面向 K8s 底层引擎的，面向内部管理者。</p>\n<p>Service 的配置文件格式与 Deployment 很类似</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: fortune-teller-service\n  namespace: default\nspec:\n  ports:\n  - name: grpc\n    port: 50051\n    protocol: TCP\n    targetPort: 50051\n  selector:\n    k8s-app: fortune-teller-app\n  type: ClusterIP\n</code></pre>\n<p>同样的，我们通过 kubectl apply -f service.yaml 命令，可以创建 Service。通过 kubectl get service 可以查看到刚刚创建的 Service。</p>\n<p><img src=\"/asset/k8s-ramp-up/13.png\"></p>\n<p>接下来，我们登陆到一个 Pod 里去测试一下是否可以访问服务。<br>Netshoot 镜像中包含了一些网络测试的工具，我们可以直接进入一个 netshoot 容器内测试。采用 deployment 的方式创建 Pod</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netshoot\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: netshoot\n  template:\n    metadata:\n      labels:\n        k8s-app: netshoot\n    spec:\n      containers:\n      - args:\n        - 1000d\n        command:\n        - /bin/sleep\n        image: nicolaka/netshoot\n        name: netshoot\n</code></pre>\n<p>与 Docker 的命令类似，使用命令 <code>kubectl cp grpcurl_1.6.0_linux_x86_64.tar.gz &lt;pod name&gt;:/</code> 复制工具到容器内。</p>\n<p>复制成功后，使用命令 kubectl exec -it <pod name> bash 可以进入到容器内。</p>\n<p>解压</p>\n<pre><code class=\"bash\">cd /\ntar -zxf grpcurl_1.6.0_linux_x86_64.tar.gz\n</code></pre>\n<p>然后我们可以测试是否可以从 K8s 集群内访问 Service。对于 K8s 集群内部的服务，K8s 有自己的 DNS 组件，所以可以直接通过服务名访问。</p>\n<pre><code class=\"bash\">./grpcurl -plaintext fortune-teller-service:50051 build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/14.png\"></p>\n<p>验证服务可以从集群内访问之后，我们就需要解决如何从集群外访问服务的问题，毕竟大多数服务是面向 K8s 集群外的用户的。其实目前我们已经了解了一种解决方案，就是使用 Nodeport 类型的 Service。但采用这种方法有几个缺点：</p>\n<ol>\n<li>每个端口只能是一种服务</li>\n<li>端口范围只能是 30000-32767</li>\n<li>如果节点 的 IP 地址发生变化，调用方需要能够察觉。<br>所以，K8s 为服务的外部访问路由提供了新的类型 Ingress。</li>\n</ol>\n<h4 id=\"Ingress\"><a href=\"#Ingress\" class=\"headerlink\" title=\"Ingress\"></a>Ingress</h4><p>Ingress 其实是一种类似于路由表一样的配置，实际的路由工作需要 Ingress Controller 执行。K8s 本身并没有提供 Ingress Controller，目前常用的是通过 Nginx 实现的版本 <a href=\"https://github.com/kubernetes/ingress-nginx\">https://github.com/kubernetes/ingress-nginx</a> 。可以使用上面压缩包中的 ingress-controller.yaml 安装</p>\n<p><img src=\"/asset/k8s-ramp-up/15.png\"></p>\n<p>Ingree nginx controller 通过宿主机暴露给外部访问的端口是随机的，所以我们修改 yaml，改成我们在一开始创建集群时映射的端口。</p>\n<p>通过 <code>kubectl get svc -n ingress-nginx</code> 我们就可以看到，暴露的是两个随机分配的端口</p>\n<p><img src=\"/asset/k8s-ramp-up/16.png\"></p>\n<p>我们手动将其改成 32080 和 32443。</p>\n<p><img src=\"/asset/k8s-ramp-up/17.png\"></p>\n<p>Ingress nginx controller 同样是通过标签选择器的方式管理 Ingress。</p>\n<p>下面是一个简单的 Ingress，将发往 Host fortune.bytedance.com 的请求路由到 Service fortune-teller-service。</p>\n<pre><code class=\"yaml\">apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: GRPC\n  name: fortune-ingress\n  namespace: default\nspec:\n  rules:\n  - host: fortune.bytedance.com\n    http:\n      paths:\n      - backend:\n          serviceName: fortune-teller-service\n          servicePort: grpc\n</code></pre>\n<p>安装 Ingress</p>\n<pre><code class=\"bash\">kubectl apply -f ingress.yaml\nkubectl get ingress\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/18.png\"></p>\n<p>Ingress nginx controller 对于 gRpc 默认只支持 SSL 的形式，而Fedlearner 中的 controller 做了一些定制化操作，使得通过 80 端口，只使用 HTTP2 也可以转发 gRpc。详情可以参考 <a href=\"https://github.com/bytedance/ingress-nginx/pull/2/files\">https://github.com/bytedance/ingress-nginx/pull/2/files</a></p>\n<p>使用下面这个命令，我们可以测试一下从外部访问服务</p>\n<pre><code class=\"bash\">grpcurl -insecure -servername &#39;fortune.bytedance.com&#39; 0.0.0.0:32443 build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/19.png\"></p>\n<p>刚才也提到了，gRpc 通常是使用 SSL 进行加密的，SSL 的关键在于公钥，私钥以及证书的验证。通过文件系统的方式确实可以处理证书的问题，但 K8s 抽象出 Secret 这种资源，大大提高了对这类文件的管理和复用能力。</p>\n<h4 id=\"Secret\"><a href=\"#Secret\" class=\"headerlink\" title=\"Secret\"></a>Secret</h4><p>Secret 解决了密码、token、密钥等敏感数据的存储问题，主要分为三种类型：</p>\n<ul>\n<li>Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的 / run/secrets/kubernetes.io/serviceaccount 目录中</li>\n<li>Opaque ：Base64 编码格式的 Secret，用来存储密码、密钥等</li>\n<li>kubernetes.io/dockerconfigjson ：用来存储 docker registry 的认证信息</li>\n</ul>\n<p>接下来，我们就来创建一个 Opaque 类型的 Secret，使得 ingress nginx controller 支持服务端的 SSL。<br>由于篇幅有限，这里简单介绍下证书相关的概念：</p>\n<ul>\n<li>CA：证书授权中心(certificate authority)，用来签发私钥，并验证公钥，私钥的合法性</li>\n<li>私钥，公钥：私钥用于加密，公钥用于解密</li>\n</ul>\n<p>Secret 支持不编写 yaml，直接从文件中创建 Secret</p>\n<pre><code class=\"bash\">kubectl create secret generic fortune-teller-ssl-verify \\\n  --from-file=ca.crt=CA.pem \\\n  --from-file=tls.crt=server-public.pem \\\n  --from-file=tls.key=server-private.key\n</code></pre>\n<p>修改 Ingress，使其使用新创建的 Secret 提供服务侧的 SSL。</p>\n<pre><code class=\"yaml\">apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: GRPC\n  name: fortune-ingress\n  namespace: default\nspec:\n  rules:\n  - host: fortune.test.com\n    http:\n      paths:\n      - backend:\n          serviceName: fortune-teller-service\n          servicePort: grpc\n  tls:\n  - hosts:\n    - fortune.test.com\n    secretName: fortune-teller-ssl-verify\n</code></pre>\n<p>因为我们使用的是自签名的证书，不被公共的 CA 所信任，所以在发送请求是需要手动指定自己所信任的 CA。</p>\n<pre><code class=\"bash\">grpcurl -cacert CA.pem \\\n  -servername &#39;fortune.test.com&#39; \\\n  127.0.0.1:32443 \\\n  build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/20.png\"></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ul>\n<li><a href=\"https://draveness.me/docker/\">https://draveness.me/docker/</a></li>\n<li><a href=\"https://www.yuque.com/kshare/2020/fe3b6f86-3b9b-48da-9770-897d838cbf41?language=zh-cn#Namespace\">https://www.yuque.com/kshare/2020/fe3b6f86-3b9b-48da-9770-897d838cbf41?language=zh-cn#Namespace</a></li>\n<li><a href=\"https://network.51cto.com/art/201907/598970.htm\">https://network.51cto.com/art/201907/598970.htm</a></li>\n<li><a href=\"https://blog.csdn.net/gatieme/article/details/51383322\">https://blog.csdn.net/gatieme/article/details/51383322</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h2><ul>\n<li>介绍 K8s，Docker 概念以及原理</li>\n<li>从 0 开始部署一个简单完整的服务</li>\n</ul>\n<h2 id=\"Docker是什么？\"><a href=\"#Docker是什么？\" class=\"headerlink\" title=\"Docker是什么？\"></a>Docker是什么？</h2><p>Docker是由Google推出的Go语言进行开发实现，基于Linux内核的 <font color=red>namespace</font>，对<font color=red>进程</font>进行封装<font color=red>隔离</font>，属于操作系统层面的容器化技术。</p>\n<p><img src=\"/asset/k8s-ramp-up/1.png\"></p>\n<h3 id=\"三大核心概念\"><a href=\"#三大核心概念\" class=\"headerlink\" title=\"三大核心概念\"></a>三大核心概念</h3><p>镜像（Image）</p>\n<p>容器（Container）</p>\n<p>仓库（Repository）</p>\n<p>从代码的角度来看，镜像就像一个类；容器是对象实例，运行时在系统中会有许多容器；仓库主要用于存储和维护这些镜像。</p>\n<h3 id=\"为什么使用-Docker？\"><a href=\"#为什么使用-Docker？\" class=\"headerlink\" title=\"为什么使用 Docker？\"></a>为什么使用 Docker？</h3><ul>\n<li>配置环境<br>开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性</li>\n<li>应用隔离<br>机器上可能同时运行多个服务。如果服务之间没有隔离，一个服务出现异常，往往可能会导致其他服务也挂掉。同时，不同服务所依赖的环境也可能发生冲突。</li>\n</ul>\n<h3 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h3><p>首先，要了解一下进程的命名空间。Linux 系统中的所有进程按照惯例是通过PID标识的，这意味着内核必须管理一个全局的PID列表。而且，所有调用者通过uname系统调用返回的系统相关信息（包括系统名称和有关内核的一些信息）都是相同的。</p>\n<p>Linux 的命名空间从内核层面上进行了虚拟化，对所有的全局资源进行一个抽象。本质上，建立了系统的不同视图。每一项全局资源都必须包装到命名空间的数据结构中，只有资源和包含资源的命名空间构成的二元组仍然是全局唯一的。不仅仅是 PID，Linux 通过同样的方法对其他资源也做了虚拟化处理。命名空间共有以下6种：</p>\n<p><img src=\"/asset/k8s-ramp-up/2.png\"></p>\n<p>借助 Linux 的命名空间，Docker 对进程进行隔离，可以从进程树的角度理解。</p>\n<p><img src=\"/asset/k8s-ramp-up/3.png\"></p>\n<p>每次在执行 <code>docker start</code> 或 <code>docker run</code> 的时候，其实是由 docker 的 daemon 进程 docker containerd，调用 Linux 系统调用 <code>clone()</code> 去创建新的进程。而创建进程的过程中就为新创建的进程分配了新的 Linux 命名空间。可以简单阅读一下 docker 的开源代码</p>\n<pre><code class=\"go\">// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L17\n// 创建容器的函数，其中又调用了设置\nfunc (daemon *Daemon) ContainerStart(name string, hostConfig *containertypes.HostConfig, checkpoint string, checkpointDir string) error\n\n// https://github.com/moby/moby/blob/470ae8422fc6f1845288eb7572253b08f1e6edf8/daemon/oci_linux.go#L212\n// 设置 Namespace\nfunc setNamespace(s *specs.Spec, ns specs.LinuxNamespace) &#123;\n   for i, n := range s.Linux.Namespaces &#123;\n      if n.Type == ns.Type &#123;\n         s.Linux.Namespaces[i] = ns\n         return\n      &#125;\n   &#125;\n   s.Linux.Namespaces = append(s.Linux.Namespaces, ns)\n&#125;\n\n// https://github.com/moby/moby/blob/49e809fbfe250f3df2deacc0c3e5c403db3b8915/daemon/start.go#L198\n// 创建新的进程\npid, err := daemon.containerd.Start(context.Background(), \n                                    container.ID, \n                                    checkpointDir,    \n                                    container.StreamConfig.Stdin() != nil | | container.Config.Tty, \n                                    container.InitializeStdio)\n</code></pre>\n\n<h3 id=\"如何安装？\"><a href=\"#如何安装？\" class=\"headerlink\" title=\"如何安装？\"></a>如何安装？</h3><p><a href=\"https://docs.docker.com/get-docker/\">https://docs.docker.com/get-docker/</a></p>\n<h2 id=\"Kubernetes是什么？\"><a href=\"#Kubernetes是什么？\" class=\"headerlink\" title=\"Kubernetes是什么？\"></a>Kubernetes是什么？</h2><p>Kubernetes 是 Google 于 2014 年基于其内部 Brog 系统开源的一个容器编排管理系统，可使用声明式的配置（以 yaml 文件的形式）自动地执行容器化应用程序的管理，包括部署、伸缩、负载均衡、回滚等。</p>\n<p>为什么叫 K8s？因为 K<font color=red>ubernete</font>s，中间是8个字母。</p>\n<p>kubernetes 提供的功能：</p>\n<ul>\n<li>自动发布与伸缩：可以通过声明式的配置文件定义想要部署的容器</li>\n<li>滚动升级与灰度发布：采用逐步替换的策略实现滚动升级</li>\n<li>服务发现与负载均衡：Kubernetes 通过 DNS 名称或 IP 地址暴露容器的访问方式，并且可在同一容器组内实现负载分发与均衡</li>\n<li>存储编排：Kubernetes 可以自动挂载指定的存储系统，如 local storage/nfs / 云存储等</li>\n<li>故障恢复：Kubernetes 自动重启已经停机的容器，替换不满足健康检查的容器</li>\n<li>密钥与配置管理：Kubernetes 可以存储与管理敏感信息，如 Docker Registry 的登录凭证，密码，ssh 密钥等</li>\n</ul>\n<h3 id=\"为什么使用-K8s？\"><a href=\"#为什么使用-K8s？\" class=\"headerlink\" title=\"为什么使用 K8s？\"></a>为什么使用 K8s？</h3><p>大型单体应用被逐渐拆分成小的、可独立运行的组件。随着部署组件的增多和数据中心的增长，配置、管理和运维变得很困难。(微服务）</p>\n<p>K8s 的定义就是容器编排和管理引擎，解决了这些问题。</p>\n<h3 id=\"如何安装？-1\"><a href=\"#如何安装？-1\" class=\"headerlink\" title=\"如何安装？\"></a>如何安装？</h3><p>由难到易(๑•̀ㅂ•́)و✧</p>\n<ul>\n<li>Kubeadm: <a href=\"https://kubernetes.io/docs/reference/setup-tools/kubeadm/\">https://kubernetes.io/docs/reference/setup-tools/kubeadm/</a></li>\n<li>MiniKube: Local kubernetes <a href=\"https://minikube.sigs.k8s.io/docs/start/\">https://minikube.sigs.k8s.io/docs/start/</a></li>\n<li>Kind: Kubernetes in Docker <a href=\"https://github.com/kubernetes-sigs/kind\">https://github.com/kubernetes-sigs/kind</a></li>\n<li>Docker-desktop（仅限 Mac）: 一键开启<br><img src=\"/asset/k8s-ramp-up/4.png\"></li>\n</ul>\n<p>其他版本的类 K8s 系统：</p>\n<ul>\n<li>K3s: <a href=\"https://github.com/k3s-io/k3s\">https://github.com/k3s-io/k3s</a></li>\n<li>K0s: <a href=\"https://github.com/k0sproject/k0s\">https://github.com/k0sproject/k0s</a></li>\n</ul>\n<h2 id=\"Kubernetes-架构\"><a href=\"#Kubernetes-架构\" class=\"headerlink\" title=\"Kubernetes 架构\"></a>Kubernetes 架构</h2><p><img src=\"/asset/k8s-ramp-up/5.png\"></p>\n<h3 id=\"master\"><a href=\"#master\" class=\"headerlink\" title=\"master\"></a>master</h3><p>Master 负责管理服务来对整个系统进行管理与控制，包括</p>\n<ul>\n<li>apiserver：作为整个系统的对外接口，提供一套 Restful API 供客户端调用，任何的资源请求 / 调用操作都是通过 kube-apiserver 提供的接口进行, 如 kubectl、kubernetes dashboard 等管理工具就是通过 apiserver 来实现对集群的管理</li>\n<li>kube-scheduler：资源调度器，负责将容器组分配到哪些节点上</li>\n<li>kube-controller-manager：管理控制器，集群中处理常规任务的后台线程，包括节点控制器（负责监听节点停机的事件并作出对应响应）、endpoint-controller（刷新服务与容器组的关联信息）、replication-controller（维护容器组的副本数为指定的数值）、Service Account &amp; Token 控制器（负责为新的命名空间创建默认的 Service Account 以及 API Access Token）</li>\n<li>etcd：数据存储，存储集群所有的配置信息</li>\n<li>coredns：实现集群内部通过服务名称进行容器组访问的功能</li>\n</ul>\n<h3 id=\"worker\"><a href=\"#worker\" class=\"headerlink\" title=\"worker\"></a>worker</h3><p>Worker 负载执行 Master 分配的任务，包括</p>\n<ul>\n<li>kubelet：是工作节点上执行操作的代理程序，负责容器的生命周期管理，定期执行容器健康检查，并上报容器的运行状态</li>\n<li>kube-proxy：是一个具有负载均衡能力的简单的网络访问代理，负责将访问某个服务的请求分配到工作节点的具体某个容器上（kube-proxy 也运行于 master node 上）</li>\n<li>Docker Daemon：Kubernetes 其实不局限于 Docker（即将取消），它支持任何实现了 Kubernetes 容器引擎接口的容器引擎，如 containerd、rktlet</li>\n</ul>\n<h3 id=\"网络通信\"><a href=\"#网络通信\" class=\"headerlink\" title=\"网络通信\"></a>网络通信</h3><p>网络通信组件只需要符合 CNI （Container Network Interface）接口规范，主要作用在于给各个容器分配集群内 IP，使得其内网 IP 能够集群内唯一，并且可以互相访问，目前常用的有 Flannel，Calico等网络组件。</p>\n<p>简单介绍下比较常用的 Flannel 的原理。Flannel 运行在第3层网络层，基于 IPv4，创建一个大型内部网络，跨越集群中每个节点。每个节点组成一个子网，每个容器在内网中有唯一的IP。</p>\n<p>首先，Flannel 会为每台节点分配一个子网段。Flanneld 在 Docker 容器启动时修改其启动参数，将其 IP 限制在当前的子网段内，具体 IP 的分配仍是由 docker 进行。Flannel通过Etcd服务维护了一张节点间的路由表，详细记录了各节点子网网段，保证不同节点的子网网段不会重复。</p>\n<p>数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，flanneld服务监听在网卡的另外一端。</p>\n<p>源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。</p>\n<p><img src=\"/asset/k8s-ramp-up/6.png\"></p>\n<h2 id=\"快速上手-K8s-概念\"><a href=\"#快速上手-K8s-概念\" class=\"headerlink\" title=\"快速上手 K8s 概念\"></a>快速上手 K8s 概念</h2><p>一些推荐的 K8s 概念介绍：</p>\n<ul>\n<li>微软的 50天 K8s 教程中（<a href=\"https://azure.microsoft.com/en-us/resources/kubernetes-learning-path/%EF%BC%89%E9%80%9A%E8%BF%87%E5%8A%A8%E7%89%A9%E5%9B%AD%E7%9A%84%E5%BD%A2%E5%BC%8F%E4%BB%8B%E7%BB%8D%E4%BA%86%E4%B8%80%E4%BA%9B\">https://azure.microsoft.com/en-us/resources/kubernetes-learning-path/）通过动物园的形式介绍了一些</a> K8s 概念 <a href=\"http://aka.ms/k8s/LearnwithPhippy\">http://aka.ms/k8s/LearnwithPhippy</a></li>\n<li>综述PPT：<a href=\"https://www2.slideshare.net/BobKillen/kubernetes-a-comprehensive-overview-updated?from_action=save\">https://www2.slideshare.net/BobKillen/kubernetes-a-comprehensive-overview-updated?from_action=save</a></li>\n</ul>\n<p>K8s 中的概念极多，比较零碎，这里通过一个简单的小例子，尽可能覆盖多的 K8s 概念。</p>\n<h2 id=\"概览\"><a href=\"#概览\" class=\"headerlink\" title=\"概览\"></a>概览</h2><p>例子使用一个开源的 fortune-teller 镜像（<code>quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1</code>） ，每次请求容器内的服务，服务会返回一句名言。希望在 MacOS 的环境下，展示一个应用在 K8s 中运行的全流程。</p>\n<p>准备环境<br>为了不影响大家本地的环境，这里使用 Kind 创建出一个独立的 K8s 集群，方便统一版本并且可以在完成快速清理掉。(Docker 双重隔离）</p>\n<ol>\n<li><p>安装 Kind 以及 gRpc 测试工具</p>\n<pre><code class=\"bash\">brew install kind\nbrew install grpcurl\n</code></pre>\n</li>\n<li><p>拉取镜像</p>\n<pre><code class=\"bash\">docker pull kindest/node:v1.16.15\ndocker pull quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n</code></pre>\n</li>\n<li><p>创建 K8s 集群，因为 Kind 是在 Docker 容器里面创建的 K8s，所以宿主机访问，需要把端口暴露出来。Kind 会默认把 K8s apiserver 的端口暴露出来，用来给 kubectl 命令使用。但为了之后的测试，我们提前把几个端口在创建的时候就暴露出来。</p>\n</li>\n</ol>\n<p>Kind 同样支持通过yaml 的形式创建集群</p>\n<pre><code class=\"yaml\">kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 32080\n    hostPort: 32080\n  - containerPort: 32443\n    hostPort: 32443\n</code></pre>\n<pre><code class=\"bash\">kind create cluster --name=fortune-teller --image=kindest/node:v1.16.15 --config kind-config.yaml\n</code></pre>\n<h3 id=\"运行-Docker-版本\"><a href=\"#运行-Docker-版本\" class=\"headerlink\" title=\"运行 Docker 版本\"></a>运行 Docker 版本</h3><ol>\n<li>启动容器<pre><code class=\"bash\">docker run -p 50051:50051 quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n</code></pre>\n</li>\n</ol>\n<p><code>-p</code> 将容器内的 50051 端口映射到宿主机的 50051 端口</p>\n<ol start=\"2\">\n<li>测试应用是否正常运行，第一次运行时可能需要给 grpcurl 开启权限<pre><code class=\"bash\">grpcurl -plaintext 127.0.0.1:50051 build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n</li>\n</ol>\n<p>应用在收到请求以后，会返回一句名言</p>\n<p><img src=\"/asset/k8s-ramp-up/7.png\"></p>\n<h3 id=\"将应用从-Docker-迁移到-K8s-中\"><a href=\"#将应用从-Docker-迁移到-K8s-中\" class=\"headerlink\" title=\"将应用从 Docker 迁移到 K8s 中\"></a>将应用从 Docker 迁移到 K8s 中</h3><p>与 Docker 中容器概念相对应的，K8s 中也有着容器的概念。对于虚拟化的容器来说，最佳实践是一个容器一个应用，但当一个服务需要多个应用组合完成时，简单的将多个应用部署到一个容器内，就破坏了应用之间的隔离性，所以 K8s 对于容器进行了一层封装，形成了 Pod 的概念。</p>\n<h4 id=\"Pod\"><a href=\"#Pod\" class=\"headerlink\" title=\"Pod\"></a>Pod</h4><p>Pod 是 Kubernetes 创建或部署的最小基本单元。一个 Pod 封装一个或多个应用容器、存储资源、一个独立的网络 IP 以及管理控制容器运行方式的策略选项。Pod 中的每个容器共享网络命名空间（包括 IP 与端口），Pod 内的容器可以使用 localhost 相互通信。Pod 可以指定一组共享存储卷 Volumes，Pod 中所有容器都可以访问共享的 Volumes。 </p>\n<p>通过 Pod，用户就可以非常方便地控制容器之间的隔离性。</p>\n<p>有了 Pod 作为基础以后，K8s 就要实现它最重要的功能，对容器的编排管理。当服务需要扩容时，K8s 需要能够快速复制 Pod，当 Pod 挂掉了，K8s 需要能够自动重启。所以 K8s 由此衍生出了 ReplicaSet 的概念。</p>\n<h4 id=\"ReplicaSet\"><a href=\"#ReplicaSet\" class=\"headerlink\" title=\"ReplicaSet\"></a>ReplicaSet</h4><p>ReplicaSet 确保在任何时候都有按配置的 Pod 副本数在运行，通过标签选择器的方式对 Pod 进行筛选和管理。在旧的版本中还有一个 ReplicaController 的概念，RC 与 RS 两者功能完全相同，区别仅仅在于 RS 对于 Pod 的标签选择器更加强大。</p>\n<p>开头提到了 K8s 使用声明式的配置自动去管理容器，而 ReplicaSet 的内容却太过具体，涉及到了 Pod 的具体维护细节。所以 K8s 在 ReplicaSet 之上又衍生出声明式配置容器的概念，Deployment。</p>\n<h4 id=\"Deployment\"><a href=\"#Deployment\" class=\"headerlink\" title=\"Deployment\"></a>Deployment</h4><p>Deployment 为 Pod 与 ReplicaSet 提供了声明式的定义，描述你想要的目标状态是什么，Deployment controller 就会帮你将 Pod 与 ReplicaSet 的实际状态改变到你想要的目标状态。</p>\n<p>以 fortune-teller 为例子，可以编写一份下面这样的 Deployment 配置文件</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1 # k8s api版本\nkind: Deployment # 资源类型\nmetadata:\n  name: fortune-teller-app # deployment 名字\n  namespace: default\nspec:\n  replicas: 1 # Pod 副本数量\n  selector:\n    matchLabels:\n      k8s-app: fortune-teller-app # 管理标签中包含 k8s-app: fortune-teller-app 的 Pod\n  template: # Pod 模板\n    metadata:\n      labels:\n        k8s-app: fortune-teller-app # Pod 标签\n    spec: # Pod 配置\n      containers:\n      - image: quay.io/kubernetes-ingress-controller/grpc-fortune-teller:0.1\n        imagePullPolicy: IfNotPresent\n        name: fortune-teller-app\n        ports:\n        - containerPort: 50051\n          name: grpc\n          protocol: TCP\n</code></pre>\n<p>将上面的内容保存到一份 yaml 文件中，执行以下命令，让 K8s 执行 yaml</p>\n<pre><code class=\"bash\">kubectl apply -f deployment.yaml\n</code></pre>\n<p>通过以下命令，我们就可以看到刚刚创建的 deployment</p>\n<pre><code class=\"bash\">kubectl get deployement\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/8.png\"></p>\n<p>此时，K8s 已经自动根据 deployment 中配置的 Pod 模板和配置，创建了 Pod。通过以下命令，我们就可以看到 K8s 自动创建的 Pod</p>\n<pre><code class=\"bash\">kubectl get pods\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/9.png\"></p>\n<p>因为 K8s 采用声明式的配置去管理 Pod，所以我们可以动态地去修改 deployment 的配置，K8s 会自动根据新的配置去管理 Pod。</p>\n<pre><code class=\"bash\">kubectl edit deployment fortune-teller-app\n</code></pre>\n<p>我们把配置文件中的副本数量修改为 2</p>\n<p><img src=\"/asset/k8s-ramp-up/10.png\"></p>\n<p>保存退出后，我们再次执行 kubectl get pods ，我们就可以看到 K8s 根据新的配置，创建了一个新的 Pod</p>\n<p><img src=\"/asset/k8s-ramp-up/11.png\"></p>\n<p>现在我们就有了两个 fortune-teller 的服务。在真实环境中，Pod 的调度由 K8s 进行管理，某个时刻服务可能在 Node1 上，而另一时刻服务可能就被调度到了 Node2 上。所以，访问具体 Pod 是一种不稳定的服务访问方法，而且目前大多数的后端服务都是无状态的服务，直接访问 Pod 也导致不能进行负载均衡。所以，K8s 在此基础上衍生出 Service 的概念。</p>\n<h4 id=\"Service\"><a href=\"#Service\" class=\"headerlink\" title=\"Service\"></a>Service</h4><p>Service 可以看做一组提供相同服务的 Pod 的对外访问接口。Kubernetes 提供三种类型的 Service：</p>\n<ul>\n<li>NodePort： 集群外部可以通过 Node IP 与 Node Port 来访问具体某个 Pod，每台机器上都会暴露同样的端口</li>\n<li>ClusterIP：指通过集群的内部 IP 暴露服务，服务只能够在集群内部可以访问，这也是默认的 ServiceType</li>\n<li>ExternalName：不指向 Pod，指向外部服务<br>Service 和 Deployment 是一对比较容易混淆的概念，两者都是对一组 Pod 进行管理，但它们两者之间的关系可以用下面这张图来概括</li>\n</ul>\n<p><img src=\"/asset/k8s-ramp-up/12.png\"></p>\n<p>Service 是面向服务调用者，也就是外部访问 K8s。而 Deployment 是面向 K8s 底层引擎的，面向内部管理者。</p>\n<p>Service 的配置文件格式与 Deployment 很类似</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: fortune-teller-service\n  namespace: default\nspec:\n  ports:\n  - name: grpc\n    port: 50051\n    protocol: TCP\n    targetPort: 50051\n  selector:\n    k8s-app: fortune-teller-app\n  type: ClusterIP\n</code></pre>\n<p>同样的，我们通过 kubectl apply -f service.yaml 命令，可以创建 Service。通过 kubectl get service 可以查看到刚刚创建的 Service。</p>\n<p><img src=\"/asset/k8s-ramp-up/13.png\"></p>\n<p>接下来，我们登陆到一个 Pod 里去测试一下是否可以访问服务。<br>Netshoot 镜像中包含了一些网络测试的工具，我们可以直接进入一个 netshoot 容器内测试。采用 deployment 的方式创建 Pod</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netshoot\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: netshoot\n  template:\n    metadata:\n      labels:\n        k8s-app: netshoot\n    spec:\n      containers:\n      - args:\n        - 1000d\n        command:\n        - /bin/sleep\n        image: nicolaka/netshoot\n        name: netshoot\n</code></pre>\n<p>与 Docker 的命令类似，使用命令 <code>kubectl cp grpcurl_1.6.0_linux_x86_64.tar.gz &lt;pod name&gt;:/</code> 复制工具到容器内。</p>\n<p>复制成功后，使用命令 kubectl exec -it <pod name> bash 可以进入到容器内。</p>\n<p>解压</p>\n<pre><code class=\"bash\">cd /\ntar -zxf grpcurl_1.6.0_linux_x86_64.tar.gz\n</code></pre>\n<p>然后我们可以测试是否可以从 K8s 集群内访问 Service。对于 K8s 集群内部的服务，K8s 有自己的 DNS 组件，所以可以直接通过服务名访问。</p>\n<pre><code class=\"bash\">./grpcurl -plaintext fortune-teller-service:50051 build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/14.png\"></p>\n<p>验证服务可以从集群内访问之后，我们就需要解决如何从集群外访问服务的问题，毕竟大多数服务是面向 K8s 集群外的用户的。其实目前我们已经了解了一种解决方案，就是使用 Nodeport 类型的 Service。但采用这种方法有几个缺点：</p>\n<ol>\n<li>每个端口只能是一种服务</li>\n<li>端口范围只能是 30000-32767</li>\n<li>如果节点 的 IP 地址发生变化，调用方需要能够察觉。<br>所以，K8s 为服务的外部访问路由提供了新的类型 Ingress。</li>\n</ol>\n<h4 id=\"Ingress\"><a href=\"#Ingress\" class=\"headerlink\" title=\"Ingress\"></a>Ingress</h4><p>Ingress 其实是一种类似于路由表一样的配置，实际的路由工作需要 Ingress Controller 执行。K8s 本身并没有提供 Ingress Controller，目前常用的是通过 Nginx 实现的版本 <a href=\"https://github.com/kubernetes/ingress-nginx\">https://github.com/kubernetes/ingress-nginx</a> 。可以使用上面压缩包中的 ingress-controller.yaml 安装</p>\n<p><img src=\"/asset/k8s-ramp-up/15.png\"></p>\n<p>Ingree nginx controller 通过宿主机暴露给外部访问的端口是随机的，所以我们修改 yaml，改成我们在一开始创建集群时映射的端口。</p>\n<p>通过 <code>kubectl get svc -n ingress-nginx</code> 我们就可以看到，暴露的是两个随机分配的端口</p>\n<p><img src=\"/asset/k8s-ramp-up/16.png\"></p>\n<p>我们手动将其改成 32080 和 32443。</p>\n<p><img src=\"/asset/k8s-ramp-up/17.png\"></p>\n<p>Ingress nginx controller 同样是通过标签选择器的方式管理 Ingress。</p>\n<p>下面是一个简单的 Ingress，将发往 Host fortune.bytedance.com 的请求路由到 Service fortune-teller-service。</p>\n<pre><code class=\"yaml\">apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: GRPC\n  name: fortune-ingress\n  namespace: default\nspec:\n  rules:\n  - host: fortune.bytedance.com\n    http:\n      paths:\n      - backend:\n          serviceName: fortune-teller-service\n          servicePort: grpc\n</code></pre>\n<p>安装 Ingress</p>\n<pre><code class=\"bash\">kubectl apply -f ingress.yaml\nkubectl get ingress\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/18.png\"></p>\n<p>Ingress nginx controller 对于 gRpc 默认只支持 SSL 的形式，而Fedlearner 中的 controller 做了一些定制化操作，使得通过 80 端口，只使用 HTTP2 也可以转发 gRpc。详情可以参考 <a href=\"https://github.com/bytedance/ingress-nginx/pull/2/files\">https://github.com/bytedance/ingress-nginx/pull/2/files</a></p>\n<p>使用下面这个命令，我们可以测试一下从外部访问服务</p>\n<pre><code class=\"bash\">grpcurl -insecure -servername &#39;fortune.bytedance.com&#39; 0.0.0.0:32443 build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/19.png\"></p>\n<p>刚才也提到了，gRpc 通常是使用 SSL 进行加密的，SSL 的关键在于公钥，私钥以及证书的验证。通过文件系统的方式确实可以处理证书的问题，但 K8s 抽象出 Secret 这种资源，大大提高了对这类文件的管理和复用能力。</p>\n<h4 id=\"Secret\"><a href=\"#Secret\" class=\"headerlink\" title=\"Secret\"></a>Secret</h4><p>Secret 解决了密码、token、密钥等敏感数据的存储问题，主要分为三种类型：</p>\n<ul>\n<li>Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的 / run/secrets/kubernetes.io/serviceaccount 目录中</li>\n<li>Opaque ：Base64 编码格式的 Secret，用来存储密码、密钥等</li>\n<li>kubernetes.io/dockerconfigjson ：用来存储 docker registry 的认证信息</li>\n</ul>\n<p>接下来，我们就来创建一个 Opaque 类型的 Secret，使得 ingress nginx controller 支持服务端的 SSL。<br>由于篇幅有限，这里简单介绍下证书相关的概念：</p>\n<ul>\n<li>CA：证书授权中心(certificate authority)，用来签发私钥，并验证公钥，私钥的合法性</li>\n<li>私钥，公钥：私钥用于加密，公钥用于解密</li>\n</ul>\n<p>Secret 支持不编写 yaml，直接从文件中创建 Secret</p>\n<pre><code class=\"bash\">kubectl create secret generic fortune-teller-ssl-verify \\\n  --from-file=ca.crt=CA.pem \\\n  --from-file=tls.crt=server-public.pem \\\n  --from-file=tls.key=server-private.key\n</code></pre>\n<p>修改 Ingress，使其使用新创建的 Secret 提供服务侧的 SSL。</p>\n<pre><code class=\"yaml\">apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: GRPC\n  name: fortune-ingress\n  namespace: default\nspec:\n  rules:\n  - host: fortune.test.com\n    http:\n      paths:\n      - backend:\n          serviceName: fortune-teller-service\n          servicePort: grpc\n  tls:\n  - hosts:\n    - fortune.test.com\n    secretName: fortune-teller-ssl-verify\n</code></pre>\n<p>因为我们使用的是自签名的证书，不被公共的 CA 所信任，所以在发送请求是需要手动指定自己所信任的 CA。</p>\n<pre><code class=\"bash\">grpcurl -cacert CA.pem \\\n  -servername &#39;fortune.test.com&#39; \\\n  127.0.0.1:32443 \\\n  build.stack.fortune.FortuneTeller/Predict\n</code></pre>\n<p><img src=\"/asset/k8s-ramp-up/20.png\"></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ul>\n<li><a href=\"https://draveness.me/docker/\">https://draveness.me/docker/</a></li>\n<li><a href=\"https://www.yuque.com/kshare/2020/fe3b6f86-3b9b-48da-9770-897d838cbf41?language=zh-cn#Namespace\">https://www.yuque.com/kshare/2020/fe3b6f86-3b9b-48da-9770-897d838cbf41?language=zh-cn#Namespace</a></li>\n<li><a href=\"https://network.51cto.com/art/201907/598970.htm\">https://network.51cto.com/art/201907/598970.htm</a></li>\n<li><a href=\"https://blog.csdn.net/gatieme/article/details/51383322\">https://blog.csdn.net/gatieme/article/details/51383322</a></li>\n</ul>\n"},{"title":"[CodeRead] TiDB Lightning","date":"2021-03-19T18:48:44.000Z","updated":"2021-03-19T18:48:44.000Z","_content":"\nTiDB Lightning 是一个将数据导入到 TiDB 中的工具，使用 Go 编写。支持 `Local`, `Importer`, `TiDB` 三种导入模式。在 TiDB 的官网中，对其[原理]()有着详细的介绍。\n\n本文从代码的角度，带领大家走过一个数据导入的过程，所以只关注一些逻辑上重要的步骤，而一些其他的细节可能不会涉及到。\n<!-- more -->\n\n![](/asset/tidb-lightning/1.png)\n首先，进入 main 函数，程序调用了两个主要的启动函数：`GoServer` 和 `RunServer`。\n\n## GoServer\n\n[https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96](https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96)\n\nGoServer 启动了一个 Api Server，有着以下这些 endpoint\n- /web\n- /metrics\n- /debug\n- /tasks\n- /progress\n- /pause\n- /resume\n- /loglevel\n\nApi Server 为整个 lightning 提供了控制，监控和查看进度等功能。是用户与 lightning 交互的入口。而当用户通过 `POST /tasks` 提交了一个数据导入任务时，Api Server 就会向队列中添加一个任务，而同样在监听着队列的 RunServer 就会开始执行。\n\n## RunServer\n\n[https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194](https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194)\n\nRunServer 是真正执行导入的地方。一个 for 循环一直从队列中获取任务配置。在获取到一个任务的配置后，开始正式执行导入任务。\n\n### RegisterMySQL\n\n这个函数的命名有点迷惑，但所幸有注释。RegisterMySQL 同时包含了向 gomysql driver 配置 TLS 和解除配置的两个作用。在运行的一开始，注册 TiDB 的 TLS 配置，这样直接可以使用 `sql.open()` 连接 TiDB。在执行导入任务结束后，通过将 `CAPath` 置为空，删除 TLS 配置。\n\n### Glue\n\nGlue 将数据和导入粘合在一起。这里在创建 Glue 时，就将 TiDB 设为了导入模式。\n\n### MyDumper\n\n确定好目标数据库也就是 backend 后，我们就可以看看数据导入的 frontend 了。`MyDumper` 是一款由 PingCAP 基于社区版，为 TiDB 定制化开发的数据导出工具。但是目前似乎已经不推荐使用了，推荐改用 `dumpling` 了。\n\n在这里创建 MyDumper 时，就会在 `setup` 中从 SQL 文件中。 这里会对表按从小到大进行排序，让小表之后先进行导入，这样可以避免大表导入时阻塞小表释放 index worker。\n\n```go\ntype mdLoaderSetup struct {\n\tloader        *MDLoader\n\tdbSchemas     []FileInfo\n\ttableSchemas  []FileInfo\n\tviewSchemas   []FileInfo\n\ttableDatas    []FileInfo\n\tdbIndexMap    map[string]int\n\ttableIndexMap map[filter.Table]int\n}\n```\n\n### Check\n\n初始化并配置好 MyDumper 后，在正式开跑前，程序还需要进行检查。这里检查了两项：1. `CheckSystemRequirement` 系统要求是否满足，lightning 对于内存要求似乎还挺高。2. `CheckSchemaConflict` 检查目标数据库的 Schema 是不是有冲突。\n\n## RestoreController\n\nlightning 的导入过程由 `RestoreController` 进行控制。`Run` 中的 `opts` 定义了导入过程所有的操作。\n\n### CheckRequirements\n\n通过多态实现，三种 backend 执行不同的检查。\n\n### SetGlobalVariables\n\n在 Server 模式下，目标数据库的 `Collation` 设置都可能不同，所以每次执行任务都需要设置一下。\n\n### RestoreSchema\n\n从这里，lightning 真正开始导入数据。首先，导入 Schema。Schema 包含 `database`, `table`, `view` 三个部分，分别对应着之前从 MyDumper 中导出的三个部分。Lightning 使用 Glue 中封装的目标数据库连接，执行 SQL，导入 Schema 信息。\n\n这里 lightning 使用了一种非常 Go 风格的实现方法。先创建出多个 goroutine，每个 goroutine 都监听着同一个 channel。之后再不断向 channel 中加入要执行的 SQL 任务，从而实现一个类似于线程池的功能。\n\n在导入 Schema 成功后，RestoreSchema 还会负责初始化 Checkpoint，并创建一个 goroutine，用来监听 Checkpoint 的变化，将多个 Checkpoint 合并到一起。\n\n最后，RestoreSchema 还会根据 Schema 中包含的元信息，对之后导入数据时划分的 chunk 数量进行一个估计。\n\n### RestoreTables\n\nRestoreTables 负责从源数据库中导出数据到目标数据库中。\n\n首先，在 `populateChunks` 函数中，使用 MyDumper 将数据划分成多个 chunk，接着将 chunk 加入到其对应的数据 engine 中。然后添加一个索引 engine 的 Checkpoint。\n\n然后，通过 `InsertEngineCheckpoint` 创建每张表的 Checkpoint。\n\n然后，在 `restoreEngines` 中，先将源数据库的数据从导出的文件，并发地转化成 KV 格式的 engine，然后再导入到目标数据库中。关于 engine 的状态机模型，可以参考我的另一篇[文章](https://blog.abingcbc.cn/2021/03/17/tikv-importer)中的总结。这个函数中包含了 lightning 最核心的逻辑，所以比较复杂，大致可以分为以下几个过程：\n\n1. 在 chunkRestore 的 `restore` 中，在 `encodeLoop` 中，利用 MyDumper 的 parser，将数据从不同的导出格式统一转换成 KV 格式，插入到 engine 中。此时，所有的 Checkpoint，包括 data 和 index，都已经写入完成。\n2. 接着，根据 engine 的状态机，关闭 engine 才能开始导入，所以我们现在需要将 engine 的状态置为 close。\n3. 关闭后，开始调用 backend 导入 engine。在所有的 data engine 导入完成后，开始导入 index engine。\n\n这里，又使用了一种非常 Go 风格的方式实现线程池的功能。利用 channel 的缓冲特性，向其中添加 worker，使用时取走，使用完再放回到 channel 中。\n\n最后，对于某些需要执行 post process 的任务，执行一下相应的任务。\n\n### FullCompact\n\n调用 TiKV 的接口，对所有新导入的数据进行压缩。但是由于之前导入数据的时候，会根据是否存在压缩任务，去尝试执行 level-1 的压缩，所以进行全量压缩的时候，需要等待之前启动的 level-1 压缩任务结束。\n\n### SwitchToNormalMode\n\n将 TiKV 切换成正常模式\n\n### CleanCheckpoints\n\n所有的导入任务都结束了，此时 checkpoint 已经无用了。所以，根据用户配置是否删除，处理 checkpoint。","source":"_posts/tidb-lightning.md","raw":"---\ntitle: \"[CodeRead] TiDB Lightning\"\ndate: 2021-03-19 18:48:44\nupdated: 2021-03-19 18:48:44\n---\n\nTiDB Lightning 是一个将数据导入到 TiDB 中的工具，使用 Go 编写。支持 `Local`, `Importer`, `TiDB` 三种导入模式。在 TiDB 的官网中，对其[原理]()有着详细的介绍。\n\n本文从代码的角度，带领大家走过一个数据导入的过程，所以只关注一些逻辑上重要的步骤，而一些其他的细节可能不会涉及到。\n<!-- more -->\n\n![](/asset/tidb-lightning/1.png)\n首先，进入 main 函数，程序调用了两个主要的启动函数：`GoServer` 和 `RunServer`。\n\n## GoServer\n\n[https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96](https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96)\n\nGoServer 启动了一个 Api Server，有着以下这些 endpoint\n- /web\n- /metrics\n- /debug\n- /tasks\n- /progress\n- /pause\n- /resume\n- /loglevel\n\nApi Server 为整个 lightning 提供了控制，监控和查看进度等功能。是用户与 lightning 交互的入口。而当用户通过 `POST /tasks` 提交了一个数据导入任务时，Api Server 就会向队列中添加一个任务，而同样在监听着队列的 RunServer 就会开始执行。\n\n## RunServer\n\n[https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194](https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194)\n\nRunServer 是真正执行导入的地方。一个 for 循环一直从队列中获取任务配置。在获取到一个任务的配置后，开始正式执行导入任务。\n\n### RegisterMySQL\n\n这个函数的命名有点迷惑，但所幸有注释。RegisterMySQL 同时包含了向 gomysql driver 配置 TLS 和解除配置的两个作用。在运行的一开始，注册 TiDB 的 TLS 配置，这样直接可以使用 `sql.open()` 连接 TiDB。在执行导入任务结束后，通过将 `CAPath` 置为空，删除 TLS 配置。\n\n### Glue\n\nGlue 将数据和导入粘合在一起。这里在创建 Glue 时，就将 TiDB 设为了导入模式。\n\n### MyDumper\n\n确定好目标数据库也就是 backend 后，我们就可以看看数据导入的 frontend 了。`MyDumper` 是一款由 PingCAP 基于社区版，为 TiDB 定制化开发的数据导出工具。但是目前似乎已经不推荐使用了，推荐改用 `dumpling` 了。\n\n在这里创建 MyDumper 时，就会在 `setup` 中从 SQL 文件中。 这里会对表按从小到大进行排序，让小表之后先进行导入，这样可以避免大表导入时阻塞小表释放 index worker。\n\n```go\ntype mdLoaderSetup struct {\n\tloader        *MDLoader\n\tdbSchemas     []FileInfo\n\ttableSchemas  []FileInfo\n\tviewSchemas   []FileInfo\n\ttableDatas    []FileInfo\n\tdbIndexMap    map[string]int\n\ttableIndexMap map[filter.Table]int\n}\n```\n\n### Check\n\n初始化并配置好 MyDumper 后，在正式开跑前，程序还需要进行检查。这里检查了两项：1. `CheckSystemRequirement` 系统要求是否满足，lightning 对于内存要求似乎还挺高。2. `CheckSchemaConflict` 检查目标数据库的 Schema 是不是有冲突。\n\n## RestoreController\n\nlightning 的导入过程由 `RestoreController` 进行控制。`Run` 中的 `opts` 定义了导入过程所有的操作。\n\n### CheckRequirements\n\n通过多态实现，三种 backend 执行不同的检查。\n\n### SetGlobalVariables\n\n在 Server 模式下，目标数据库的 `Collation` 设置都可能不同，所以每次执行任务都需要设置一下。\n\n### RestoreSchema\n\n从这里，lightning 真正开始导入数据。首先，导入 Schema。Schema 包含 `database`, `table`, `view` 三个部分，分别对应着之前从 MyDumper 中导出的三个部分。Lightning 使用 Glue 中封装的目标数据库连接，执行 SQL，导入 Schema 信息。\n\n这里 lightning 使用了一种非常 Go 风格的实现方法。先创建出多个 goroutine，每个 goroutine 都监听着同一个 channel。之后再不断向 channel 中加入要执行的 SQL 任务，从而实现一个类似于线程池的功能。\n\n在导入 Schema 成功后，RestoreSchema 还会负责初始化 Checkpoint，并创建一个 goroutine，用来监听 Checkpoint 的变化，将多个 Checkpoint 合并到一起。\n\n最后，RestoreSchema 还会根据 Schema 中包含的元信息，对之后导入数据时划分的 chunk 数量进行一个估计。\n\n### RestoreTables\n\nRestoreTables 负责从源数据库中导出数据到目标数据库中。\n\n首先，在 `populateChunks` 函数中，使用 MyDumper 将数据划分成多个 chunk，接着将 chunk 加入到其对应的数据 engine 中。然后添加一个索引 engine 的 Checkpoint。\n\n然后，通过 `InsertEngineCheckpoint` 创建每张表的 Checkpoint。\n\n然后，在 `restoreEngines` 中，先将源数据库的数据从导出的文件，并发地转化成 KV 格式的 engine，然后再导入到目标数据库中。关于 engine 的状态机模型，可以参考我的另一篇[文章](https://blog.abingcbc.cn/2021/03/17/tikv-importer)中的总结。这个函数中包含了 lightning 最核心的逻辑，所以比较复杂，大致可以分为以下几个过程：\n\n1. 在 chunkRestore 的 `restore` 中，在 `encodeLoop` 中，利用 MyDumper 的 parser，将数据从不同的导出格式统一转换成 KV 格式，插入到 engine 中。此时，所有的 Checkpoint，包括 data 和 index，都已经写入完成。\n2. 接着，根据 engine 的状态机，关闭 engine 才能开始导入，所以我们现在需要将 engine 的状态置为 close。\n3. 关闭后，开始调用 backend 导入 engine。在所有的 data engine 导入完成后，开始导入 index engine。\n\n这里，又使用了一种非常 Go 风格的方式实现线程池的功能。利用 channel 的缓冲特性，向其中添加 worker，使用时取走，使用完再放回到 channel 中。\n\n最后，对于某些需要执行 post process 的任务，执行一下相应的任务。\n\n### FullCompact\n\n调用 TiKV 的接口，对所有新导入的数据进行压缩。但是由于之前导入数据的时候，会根据是否存在压缩任务，去尝试执行 level-1 的压缩，所以进行全量压缩的时候，需要等待之前启动的 level-1 压缩任务结束。\n\n### SwitchToNormalMode\n\n将 TiKV 切换成正常模式\n\n### CleanCheckpoints\n\n所有的导入任务都结束了，此时 checkpoint 已经无用了。所以，根据用户配置是否删除，处理 checkpoint。","slug":"tidb-lightning","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl5i8k7y600081rpfgnmsa04r","content":"<p>TiDB Lightning 是一个将数据导入到 TiDB 中的工具，使用 Go 编写。支持 <code>Local</code>, <code>Importer</code>, <code>TiDB</code> 三种导入模式。在 TiDB 的官网中，对其<a href=\"\">原理</a>有着详细的介绍。</p>\n<p>本文从代码的角度，带领大家走过一个数据导入的过程，所以只关注一些逻辑上重要的步骤，而一些其他的细节可能不会涉及到。</p>\n<span id=\"more\"></span>\n\n<p><img src=\"/asset/tidb-lightning/1.png\"><br>首先，进入 main 函数，程序调用了两个主要的启动函数：<code>GoServer</code> 和 <code>RunServer</code>。</p>\n<h2 id=\"GoServer\"><a href=\"#GoServer\" class=\"headerlink\" title=\"GoServer\"></a>GoServer</h2><p><a href=\"https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96\">https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96</a></p>\n<p>GoServer 启动了一个 Api Server，有着以下这些 endpoint</p>\n<ul>\n<li>/web</li>\n<li>/metrics</li>\n<li>/debug</li>\n<li>/tasks</li>\n<li>/progress</li>\n<li>/pause</li>\n<li>/resume</li>\n<li>/loglevel</li>\n</ul>\n<p>Api Server 为整个 lightning 提供了控制，监控和查看进度等功能。是用户与 lightning 交互的入口。而当用户通过 <code>POST /tasks</code> 提交了一个数据导入任务时，Api Server 就会向队列中添加一个任务，而同样在监听着队列的 RunServer 就会开始执行。</p>\n<h2 id=\"RunServer\"><a href=\"#RunServer\" class=\"headerlink\" title=\"RunServer\"></a>RunServer</h2><p><a href=\"https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194\">https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194</a></p>\n<p>RunServer 是真正执行导入的地方。一个 for 循环一直从队列中获取任务配置。在获取到一个任务的配置后，开始正式执行导入任务。</p>\n<h3 id=\"RegisterMySQL\"><a href=\"#RegisterMySQL\" class=\"headerlink\" title=\"RegisterMySQL\"></a>RegisterMySQL</h3><p>这个函数的命名有点迷惑，但所幸有注释。RegisterMySQL 同时包含了向 gomysql driver 配置 TLS 和解除配置的两个作用。在运行的一开始，注册 TiDB 的 TLS 配置，这样直接可以使用 <code>sql.open()</code> 连接 TiDB。在执行导入任务结束后，通过将 <code>CAPath</code> 置为空，删除 TLS 配置。</p>\n<h3 id=\"Glue\"><a href=\"#Glue\" class=\"headerlink\" title=\"Glue\"></a>Glue</h3><p>Glue 将数据和导入粘合在一起。这里在创建 Glue 时，就将 TiDB 设为了导入模式。</p>\n<h3 id=\"MyDumper\"><a href=\"#MyDumper\" class=\"headerlink\" title=\"MyDumper\"></a>MyDumper</h3><p>确定好目标数据库也就是 backend 后，我们就可以看看数据导入的 frontend 了。<code>MyDumper</code> 是一款由 PingCAP 基于社区版，为 TiDB 定制化开发的数据导出工具。但是目前似乎已经不推荐使用了，推荐改用 <code>dumpling</code> 了。</p>\n<p>在这里创建 MyDumper 时，就会在 <code>setup</code> 中从 SQL 文件中。 这里会对表按从小到大进行排序，让小表之后先进行导入，这样可以避免大表导入时阻塞小表释放 index worker。</p>\n<pre><code class=\"go\">type mdLoaderSetup struct &#123;\n    loader        *MDLoader\n    dbSchemas     []FileInfo\n    tableSchemas  []FileInfo\n    viewSchemas   []FileInfo\n    tableDatas    []FileInfo\n    dbIndexMap    map[string]int\n    tableIndexMap map[filter.Table]int\n&#125;\n</code></pre>\n<h3 id=\"Check\"><a href=\"#Check\" class=\"headerlink\" title=\"Check\"></a>Check</h3><p>初始化并配置好 MyDumper 后，在正式开跑前，程序还需要进行检查。这里检查了两项：1. <code>CheckSystemRequirement</code> 系统要求是否满足，lightning 对于内存要求似乎还挺高。2. <code>CheckSchemaConflict</code> 检查目标数据库的 Schema 是不是有冲突。</p>\n<h2 id=\"RestoreController\"><a href=\"#RestoreController\" class=\"headerlink\" title=\"RestoreController\"></a>RestoreController</h2><p>lightning 的导入过程由 <code>RestoreController</code> 进行控制。<code>Run</code> 中的 <code>opts</code> 定义了导入过程所有的操作。</p>\n<h3 id=\"CheckRequirements\"><a href=\"#CheckRequirements\" class=\"headerlink\" title=\"CheckRequirements\"></a>CheckRequirements</h3><p>通过多态实现，三种 backend 执行不同的检查。</p>\n<h3 id=\"SetGlobalVariables\"><a href=\"#SetGlobalVariables\" class=\"headerlink\" title=\"SetGlobalVariables\"></a>SetGlobalVariables</h3><p>在 Server 模式下，目标数据库的 <code>Collation</code> 设置都可能不同，所以每次执行任务都需要设置一下。</p>\n<h3 id=\"RestoreSchema\"><a href=\"#RestoreSchema\" class=\"headerlink\" title=\"RestoreSchema\"></a>RestoreSchema</h3><p>从这里，lightning 真正开始导入数据。首先，导入 Schema。Schema 包含 <code>database</code>, <code>table</code>, <code>view</code> 三个部分，分别对应着之前从 MyDumper 中导出的三个部分。Lightning 使用 Glue 中封装的目标数据库连接，执行 SQL，导入 Schema 信息。</p>\n<p>这里 lightning 使用了一种非常 Go 风格的实现方法。先创建出多个 goroutine，每个 goroutine 都监听着同一个 channel。之后再不断向 channel 中加入要执行的 SQL 任务，从而实现一个类似于线程池的功能。</p>\n<p>在导入 Schema 成功后，RestoreSchema 还会负责初始化 Checkpoint，并创建一个 goroutine，用来监听 Checkpoint 的变化，将多个 Checkpoint 合并到一起。</p>\n<p>最后，RestoreSchema 还会根据 Schema 中包含的元信息，对之后导入数据时划分的 chunk 数量进行一个估计。</p>\n<h3 id=\"RestoreTables\"><a href=\"#RestoreTables\" class=\"headerlink\" title=\"RestoreTables\"></a>RestoreTables</h3><p>RestoreTables 负责从源数据库中导出数据到目标数据库中。</p>\n<p>首先，在 <code>populateChunks</code> 函数中，使用 MyDumper 将数据划分成多个 chunk，接着将 chunk 加入到其对应的数据 engine 中。然后添加一个索引 engine 的 Checkpoint。</p>\n<p>然后，通过 <code>InsertEngineCheckpoint</code> 创建每张表的 Checkpoint。</p>\n<p>然后，在 <code>restoreEngines</code> 中，先将源数据库的数据从导出的文件，并发地转化成 KV 格式的 engine，然后再导入到目标数据库中。关于 engine 的状态机模型，可以参考我的另一篇<a href=\"https://blog.abingcbc.cn/2021/03/17/tikv-importer\">文章</a>中的总结。这个函数中包含了 lightning 最核心的逻辑，所以比较复杂，大致可以分为以下几个过程：</p>\n<ol>\n<li>在 chunkRestore 的 <code>restore</code> 中，在 <code>encodeLoop</code> 中，利用 MyDumper 的 parser，将数据从不同的导出格式统一转换成 KV 格式，插入到 engine 中。此时，所有的 Checkpoint，包括 data 和 index，都已经写入完成。</li>\n<li>接着，根据 engine 的状态机，关闭 engine 才能开始导入，所以我们现在需要将 engine 的状态置为 close。</li>\n<li>关闭后，开始调用 backend 导入 engine。在所有的 data engine 导入完成后，开始导入 index engine。</li>\n</ol>\n<p>这里，又使用了一种非常 Go 风格的方式实现线程池的功能。利用 channel 的缓冲特性，向其中添加 worker，使用时取走，使用完再放回到 channel 中。</p>\n<p>最后，对于某些需要执行 post process 的任务，执行一下相应的任务。</p>\n<h3 id=\"FullCompact\"><a href=\"#FullCompact\" class=\"headerlink\" title=\"FullCompact\"></a>FullCompact</h3><p>调用 TiKV 的接口，对所有新导入的数据进行压缩。但是由于之前导入数据的时候，会根据是否存在压缩任务，去尝试执行 level-1 的压缩，所以进行全量压缩的时候，需要等待之前启动的 level-1 压缩任务结束。</p>\n<h3 id=\"SwitchToNormalMode\"><a href=\"#SwitchToNormalMode\" class=\"headerlink\" title=\"SwitchToNormalMode\"></a>SwitchToNormalMode</h3><p>将 TiKV 切换成正常模式</p>\n<h3 id=\"CleanCheckpoints\"><a href=\"#CleanCheckpoints\" class=\"headerlink\" title=\"CleanCheckpoints\"></a>CleanCheckpoints</h3><p>所有的导入任务都结束了，此时 checkpoint 已经无用了。所以，根据用户配置是否删除，处理 checkpoint。</p>\n","site":{"data":{}},"excerpt":"<p>TiDB Lightning 是一个将数据导入到 TiDB 中的工具，使用 Go 编写。支持 <code>Local</code>, <code>Importer</code>, <code>TiDB</code> 三种导入模式。在 TiDB 的官网中，对其<a href=\"\">原理</a>有着详细的介绍。</p>\n<p>本文从代码的角度，带领大家走过一个数据导入的过程，所以只关注一些逻辑上重要的步骤，而一些其他的细节可能不会涉及到。</p>","more":"<p><img src=\"/asset/tidb-lightning/1.png\"><br>首先，进入 main 函数，程序调用了两个主要的启动函数：<code>GoServer</code> 和 <code>RunServer</code>。</p>\n<h2 id=\"GoServer\"><a href=\"#GoServer\" class=\"headerlink\" title=\"GoServer\"></a>GoServer</h2><p><a href=\"https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96\">https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L96</a></p>\n<p>GoServer 启动了一个 Api Server，有着以下这些 endpoint</p>\n<ul>\n<li>/web</li>\n<li>/metrics</li>\n<li>/debug</li>\n<li>/tasks</li>\n<li>/progress</li>\n<li>/pause</li>\n<li>/resume</li>\n<li>/loglevel</li>\n</ul>\n<p>Api Server 为整个 lightning 提供了控制，监控和查看进度等功能。是用户与 lightning 交互的入口。而当用户通过 <code>POST /tasks</code> 提交了一个数据导入任务时，Api Server 就会向队列中添加一个任务，而同样在监听着队列的 RunServer 就会开始执行。</p>\n<h2 id=\"RunServer\"><a href=\"#RunServer\" class=\"headerlink\" title=\"RunServer\"></a>RunServer</h2><p><a href=\"https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194\">https://github.com/pingcap/br/blob/0b223bc5358cbe7ef6a54ad10fdd7aca81bf547f/pkg/lightning/lightning.go#L194</a></p>\n<p>RunServer 是真正执行导入的地方。一个 for 循环一直从队列中获取任务配置。在获取到一个任务的配置后，开始正式执行导入任务。</p>\n<h3 id=\"RegisterMySQL\"><a href=\"#RegisterMySQL\" class=\"headerlink\" title=\"RegisterMySQL\"></a>RegisterMySQL</h3><p>这个函数的命名有点迷惑，但所幸有注释。RegisterMySQL 同时包含了向 gomysql driver 配置 TLS 和解除配置的两个作用。在运行的一开始，注册 TiDB 的 TLS 配置，这样直接可以使用 <code>sql.open()</code> 连接 TiDB。在执行导入任务结束后，通过将 <code>CAPath</code> 置为空，删除 TLS 配置。</p>\n<h3 id=\"Glue\"><a href=\"#Glue\" class=\"headerlink\" title=\"Glue\"></a>Glue</h3><p>Glue 将数据和导入粘合在一起。这里在创建 Glue 时，就将 TiDB 设为了导入模式。</p>\n<h3 id=\"MyDumper\"><a href=\"#MyDumper\" class=\"headerlink\" title=\"MyDumper\"></a>MyDumper</h3><p>确定好目标数据库也就是 backend 后，我们就可以看看数据导入的 frontend 了。<code>MyDumper</code> 是一款由 PingCAP 基于社区版，为 TiDB 定制化开发的数据导出工具。但是目前似乎已经不推荐使用了，推荐改用 <code>dumpling</code> 了。</p>\n<p>在这里创建 MyDumper 时，就会在 <code>setup</code> 中从 SQL 文件中。 这里会对表按从小到大进行排序，让小表之后先进行导入，这样可以避免大表导入时阻塞小表释放 index worker。</p>\n<pre><code class=\"go\">type mdLoaderSetup struct &#123;\n    loader        *MDLoader\n    dbSchemas     []FileInfo\n    tableSchemas  []FileInfo\n    viewSchemas   []FileInfo\n    tableDatas    []FileInfo\n    dbIndexMap    map[string]int\n    tableIndexMap map[filter.Table]int\n&#125;\n</code></pre>\n<h3 id=\"Check\"><a href=\"#Check\" class=\"headerlink\" title=\"Check\"></a>Check</h3><p>初始化并配置好 MyDumper 后，在正式开跑前，程序还需要进行检查。这里检查了两项：1. <code>CheckSystemRequirement</code> 系统要求是否满足，lightning 对于内存要求似乎还挺高。2. <code>CheckSchemaConflict</code> 检查目标数据库的 Schema 是不是有冲突。</p>\n<h2 id=\"RestoreController\"><a href=\"#RestoreController\" class=\"headerlink\" title=\"RestoreController\"></a>RestoreController</h2><p>lightning 的导入过程由 <code>RestoreController</code> 进行控制。<code>Run</code> 中的 <code>opts</code> 定义了导入过程所有的操作。</p>\n<h3 id=\"CheckRequirements\"><a href=\"#CheckRequirements\" class=\"headerlink\" title=\"CheckRequirements\"></a>CheckRequirements</h3><p>通过多态实现，三种 backend 执行不同的检查。</p>\n<h3 id=\"SetGlobalVariables\"><a href=\"#SetGlobalVariables\" class=\"headerlink\" title=\"SetGlobalVariables\"></a>SetGlobalVariables</h3><p>在 Server 模式下，目标数据库的 <code>Collation</code> 设置都可能不同，所以每次执行任务都需要设置一下。</p>\n<h3 id=\"RestoreSchema\"><a href=\"#RestoreSchema\" class=\"headerlink\" title=\"RestoreSchema\"></a>RestoreSchema</h3><p>从这里，lightning 真正开始导入数据。首先，导入 Schema。Schema 包含 <code>database</code>, <code>table</code>, <code>view</code> 三个部分，分别对应着之前从 MyDumper 中导出的三个部分。Lightning 使用 Glue 中封装的目标数据库连接，执行 SQL，导入 Schema 信息。</p>\n<p>这里 lightning 使用了一种非常 Go 风格的实现方法。先创建出多个 goroutine，每个 goroutine 都监听着同一个 channel。之后再不断向 channel 中加入要执行的 SQL 任务，从而实现一个类似于线程池的功能。</p>\n<p>在导入 Schema 成功后，RestoreSchema 还会负责初始化 Checkpoint，并创建一个 goroutine，用来监听 Checkpoint 的变化，将多个 Checkpoint 合并到一起。</p>\n<p>最后，RestoreSchema 还会根据 Schema 中包含的元信息，对之后导入数据时划分的 chunk 数量进行一个估计。</p>\n<h3 id=\"RestoreTables\"><a href=\"#RestoreTables\" class=\"headerlink\" title=\"RestoreTables\"></a>RestoreTables</h3><p>RestoreTables 负责从源数据库中导出数据到目标数据库中。</p>\n<p>首先，在 <code>populateChunks</code> 函数中，使用 MyDumper 将数据划分成多个 chunk，接着将 chunk 加入到其对应的数据 engine 中。然后添加一个索引 engine 的 Checkpoint。</p>\n<p>然后，通过 <code>InsertEngineCheckpoint</code> 创建每张表的 Checkpoint。</p>\n<p>然后，在 <code>restoreEngines</code> 中，先将源数据库的数据从导出的文件，并发地转化成 KV 格式的 engine，然后再导入到目标数据库中。关于 engine 的状态机模型，可以参考我的另一篇<a href=\"https://blog.abingcbc.cn/2021/03/17/tikv-importer\">文章</a>中的总结。这个函数中包含了 lightning 最核心的逻辑，所以比较复杂，大致可以分为以下几个过程：</p>\n<ol>\n<li>在 chunkRestore 的 <code>restore</code> 中，在 <code>encodeLoop</code> 中，利用 MyDumper 的 parser，将数据从不同的导出格式统一转换成 KV 格式，插入到 engine 中。此时，所有的 Checkpoint，包括 data 和 index，都已经写入完成。</li>\n<li>接着，根据 engine 的状态机，关闭 engine 才能开始导入，所以我们现在需要将 engine 的状态置为 close。</li>\n<li>关闭后，开始调用 backend 导入 engine。在所有的 data engine 导入完成后，开始导入 index engine。</li>\n</ol>\n<p>这里，又使用了一种非常 Go 风格的方式实现线程池的功能。利用 channel 的缓冲特性，向其中添加 worker，使用时取走，使用完再放回到 channel 中。</p>\n<p>最后，对于某些需要执行 post process 的任务，执行一下相应的任务。</p>\n<h3 id=\"FullCompact\"><a href=\"#FullCompact\" class=\"headerlink\" title=\"FullCompact\"></a>FullCompact</h3><p>调用 TiKV 的接口，对所有新导入的数据进行压缩。但是由于之前导入数据的时候，会根据是否存在压缩任务，去尝试执行 level-1 的压缩，所以进行全量压缩的时候，需要等待之前启动的 level-1 压缩任务结束。</p>\n<h3 id=\"SwitchToNormalMode\"><a href=\"#SwitchToNormalMode\" class=\"headerlink\" title=\"SwitchToNormalMode\"></a>SwitchToNormalMode</h3><p>将 TiKV 切换成正常模式</p>\n<h3 id=\"CleanCheckpoints\"><a href=\"#CleanCheckpoints\" class=\"headerlink\" title=\"CleanCheckpoints\"></a>CleanCheckpoints</h3><p>所有的导入任务都结束了，此时 checkpoint 已经无用了。所以，根据用户配置是否删除，处理 checkpoint。</p>"},{"title":"[PaperRead] Time2graph: Revisiting time series modeling with dynamic shapelets","date":"2021-09-06T18:39:53.000Z","updated":"2021-09-06T18:39:53.000Z","_content":"\nShapelet [<sup>1</sup>](#shapelet) 是一种常见的对时序数据进行建模的方法之一。从 2009 年在 KDD 上发布第一篇论文以来，其经过了各式各样的变形和改进。Time2graph [<sup>2</sup>](#time2graph) 观察到 shapelet 具有 time-aware 的性质，因此提出了一种基于 shapelet 的时序数据的建模方法，将数据建模成图结构。\n<!-- more -->\n\n## What\n### Shapelet\nA shapelet is a segment that is representative of a certain class.\nShapelet 是一段具有代表性的子序列，它可以将一条时序数据的所有分段分成两类，一类与其相似，另一类与其不同。\n### Time-aware shapelet\nTime2graph 中所创新性提出的包含时间信息的 shapelet，利用图对其在时间维度上的变化进行捕捉。\n\n## Why\n传统的 shaplet 以及其改进形式都忽略了 shapelet 在不同的时间区间上的表现能力。这造成两点不足：\n1. 相同 shapelet 在不同时间区间上的含义不同。\n2. shapelet 之间的变化关系也表示了时序数据的一些信息。\n\n## How\n### Time-Aware Shapelet Extraction\nTime-aware shapelet 的抽取可以分为以下三个步骤\n1. 分段\n\n    这里需要超参数如：每个分段的长度\n2. 选取 Candidate\n\n    这一部分在论文中没有叙述，但阅读源代码后，我们可以发现，针对所有分段后得到的片段，有两种方法从中得到 candidate shapelet。第一种是通过聚类，例如 K-Means，和 DTW 作为距离进行聚类，然后选取类中心的片段作为 candidate。第二种则是贪婪的方法，即遍历计算一个片段与其他所有片段之间的距离，然后选取平均最小的片段作为 candidate。\n3. 计算评分\n\n    Time2graph 设计了新的评分方法来衡量 shapelet 在时间维度上的重要度，这部分比较复杂，后文会进行详细的介绍。对所有的 shapelet candidate 打分后，我们就可以选取 top-k 作为真正的 shapelet。\n\n#### Distance\nTime2graph 设计了两个需要训练的参数，local factor **w** 和 global factor **u** 来分别衡量 shapelet 内部每个元素的重要度以及 shapelet 在不同时间段上的重要度。\n\n训练的过程需要进行梯度下降，损失函数如下图所示\n![](/asset/time2graph/loss_function.png)\n\n```\nv: shapelet\nT: time series\nS∗(v, T): the set of distances with respect to a specific group T∗\ng: differentiable function measures the distances between distributions of two finite sets \n```\n最后两项为 penalties。当梯度下降使得损失函数的值越来越小时，`w` 和 `u` 就越来越可以使得 shapelet 到 positive 和 negative 的距离之差越来越大，这样 shapelet 就越能代表一个类。\n\n接下来，详细介绍损失函数中所用到的两个距离函数。\n\n1. shapelet 与 segment 之间的距离\n\n![](/asset/time2graph/dist_seg.png)\n\n```\nw: local factor 需要学习的参数\nu: global factor 需要学习的参数\ns: segment\na: alignment，利用 DTW 可将长度为 i 的 v 和长度为 j 的 s 对齐成长度为 p 的 a1 和 a2\n```\n\n对于 DTW alignment，下面这张图形象地解释了整个对齐的过程\n\n![](/asset/time2graph/dtw.png)\n\n对于序列 s1 和 s2，s1 中的一个点可能对应 s2 中的多个点，同时 s2 中的一个点也可能对应 s1 中的多个点。所以，两者对齐后，会得到一个新的长度的序列。而 shapelet 与 segment 的距离，其实就是计算两者对齐后的欧氏距离。\n\n2. shapelet 与 time series 之间的距离\n\n![](/asset/time2graph/dist_series.png)\n\n这个就比较简单了，`d` 即为刚刚介绍的 shapelet 与 segment 之间的距离。shapelet 与 time series 的距离即为与 time series 的所有分段中的权重最小值。\n\n### Shapelet Evolution Graph\n接下来，就是要构建图来表示 shapelet 之间的转移关系。\n\n![](/asset/time2graph/graph.png)\n\n算法先将 shapelet 与 segment 关联起来，edge 的权重由以下这个公式计算\n\n![](/asset/time2graph/edge_weight.png)\n\n公式计算的是 shapelet 与 segment 相关的概率，如果 shapelet 与 segment 之间的距离越近，那么相关的概率越高。\n\n接下来，算法将相邻的 segment 的 shapelet 连接起来，而 edge 的权重则是前后两个 segment 与各自 shapelet 之间关联概率的乘积，代表了从前一个 shapelet 迁移到后一个 shapelet 的概率。\n\n![](/asset/time2graph/transition.png)\n\n最后，算法将一个 shapelet 所有迁移出去的 edge 的权重进行归一化，使其和为 1。\n\n算法的伪代码如下所示\n\n![](/asset/time2graph/graph_algo.png)\n\n### Representation Learning\n最后，time2graph 采用了 DeepWalk [<sup>3</sup>](#deepwalk)的算法，将 shapelet 转换为 embedding 的形式。\n\n接下来，就可以利用 shapelet 的 embedding 来表示 segment 和 time series。\n\nsegment 的向量化表示如下面这个公式，就是其相关 shapelet 的 embedding 的概率权重和\n\n![](/asset/time2graph/segment.png)\n\n而 time series 的向量化表示就是将其所有 segment 的 embedding 的拼接。\n\n算法的伪代码如下所示\n\n![](/asset/time2graph/representation.png)\n\n### Apply Time2graph\n至此，我们就完整的了解了 time2graph 的训练过程。那么 time2graph 的应用过程也与之类似。\n\n首先，算法的输入仍然是一个 time series。算法会先对其进行分段，然后为每个 segment 分配相关的 shapelet。接下来，就可以得到 segment 和 time series 的 embedding，进行后续聚类或者其他操作。\n\n## Reference\n<div id=\"shapelet\" />\n- [1] Ye, Lexiang, and Eamonn Keogh. \"Time series shapelets: a new primitive for data mining.\" Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 2009.\n\n<div id=\"time2graph\" />\n- [2] Cheng, Ziqiang, et al. \"Time2graph: Revisiting time series modeling with dynamic shapelets.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.\n\n<div id=\"deepwalk\" />\n- [3] Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. \"Deepwalk: Online learning of social representations.\" Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 2014.","source":"_posts/time2graph.md","raw":"---\ntitle: \"[PaperRead] Time2graph: Revisiting time series modeling with dynamic shapelets\"\ndate: 2021-09-06 18:39:53\nupdated: 2021-09-06 18:39:53\n---\n\nShapelet [<sup>1</sup>](#shapelet) 是一种常见的对时序数据进行建模的方法之一。从 2009 年在 KDD 上发布第一篇论文以来，其经过了各式各样的变形和改进。Time2graph [<sup>2</sup>](#time2graph) 观察到 shapelet 具有 time-aware 的性质，因此提出了一种基于 shapelet 的时序数据的建模方法，将数据建模成图结构。\n<!-- more -->\n\n## What\n### Shapelet\nA shapelet is a segment that is representative of a certain class.\nShapelet 是一段具有代表性的子序列，它可以将一条时序数据的所有分段分成两类，一类与其相似，另一类与其不同。\n### Time-aware shapelet\nTime2graph 中所创新性提出的包含时间信息的 shapelet，利用图对其在时间维度上的变化进行捕捉。\n\n## Why\n传统的 shaplet 以及其改进形式都忽略了 shapelet 在不同的时间区间上的表现能力。这造成两点不足：\n1. 相同 shapelet 在不同时间区间上的含义不同。\n2. shapelet 之间的变化关系也表示了时序数据的一些信息。\n\n## How\n### Time-Aware Shapelet Extraction\nTime-aware shapelet 的抽取可以分为以下三个步骤\n1. 分段\n\n    这里需要超参数如：每个分段的长度\n2. 选取 Candidate\n\n    这一部分在论文中没有叙述，但阅读源代码后，我们可以发现，针对所有分段后得到的片段，有两种方法从中得到 candidate shapelet。第一种是通过聚类，例如 K-Means，和 DTW 作为距离进行聚类，然后选取类中心的片段作为 candidate。第二种则是贪婪的方法，即遍历计算一个片段与其他所有片段之间的距离，然后选取平均最小的片段作为 candidate。\n3. 计算评分\n\n    Time2graph 设计了新的评分方法来衡量 shapelet 在时间维度上的重要度，这部分比较复杂，后文会进行详细的介绍。对所有的 shapelet candidate 打分后，我们就可以选取 top-k 作为真正的 shapelet。\n\n#### Distance\nTime2graph 设计了两个需要训练的参数，local factor **w** 和 global factor **u** 来分别衡量 shapelet 内部每个元素的重要度以及 shapelet 在不同时间段上的重要度。\n\n训练的过程需要进行梯度下降，损失函数如下图所示\n![](/asset/time2graph/loss_function.png)\n\n```\nv: shapelet\nT: time series\nS∗(v, T): the set of distances with respect to a specific group T∗\ng: differentiable function measures the distances between distributions of two finite sets \n```\n最后两项为 penalties。当梯度下降使得损失函数的值越来越小时，`w` 和 `u` 就越来越可以使得 shapelet 到 positive 和 negative 的距离之差越来越大，这样 shapelet 就越能代表一个类。\n\n接下来，详细介绍损失函数中所用到的两个距离函数。\n\n1. shapelet 与 segment 之间的距离\n\n![](/asset/time2graph/dist_seg.png)\n\n```\nw: local factor 需要学习的参数\nu: global factor 需要学习的参数\ns: segment\na: alignment，利用 DTW 可将长度为 i 的 v 和长度为 j 的 s 对齐成长度为 p 的 a1 和 a2\n```\n\n对于 DTW alignment，下面这张图形象地解释了整个对齐的过程\n\n![](/asset/time2graph/dtw.png)\n\n对于序列 s1 和 s2，s1 中的一个点可能对应 s2 中的多个点，同时 s2 中的一个点也可能对应 s1 中的多个点。所以，两者对齐后，会得到一个新的长度的序列。而 shapelet 与 segment 的距离，其实就是计算两者对齐后的欧氏距离。\n\n2. shapelet 与 time series 之间的距离\n\n![](/asset/time2graph/dist_series.png)\n\n这个就比较简单了，`d` 即为刚刚介绍的 shapelet 与 segment 之间的距离。shapelet 与 time series 的距离即为与 time series 的所有分段中的权重最小值。\n\n### Shapelet Evolution Graph\n接下来，就是要构建图来表示 shapelet 之间的转移关系。\n\n![](/asset/time2graph/graph.png)\n\n算法先将 shapelet 与 segment 关联起来，edge 的权重由以下这个公式计算\n\n![](/asset/time2graph/edge_weight.png)\n\n公式计算的是 shapelet 与 segment 相关的概率，如果 shapelet 与 segment 之间的距离越近，那么相关的概率越高。\n\n接下来，算法将相邻的 segment 的 shapelet 连接起来，而 edge 的权重则是前后两个 segment 与各自 shapelet 之间关联概率的乘积，代表了从前一个 shapelet 迁移到后一个 shapelet 的概率。\n\n![](/asset/time2graph/transition.png)\n\n最后，算法将一个 shapelet 所有迁移出去的 edge 的权重进行归一化，使其和为 1。\n\n算法的伪代码如下所示\n\n![](/asset/time2graph/graph_algo.png)\n\n### Representation Learning\n最后，time2graph 采用了 DeepWalk [<sup>3</sup>](#deepwalk)的算法，将 shapelet 转换为 embedding 的形式。\n\n接下来，就可以利用 shapelet 的 embedding 来表示 segment 和 time series。\n\nsegment 的向量化表示如下面这个公式，就是其相关 shapelet 的 embedding 的概率权重和\n\n![](/asset/time2graph/segment.png)\n\n而 time series 的向量化表示就是将其所有 segment 的 embedding 的拼接。\n\n算法的伪代码如下所示\n\n![](/asset/time2graph/representation.png)\n\n### Apply Time2graph\n至此，我们就完整的了解了 time2graph 的训练过程。那么 time2graph 的应用过程也与之类似。\n\n首先，算法的输入仍然是一个 time series。算法会先对其进行分段，然后为每个 segment 分配相关的 shapelet。接下来，就可以得到 segment 和 time series 的 embedding，进行后续聚类或者其他操作。\n\n## Reference\n<div id=\"shapelet\" />\n- [1] Ye, Lexiang, and Eamonn Keogh. \"Time series shapelets: a new primitive for data mining.\" Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 2009.\n\n<div id=\"time2graph\" />\n- [2] Cheng, Ziqiang, et al. \"Time2graph: Revisiting time series modeling with dynamic shapelets.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.\n\n<div id=\"deepwalk\" />\n- [3] Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. \"Deepwalk: Online learning of social representations.\" Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 2014.","slug":"time2graph","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl5i8k7y700091rpfhnt9ehwq","content":"<p>Shapelet <a href=\"#shapelet\"><sup>1</sup></a> 是一种常见的对时序数据进行建模的方法之一。从 2009 年在 KDD 上发布第一篇论文以来，其经过了各式各样的变形和改进。Time2graph <a href=\"#time2graph\"><sup>2</sup></a> 观察到 shapelet 具有 time-aware 的性质，因此提出了一种基于 shapelet 的时序数据的建模方法，将数据建模成图结构。</p>\n<span id=\"more\"></span>\n\n<h2 id=\"What\"><a href=\"#What\" class=\"headerlink\" title=\"What\"></a>What</h2><h3 id=\"Shapelet\"><a href=\"#Shapelet\" class=\"headerlink\" title=\"Shapelet\"></a>Shapelet</h3><p>A shapelet is a segment that is representative of a certain class.<br>Shapelet 是一段具有代表性的子序列，它可以将一条时序数据的所有分段分成两类，一类与其相似，另一类与其不同。</p>\n<h3 id=\"Time-aware-shapelet\"><a href=\"#Time-aware-shapelet\" class=\"headerlink\" title=\"Time-aware shapelet\"></a>Time-aware shapelet</h3><p>Time2graph 中所创新性提出的包含时间信息的 shapelet，利用图对其在时间维度上的变化进行捕捉。</p>\n<h2 id=\"Why\"><a href=\"#Why\" class=\"headerlink\" title=\"Why\"></a>Why</h2><p>传统的 shaplet 以及其改进形式都忽略了 shapelet 在不同的时间区间上的表现能力。这造成两点不足：</p>\n<ol>\n<li>相同 shapelet 在不同时间区间上的含义不同。</li>\n<li>shapelet 之间的变化关系也表示了时序数据的一些信息。</li>\n</ol>\n<h2 id=\"How\"><a href=\"#How\" class=\"headerlink\" title=\"How\"></a>How</h2><h3 id=\"Time-Aware-Shapelet-Extraction\"><a href=\"#Time-Aware-Shapelet-Extraction\" class=\"headerlink\" title=\"Time-Aware Shapelet Extraction\"></a>Time-Aware Shapelet Extraction</h3><p>Time-aware shapelet 的抽取可以分为以下三个步骤</p>\n<ol>\n<li><p>分段</p>\n<p> 这里需要超参数如：每个分段的长度</p>\n</li>\n<li><p>选取 Candidate</p>\n<p> 这一部分在论文中没有叙述，但阅读源代码后，我们可以发现，针对所有分段后得到的片段，有两种方法从中得到 candidate shapelet。第一种是通过聚类，例如 K-Means，和 DTW 作为距离进行聚类，然后选取类中心的片段作为 candidate。第二种则是贪婪的方法，即遍历计算一个片段与其他所有片段之间的距离，然后选取平均最小的片段作为 candidate。</p>\n</li>\n<li><p>计算评分</p>\n<p> Time2graph 设计了新的评分方法来衡量 shapelet 在时间维度上的重要度，这部分比较复杂，后文会进行详细的介绍。对所有的 shapelet candidate 打分后，我们就可以选取 top-k 作为真正的 shapelet。</p>\n</li>\n</ol>\n<h4 id=\"Distance\"><a href=\"#Distance\" class=\"headerlink\" title=\"Distance\"></a>Distance</h4><p>Time2graph 设计了两个需要训练的参数，local factor <strong>w</strong> 和 global factor <strong>u</strong> 来分别衡量 shapelet 内部每个元素的重要度以及 shapelet 在不同时间段上的重要度。</p>\n<p>训练的过程需要进行梯度下降，损失函数如下图所示<br><img src=\"/asset/time2graph/loss_function.png\"></p>\n<pre><code>v: shapelet\nT: time series\nS∗(v, T): the set of distances with respect to a specific group T∗\ng: differentiable function measures the distances between distributions of two finite sets \n</code></pre>\n<p>最后两项为 penalties。当梯度下降使得损失函数的值越来越小时，<code>w</code> 和 <code>u</code> 就越来越可以使得 shapelet 到 positive 和 negative 的距离之差越来越大，这样 shapelet 就越能代表一个类。</p>\n<p>接下来，详细介绍损失函数中所用到的两个距离函数。</p>\n<ol>\n<li>shapelet 与 segment 之间的距离</li>\n</ol>\n<p><img src=\"/asset/time2graph/dist_seg.png\"></p>\n<pre><code>w: local factor 需要学习的参数\nu: global factor 需要学习的参数\ns: segment\na: alignment，利用 DTW 可将长度为 i 的 v 和长度为 j 的 s 对齐成长度为 p 的 a1 和 a2\n</code></pre>\n<p>对于 DTW alignment，下面这张图形象地解释了整个对齐的过程</p>\n<p><img src=\"/asset/time2graph/dtw.png\"></p>\n<p>对于序列 s1 和 s2，s1 中的一个点可能对应 s2 中的多个点，同时 s2 中的一个点也可能对应 s1 中的多个点。所以，两者对齐后，会得到一个新的长度的序列。而 shapelet 与 segment 的距离，其实就是计算两者对齐后的欧氏距离。</p>\n<ol start=\"2\">\n<li>shapelet 与 time series 之间的距离</li>\n</ol>\n<p><img src=\"/asset/time2graph/dist_series.png\"></p>\n<p>这个就比较简单了，<code>d</code> 即为刚刚介绍的 shapelet 与 segment 之间的距离。shapelet 与 time series 的距离即为与 time series 的所有分段中的权重最小值。</p>\n<h3 id=\"Shapelet-Evolution-Graph\"><a href=\"#Shapelet-Evolution-Graph\" class=\"headerlink\" title=\"Shapelet Evolution Graph\"></a>Shapelet Evolution Graph</h3><p>接下来，就是要构建图来表示 shapelet 之间的转移关系。</p>\n<p><img src=\"/asset/time2graph/graph.png\"></p>\n<p>算法先将 shapelet 与 segment 关联起来，edge 的权重由以下这个公式计算</p>\n<p><img src=\"/asset/time2graph/edge_weight.png\"></p>\n<p>公式计算的是 shapelet 与 segment 相关的概率，如果 shapelet 与 segment 之间的距离越近，那么相关的概率越高。</p>\n<p>接下来，算法将相邻的 segment 的 shapelet 连接起来，而 edge 的权重则是前后两个 segment 与各自 shapelet 之间关联概率的乘积，代表了从前一个 shapelet 迁移到后一个 shapelet 的概率。</p>\n<p><img src=\"/asset/time2graph/transition.png\"></p>\n<p>最后，算法将一个 shapelet 所有迁移出去的 edge 的权重进行归一化，使其和为 1。</p>\n<p>算法的伪代码如下所示</p>\n<p><img src=\"/asset/time2graph/graph_algo.png\"></p>\n<h3 id=\"Representation-Learning\"><a href=\"#Representation-Learning\" class=\"headerlink\" title=\"Representation Learning\"></a>Representation Learning</h3><p>最后，time2graph 采用了 DeepWalk <a href=\"#deepwalk\"><sup>3</sup></a>的算法，将 shapelet 转换为 embedding 的形式。</p>\n<p>接下来，就可以利用 shapelet 的 embedding 来表示 segment 和 time series。</p>\n<p>segment 的向量化表示如下面这个公式，就是其相关 shapelet 的 embedding 的概率权重和</p>\n<p><img src=\"/asset/time2graph/segment.png\"></p>\n<p>而 time series 的向量化表示就是将其所有 segment 的 embedding 的拼接。</p>\n<p>算法的伪代码如下所示</p>\n<p><img src=\"/asset/time2graph/representation.png\"></p>\n<h3 id=\"Apply-Time2graph\"><a href=\"#Apply-Time2graph\" class=\"headerlink\" title=\"Apply Time2graph\"></a>Apply Time2graph</h3><p>至此，我们就完整的了解了 time2graph 的训练过程。那么 time2graph 的应用过程也与之类似。</p>\n<p>首先，算法的输入仍然是一个 time series。算法会先对其进行分段，然后为每个 segment 分配相关的 shapelet。接下来，就可以得到 segment 和 time series 的 embedding，进行后续聚类或者其他操作。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><div id=\"shapelet\" />\n- [1] Ye, Lexiang, and Eamonn Keogh. \"Time series shapelets: a new primitive for data mining.\" Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 2009.\n\n<div id=\"time2graph\" />\n- [2] Cheng, Ziqiang, et al. \"Time2graph: Revisiting time series modeling with dynamic shapelets.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.\n\n<div id=\"deepwalk\" />\n- [3] Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. \"Deepwalk: Online learning of social representations.\" Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 2014.","site":{"data":{}},"excerpt":"<p>Shapelet <a href=\"#shapelet\"><sup>1</sup></a> 是一种常见的对时序数据进行建模的方法之一。从 2009 年在 KDD 上发布第一篇论文以来，其经过了各式各样的变形和改进。Time2graph <a href=\"#time2graph\"><sup>2</sup></a> 观察到 shapelet 具有 time-aware 的性质，因此提出了一种基于 shapelet 的时序数据的建模方法，将数据建模成图结构。</p>","more":"<h2 id=\"What\"><a href=\"#What\" class=\"headerlink\" title=\"What\"></a>What</h2><h3 id=\"Shapelet\"><a href=\"#Shapelet\" class=\"headerlink\" title=\"Shapelet\"></a>Shapelet</h3><p>A shapelet is a segment that is representative of a certain class.<br>Shapelet 是一段具有代表性的子序列，它可以将一条时序数据的所有分段分成两类，一类与其相似，另一类与其不同。</p>\n<h3 id=\"Time-aware-shapelet\"><a href=\"#Time-aware-shapelet\" class=\"headerlink\" title=\"Time-aware shapelet\"></a>Time-aware shapelet</h3><p>Time2graph 中所创新性提出的包含时间信息的 shapelet，利用图对其在时间维度上的变化进行捕捉。</p>\n<h2 id=\"Why\"><a href=\"#Why\" class=\"headerlink\" title=\"Why\"></a>Why</h2><p>传统的 shaplet 以及其改进形式都忽略了 shapelet 在不同的时间区间上的表现能力。这造成两点不足：</p>\n<ol>\n<li>相同 shapelet 在不同时间区间上的含义不同。</li>\n<li>shapelet 之间的变化关系也表示了时序数据的一些信息。</li>\n</ol>\n<h2 id=\"How\"><a href=\"#How\" class=\"headerlink\" title=\"How\"></a>How</h2><h3 id=\"Time-Aware-Shapelet-Extraction\"><a href=\"#Time-Aware-Shapelet-Extraction\" class=\"headerlink\" title=\"Time-Aware Shapelet Extraction\"></a>Time-Aware Shapelet Extraction</h3><p>Time-aware shapelet 的抽取可以分为以下三个步骤</p>\n<ol>\n<li><p>分段</p>\n<p> 这里需要超参数如：每个分段的长度</p>\n</li>\n<li><p>选取 Candidate</p>\n<p> 这一部分在论文中没有叙述，但阅读源代码后，我们可以发现，针对所有分段后得到的片段，有两种方法从中得到 candidate shapelet。第一种是通过聚类，例如 K-Means，和 DTW 作为距离进行聚类，然后选取类中心的片段作为 candidate。第二种则是贪婪的方法，即遍历计算一个片段与其他所有片段之间的距离，然后选取平均最小的片段作为 candidate。</p>\n</li>\n<li><p>计算评分</p>\n<p> Time2graph 设计了新的评分方法来衡量 shapelet 在时间维度上的重要度，这部分比较复杂，后文会进行详细的介绍。对所有的 shapelet candidate 打分后，我们就可以选取 top-k 作为真正的 shapelet。</p>\n</li>\n</ol>\n<h4 id=\"Distance\"><a href=\"#Distance\" class=\"headerlink\" title=\"Distance\"></a>Distance</h4><p>Time2graph 设计了两个需要训练的参数，local factor <strong>w</strong> 和 global factor <strong>u</strong> 来分别衡量 shapelet 内部每个元素的重要度以及 shapelet 在不同时间段上的重要度。</p>\n<p>训练的过程需要进行梯度下降，损失函数如下图所示<br><img src=\"/asset/time2graph/loss_function.png\"></p>\n<pre><code>v: shapelet\nT: time series\nS∗(v, T): the set of distances with respect to a specific group T∗\ng: differentiable function measures the distances between distributions of two finite sets \n</code></pre>\n<p>最后两项为 penalties。当梯度下降使得损失函数的值越来越小时，<code>w</code> 和 <code>u</code> 就越来越可以使得 shapelet 到 positive 和 negative 的距离之差越来越大，这样 shapelet 就越能代表一个类。</p>\n<p>接下来，详细介绍损失函数中所用到的两个距离函数。</p>\n<ol>\n<li>shapelet 与 segment 之间的距离</li>\n</ol>\n<p><img src=\"/asset/time2graph/dist_seg.png\"></p>\n<pre><code>w: local factor 需要学习的参数\nu: global factor 需要学习的参数\ns: segment\na: alignment，利用 DTW 可将长度为 i 的 v 和长度为 j 的 s 对齐成长度为 p 的 a1 和 a2\n</code></pre>\n<p>对于 DTW alignment，下面这张图形象地解释了整个对齐的过程</p>\n<p><img src=\"/asset/time2graph/dtw.png\"></p>\n<p>对于序列 s1 和 s2，s1 中的一个点可能对应 s2 中的多个点，同时 s2 中的一个点也可能对应 s1 中的多个点。所以，两者对齐后，会得到一个新的长度的序列。而 shapelet 与 segment 的距离，其实就是计算两者对齐后的欧氏距离。</p>\n<ol start=\"2\">\n<li>shapelet 与 time series 之间的距离</li>\n</ol>\n<p><img src=\"/asset/time2graph/dist_series.png\"></p>\n<p>这个就比较简单了，<code>d</code> 即为刚刚介绍的 shapelet 与 segment 之间的距离。shapelet 与 time series 的距离即为与 time series 的所有分段中的权重最小值。</p>\n<h3 id=\"Shapelet-Evolution-Graph\"><a href=\"#Shapelet-Evolution-Graph\" class=\"headerlink\" title=\"Shapelet Evolution Graph\"></a>Shapelet Evolution Graph</h3><p>接下来，就是要构建图来表示 shapelet 之间的转移关系。</p>\n<p><img src=\"/asset/time2graph/graph.png\"></p>\n<p>算法先将 shapelet 与 segment 关联起来，edge 的权重由以下这个公式计算</p>\n<p><img src=\"/asset/time2graph/edge_weight.png\"></p>\n<p>公式计算的是 shapelet 与 segment 相关的概率，如果 shapelet 与 segment 之间的距离越近，那么相关的概率越高。</p>\n<p>接下来，算法将相邻的 segment 的 shapelet 连接起来，而 edge 的权重则是前后两个 segment 与各自 shapelet 之间关联概率的乘积，代表了从前一个 shapelet 迁移到后一个 shapelet 的概率。</p>\n<p><img src=\"/asset/time2graph/transition.png\"></p>\n<p>最后，算法将一个 shapelet 所有迁移出去的 edge 的权重进行归一化，使其和为 1。</p>\n<p>算法的伪代码如下所示</p>\n<p><img src=\"/asset/time2graph/graph_algo.png\"></p>\n<h3 id=\"Representation-Learning\"><a href=\"#Representation-Learning\" class=\"headerlink\" title=\"Representation Learning\"></a>Representation Learning</h3><p>最后，time2graph 采用了 DeepWalk <a href=\"#deepwalk\"><sup>3</sup></a>的算法，将 shapelet 转换为 embedding 的形式。</p>\n<p>接下来，就可以利用 shapelet 的 embedding 来表示 segment 和 time series。</p>\n<p>segment 的向量化表示如下面这个公式，就是其相关 shapelet 的 embedding 的概率权重和</p>\n<p><img src=\"/asset/time2graph/segment.png\"></p>\n<p>而 time series 的向量化表示就是将其所有 segment 的 embedding 的拼接。</p>\n<p>算法的伪代码如下所示</p>\n<p><img src=\"/asset/time2graph/representation.png\"></p>\n<h3 id=\"Apply-Time2graph\"><a href=\"#Apply-Time2graph\" class=\"headerlink\" title=\"Apply Time2graph\"></a>Apply Time2graph</h3><p>至此，我们就完整的了解了 time2graph 的训练过程。那么 time2graph 的应用过程也与之类似。</p>\n<p>首先，算法的输入仍然是一个 time series。算法会先对其进行分段，然后为每个 segment 分配相关的 shapelet。接下来，就可以得到 segment 和 time series 的 embedding，进行后续聚类或者其他操作。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><div id=\"shapelet\" />\n- [1] Ye, Lexiang, and Eamonn Keogh. \"Time series shapelets: a new primitive for data mining.\" Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 2009.\n\n<div id=\"time2graph\" />\n- [2] Cheng, Ziqiang, et al. \"Time2graph: Revisiting time series modeling with dynamic shapelets.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.\n\n<div id=\"deepwalk\" />\n- [3] Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. \"Deepwalk: Online learning of social representations.\" Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 2014."}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}