<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning | Abing's Blog</title><link>https://blog.abingcbc.cn/tags/deep-learning/</link><atom:link href="https://blog.abingcbc.cn/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><description>Deep Learning</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>zh-cn</language><lastBuildDate>Fri, 15 Jul 2022 22:58:41 +0000</lastBuildDate><item><title>Awesome Tech Post 第0期</title><link>https://blog.abingcbc.cn/posts/awesome-tech-post-0/</link><pubDate>Fri, 15 Jul 2022 22:58:41 +0000</pubDate><guid>https://blog.abingcbc.cn/posts/awesome-tech-post-0/</guid><description>&lt;p>每期 Awesome Tech Post 都会摘录推荐 5 篇优质技术博客，对这些文章的内容进行提炼总结。每期覆盖领域各不相同，可能从后端到可视化，从工程到算法等等，但每篇文章都会对领域内的某一问题进行深入分析或提出独到见解。欢迎大家私信推荐文章~&lt;/p>
&lt;h2 id="tldr" class="relative group">TL;DR &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#tldr" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h2>
&lt;p>本期内容：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>如何对 Kubernetes Operator 进行分布式 Tracing &lt;code>Cloud&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Kubernetes 多集群管理与联邦 &lt;code>Cloud&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>强化学习中对无效动作的 mask &lt;code>Reinforcement Learning&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>一些深度学习中的采样方式和损失函数 &lt;code>Deep Learning&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如何在一个全新的领域开展学习 &lt;code>Soft Skill&lt;/code>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="0-how-to-monitor-kubernetes-operators-by-distributed-tracing" class="relative group">0. How to Monitor Kubernetes Operators By Distributed-Tracing? &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#0-how-to-monitor-kubernetes-operators-by-distributed-tracing" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h2>
&lt;p>原文链接：&lt;a
href="https://yue9944882.github.io/posts/how-to-monitor-kubernetes-operator-by-distributed-tracing/"
target="_blank" rel="noreferrer noopener"
>https://yue9944882.github.io/posts/how-to-monitor-kubernetes-operator-by-distributed-tracing/&lt;/a>&lt;/p>
&lt;p>在一个健全的系统中，应当对一条请求在完整生命周期中完成了哪些处理都进行监控追踪。随着现在大量的分布式应用和微服务的落地，一条请求可能跨越多个服务，甚至集群。对于这类请求的 tracing（追踪）问题就是 distributed tracing。Tracing 的整个流程可以被建模成一个树，其中每个节点是请求所经过的处理（根据监控的粒度，可以是一个服务，也可以是一个函数）。请求经过的每个处理被称作为 span。&lt;/p>
&lt;h3 id="异步问题" class="relative group">异步问题 &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#%e5%bc%82%e6%ad%a5%e9%97%ae%e9%a2%98" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h3>
&lt;p>在微服务中，一条请求 R 到达微服务1后，一般通过 HTTP 或 RPC，进一步请求到微服务2进行处理，再到微服务N。最后，沿着这一条链，进行反向的返回 response。显然，这是一个同步的过程，我们可以很清晰的看出 R 的处理流程，R 的 tracing 结果就是这条转发树。&lt;/p>
&lt;p style="text-align: center">
&lt;img title="" src="./2022-07-12-22-28-28-image.png" alt="" data-align="center" width="764">
&lt;/p>
&lt;p>然而，与微服务不同，k8s operator 采用了完全不同的协作模式。K8s operator 不会与其他 operator 直接进行交互，而是生成一个 Event（例如，创建一个 Pod），k8s 会将这个 Event 放入到一个队列中。所有的 operator 都不断地轮询，从这个队列中获取符合自己过滤条件的 Event。显然，这是一个异步的过程。一异步，问题就麻烦了：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Event 的生成/消费不是线性的&lt;/p>
&lt;p>Event 的生成和消费不是一一对应的。一方面，operator 可能将多个 Event 合并成一个任务，或者将一个 Event 分成多个任务。另一方面，一个任务由于重试策略可能会执行多次（在一个 operator 上产生多个 span）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Event 循环&lt;/p>
&lt;p>当一个 operator 根据 Event 改变 k8s 的资源后，又会产生一个新的 Event（k8s 中有资源被改变）。这就提出了一个问题，什么是这次 tracing 的结束？详细的讨论可以参考原文。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="operator-分类" class="relative group">Operator 分类 &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#operator-%e5%88%86%e7%b1%bb" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h3>
&lt;p>每个 operator 都可能监听一个主要的 resource 和多个次要的 resource，因此，可以对 operator 进行如下的分类：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Type A：operator 只接收 Event。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Type B：operator 接收 Event，对 k8s 外的系统进行操作。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Type C：operator 只修改自己监听的资源。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Type D：operator 只修改不被自己监听的资源。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Type E：operator 修改任意的资源（Type C + Type D）。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>对于 Type A 和 Type B 来说，请求到他们这里就结束了，所以他们是 leaf span。&lt;/p>
&lt;p>对于 Type C 和 Type D 来说，由于不能确定有没有其他 operator 在监听同一个资源，所以无法判断其是否是 leaf span。对于 Type C 可以肯定的是，在 operator 完成最后一次 write 的时候，他仍会收到一个 Event（因为它所修改的资源正是自己监听的资源），并且会 drop 这个 Event（这个 Event 是由自身修改产生的，无意义）。因此，我们可以确定这个 operator 上多次 span 的 parent/child 关系。而对于 Type D，无法收到修改资源的最后一次 write 的 Event，所以，我们只能建立这个 operator 上多次 span 之间较弱的 link 关系。&lt;/p>
&lt;p>对于 Type E 来说，这是最复杂但又是最常见的类型。Type E 其实是 Type C 和 Type D 的组合，所以对于 Type E 操作的每个资源，我们可以按照资源的类型，将 Type E 当前的 span 暂时转换成 Type C 或 Type D 来处理。&lt;/p>
&lt;h2 id="1-kubernetes集群联邦和资源分发" class="relative group">1. Kubernetes、集群联邦和资源分发 &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#1-kubernetes%e9%9b%86%e7%be%a4%e8%81%94%e9%82%a6%e5%92%8c%e8%b5%84%e6%ba%90%e5%88%86%e5%8f%91" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h2>
&lt;p>原文链接：&lt;a
href="https://draveness.me/kuberentes-federation/"
target="_blank" rel="noreferrer noopener"
>https://draveness.me/kuberentes-federation/&lt;/a>&lt;/p>
&lt;p>Kubernetes 目前最多可以支持管理 5000 个节点，对于超过 5000 个节点的集群管理，就需要寻找其他方法对多个 K8s 集群进行管理。多集群其实不是一个新的概念，在很久之前，就在业界看到过 Mesos + K8s 的多集群管理方法。但是，多集群中的每个集群都相对独立，彼此之间没有联系，每个服务都是独立的运行在一个集群里的。而集群联邦则是在此基础上增加了两个重要的功能：跨集群的服务发现和跨集群的调度，使得一个多应用服务可以运行在多个集群上，进一步提升了服务的稳定性和可用性。&lt;/p>
&lt;p>文章中，作者以两个比较出名的集群联邦项目为例，介绍了目前集群联邦的方案：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Kubefed&lt;/strong> 会为每个原生资源（e.g. Deployment）生成对应的联邦资源（e.g. FederatedDeployment）作为管理。联邦资源中会包含 Template（资源信息）、Placement（所需部署的集群）和 Overrides（同步资源到集群时，需要覆写的属性）三个部分。在分发到下游集群时，Kubefed 再根据联邦资源生成具体的原生资源。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Karmada&lt;/strong> 是 Kubefed 项目的延续，其中的概念也几乎全盘继承自 Kubefed。稍有不同的是，Karmada 保留了原生资源，并将 Kubefed 中联邦资源的 Placement 和 Override 抽离了出来，作为两个新的自定义资源 PropagationPolicy 和 OverriderPolicy。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;figure>
&lt;p style="text-align: center">
&lt;img title="" src="./0a98b72fdc1acab504726847f894f6f9df8a699e.png" alt="16477814091159-kubefed-karmada-api.png" data-align="center" width="497">
&lt;/p>
&lt;figcaption align = "center">图片来自原文&lt;/figcaption>
&lt;/figure>
&lt;p>对于任务调度来说，文章中提到了“因为上下文的不足，集群联邦不需要也没有办法保证调度的全局最优解，而提供跨集群的部署和故障转移就已经可以满足常见的需求了”。&lt;/p>
&lt;h2 id="2-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms" class="relative group">2. A Closer Look at Invalid Action Masking in Policy Gradient Algorithms &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#2-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h2>
&lt;p>原文链接：&lt;a
href="https://costa.sh/blog-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms.html"
target="_blank" rel="noreferrer noopener"
>https://costa.sh/blog-a-closer-look-at-invalid-action-masking-in-policy-gradient-algorithms.html&lt;/a>&lt;/p>
&lt;p>本篇文章是作者对所发表的同名论文 &lt;a
href="https://arxiv.org/abs/2006.14171"
target="_blank" rel="noreferrer noopener"
>https://arxiv.org/abs/2006.14171&lt;/a> 的介绍。对强化学习稍微有所了解的同学应该都知道 Policy Gradient，属于强化学习的两大分类之一。而 Invalid Action（无效动作）是强化学习中经常遇到的问题，例如，在训练模型走迷宫时，前方有障碍物，那么前进这一动作就是 invalid 的。那么，在训练时，需要对模型过滤掉这类动作，也就是 masking。而本篇文章就在尝试解释 Policy Gradient 算法中 invalid action masking 的工作原理。虽然 masking 在很多论文里都用到了，但都只是一句话带过（我之前读到的几篇甚至不会提到这些细节），没有对 masking 的原理进行深入探索。这也是文章作者的 motivition 之一。&lt;/p>
&lt;p>简单来说，invalid action masking 就是在模型根据概率采样动作时，采用一个 mask 将 invalid action 的概率置为 0。文章中作者将 invalid action masking 建模成以下的函数 \(inv_s\)：&lt;/p>
&lt;p style="text-align: center">
&lt;img title="" src="./2022-07-14-21-55-41-image.png" alt="" data-align="center" width="300">
&lt;/p>
&lt;p>\(l(s)\) 是状态 \(s\) 的 log 值。\(inv_s\) 在两种情况下都是可微的，在常数 \(M\) 时，梯度为0，因此，反向传播时不会更新模型有关 invalid action 相关的参数。&lt;/p>
&lt;p>policy 在采样时的概率为：&lt;/p>
&lt;p style="text-align: center">
&lt;img title="" src="./2022-07-14-21-56-23-image.png" alt="" data-align="center" width="235">
&lt;/p>
&lt;p>文章中还通过量化的实验结果来验证 invalid action masking 的有效性，详情可以阅读原文。&lt;/p>
&lt;h2 id="3-深度学习新的采样方式和损失函数--论文笔记" class="relative group">3. 深度学习新的采样方式和损失函数&amp;ndash;论文笔记 &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#3-%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%96%b0%e7%9a%84%e9%87%87%e6%a0%b7%e6%96%b9%e5%bc%8f%e5%92%8c%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0--%e8%ae%ba%e6%96%87%e7%ac%94%e8%ae%b0" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h2>
&lt;p>原文链接：&lt;a
href="https://zhuanlan.zhihu.com/p/27748177"
target="_blank" rel="noreferrer noopener"
>https://zhuanlan.zhihu.com/p/27748177&lt;/a>&lt;/p>
&lt;p>本篇文章是对论文《Sampling matters in deep embedding learning》&lt;a
href="https://arxiv.org/pdf/1706.07567.pdf"
target="_blank" rel="noreferrer noopener"
>https://arxiv.org/pdf/1706.07567.pdf&lt;/a> 的概述。论文主要解决的是 deep embedding learning 中的采样问题和损失函数问题。文章对论文的主要内容进行了很好的概述，这里就不再赘述了，就简单的罗列一些 insight 和文章中没解释清楚的部分：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Triplet loss 优于 Contrastive loss 的原因有两点：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>constrative loss 假设所有样本都符合相同分布，而 triplet loss 没有这个假设。因此，可以适应各种空间形状，一定程度上能够抵御噪声的影响&lt;/p>
&lt;/li>
&lt;li>
&lt;p>triplet loss 优化的目标是正负样本之间的相对距离差，即正样本之间的距离小于正负样本之间的距离。而 constrative loss 优化的目标是绝对距离，即所有的正样本之间的距离也要尽可能小。这是没有必要的。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>假设负样本均匀分布，我们也均匀随机采样。那么，采样的负样本 pairwise distance 符合如下的分布：&lt;/p>
&lt;p style="text-align: center">
&lt;img title="test" src="./fdad5d725c64c0b589b8d107e5e228d285497474.png" alt="" data-align="center" width="318">
&lt;/p>
&lt;p>换句话说，在高维空间里，采样得到的负样本 pairwise distance 基本上都是大于 \(\sqrt{2}\) 的。论文针对这个问题，提出的方法是 distance weighted sampling。以距离概率值的倒数 \(q(d)^{-1}\) 作为样本采样的权重，这样在修正样本距离分布的 bias 的同时控制了 variance。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Triplet loss 采用的是一种 hard negtive mining 的方法，也就是正负样本的区分是 hard 的。负样本的梯度通过如下的公式计算：&lt;/p>
&lt;img src="./f5026649684b309b85e2f5880e0a84bfdd7c01de.png" title="" alt="v2-966f53117397f560a6395fef136ca5f8_1440w.png" data-align="center">
&lt;p>梯度的方向取决于 \(h_{an}\) ，即 anchor 样本与负样本的向量差。那么，如果差向量的绝对值特别小，并且这个负样本是异常值，那对模型的梯度会造成很大的影响。&lt;/p>
&lt;p style="text-align: center">
&lt;img src="./bd74f1ae5c04b74ee412affefedd54fceffb790d.png" title="" alt="v2-80e1c0da65e11100f88b6175857a79ba_1440w.png" data-align="center">
&lt;/p>
&lt;p>上图中展示了随着 pairwise distance 的增加，各种 loss 中正负样本是如何变化的。蓝色实线是正样本，绿色虚线是负样本。对于图b，\(D_{an}\) 越小，梯度值也越趋向于 0。根据 triplet loss 的计算公式，我们可以发现，这导致了模型趋向于这个点的梯度（正）会很大，但是远离这个点的梯度（负）很小。论文提出了 margin based loss 来解决这一问题，即图 d 中的 loss。在 \(D_{an}\) 很小的时候，仍然保证了负样本的梯度为一个常数，这有点类似 ReLU 的思想。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="4-如何在一个全新的领域展开学习" class="relative group">4. 如何在一个全新的领域展开学习 &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#4-%e5%a6%82%e4%bd%95%e5%9c%a8%e4%b8%80%e4%b8%aa%e5%85%a8%e6%96%b0%e7%9a%84%e9%a2%86%e5%9f%9f%e5%b1%95%e5%bc%80%e5%ad%a6%e4%b9%a0" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h2>
&lt;p>原文链接：&lt;a
href="https://ichn.xyz/blog/how-to-start-learning-in-a-new-area"
target="_blank" rel="noreferrer noopener"
>https://ichn.xyz/blog/how-to-start-learning-in-a-new-area&lt;/a>&lt;/p>
&lt;p>学习计算机最重要的点在于关注能力的成长。而所有能力中最重要的，莫过于学习的能力，学习能力是培养其他能力的元能力。文章作者在接触了大量计算机细分领域后，总结出了几点特定的套路：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>明确动机&lt;/p>
&lt;p>要有明确且实际的需求。一是可以解决兴趣使然导致的选择困难，二是在学习的过程中，感受到切实的正反馈。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>背景调查&lt;/p>
&lt;p>一旦明确了学习的动机和目标，应该更系统性地、刻意地对这个领域展开背景调查。&lt;/p>
&lt;blockquote>
&lt;p>知道即将学习的知识可以解决什么样的问题，这种解决手段和其他方式相比的优劣，这个领域和其他领域、特别是自己已经熟悉的领域的关系是怎样的，这个领域的发展历史和发展脉络是怎样的，有哪些独特且重要的概念？&lt;/p>
&lt;p>背景调查获取的信息通常是宏观或者碎片化的，这并不是真正的学习。但这个过程可以提高你对这个领域的熟悉程度，在你的话语体系和思考方式中加入这个领域的成分，并提高你对这个领域的品味与认知。&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>资源汇集与整理&lt;/p>
&lt;p>了解这个领域有哪些重要的资料，更重要的是会拥有判断这个方向的学习资料的优劣的能力。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>制定计划，然后无情地执行&lt;/p>
&lt;p>如果执行学习计划中会有枯燥的感觉，就需要回顾自己的动机、目标，并稍稍跳出来重新审视一下学习计划。&lt;/p>
&lt;p>如果确认学习路径的正确性，就应该专注，而不是继续在这个领域中漫无目的的探索，这样才能进入深水区。&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Time2graph: Revisiting time series modeling with dynamic shapelets</title><link>https://blog.abingcbc.cn/posts/time2graph/</link><pubDate>Mon, 06 Sep 2021 18:39:53 +0000</pubDate><guid>https://blog.abingcbc.cn/posts/time2graph/</guid><description>&lt;p>Shapelet &lt;a
href="#shapelet"
>&lt;sup>1&lt;/sup>&lt;/a> 是一种常见的对时序数据进行建模的方法之一。从 2009 年在 KDD 上发布第一篇论文以来，其经过了各式各样的变形和改进。Time2graph &lt;a
href="#time2graph"
>&lt;sup>2&lt;/sup>&lt;/a> 观察到 shapelet 具有 time-aware 的性质，因此提出了一种基于 shapelet 的时序数据的建模方法，将数据建模成图结构。&lt;/p>
&lt;h2 id="what" class="relative group">What &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#what" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h2>
&lt;h3 id="shapelet" class="relative group">Shapelet &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#shapelet" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h3>
&lt;p>A shapelet is a segment that is representative of a certain class.
Shapelet 是一段具有代表性的子序列，它可以将一条时序数据的所有分段分成两类，一类与其相似，另一类与其不同。&lt;/p>
&lt;h3 id="time-aware-shapelet" class="relative group">Time-aware shapelet &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#time-aware-shapelet" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h3>
&lt;p>Time2graph 中所创新性提出的包含时间信息的 shapelet，利用图对其在时间维度上的变化进行捕捉。&lt;/p>
&lt;h2 id="why" class="relative group">Why &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#why" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h2>
&lt;p>传统的 shaplet 以及其改进形式都忽略了 shapelet 在不同的时间区间上的表现能力。这造成两点不足：&lt;/p>
&lt;ol>
&lt;li>相同 shapelet 在不同时间区间上的含义不同。&lt;/li>
&lt;li>shapelet 之间的变化关系也表示了时序数据的一些信息。&lt;/li>
&lt;/ol>
&lt;h2 id="how" class="relative group">How &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#how" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h2>
&lt;h3 id="time-aware-shapelet-extraction" class="relative group">Time-Aware Shapelet Extraction &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#time-aware-shapelet-extraction" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h3>
&lt;p>Time-aware shapelet 的抽取可以分为以下三个步骤&lt;/p>
&lt;ol>
&lt;li>
&lt;p>分段&lt;/p>
&lt;p>这里需要超参数如：每个分段的长度&lt;/p>
&lt;/li>
&lt;li>
&lt;p>选取 Candidate&lt;/p>
&lt;p>这一部分在论文中没有叙述，但阅读源代码后，我们可以发现，针对所有分段后得到的片段，有两种方法从中得到 candidate shapelet。第一种是通过聚类，例如 K-Means，和 DTW 作为距离进行聚类，然后选取类中心的片段作为 candidate。第二种则是贪婪的方法，即遍历计算一个片段与其他所有片段之间的距离，然后选取平均最小的片段作为 candidate。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>计算评分&lt;/p>
&lt;p>Time2graph 设计了新的评分方法来衡量 shapelet 在时间维度上的重要度，这部分比较复杂，后文会进行详细的介绍。对所有的 shapelet candidate 打分后，我们就可以选取 top-k 作为真正的 shapelet。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="distance" class="relative group">Distance &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#distance" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h4>
&lt;p>Time2graph 设计了两个需要训练的参数，local factor &lt;strong>w&lt;/strong> 和 global factor &lt;strong>u&lt;/strong> 来分别衡量 shapelet 内部每个元素的重要度以及 shapelet 在不同时间段上的重要度。&lt;/p>
&lt;p>训练的过程需要进行梯度下降，损失函数如下图所示
&lt;figure>
&lt;img class="mx-auto my-0 rounded-md" src="./loss_function.png" alt="" />
&lt;/figure>
&lt;/p>
&lt;pre tabindex="0">&lt;code>v: shapelet
T: time series
S∗(v, T): the set of distances with respect to a specific group T∗
g: differentiable function measures the distances between distributions of two finite sets
&lt;/code>&lt;/pre>&lt;p>最后两项为 penalties。当梯度下降使得损失函数的值越来越小时，&lt;code>w&lt;/code> 和 &lt;code>u&lt;/code> 就越来越可以使得 shapelet 到 positive 和 negative 的距离之差越来越大，这样 shapelet 就越能代表一个类。&lt;/p>
&lt;p>接下来，详细介绍损失函数中所用到的两个距离函数。&lt;/p>
&lt;ol>
&lt;li>shapelet 与 segment 之间的距离&lt;/li>
&lt;/ol>
&lt;p>
&lt;figure>
&lt;img class="mx-auto my-0 rounded-md" src="./dist_seg.png" alt="" />
&lt;/figure>
&lt;/p>
&lt;pre tabindex="0">&lt;code>w: local factor 需要学习的参数
u: global factor 需要学习的参数
s: segment
a: alignment，利用 DTW 可将长度为 i 的 v 和长度为 j 的 s 对齐成长度为 p 的 a1 和 a2
&lt;/code>&lt;/pre>&lt;p>对于 DTW alignment，下面这张图形象地解释了整个对齐的过程&lt;/p>
&lt;p>
&lt;figure>
&lt;img class="mx-auto my-0 rounded-md" src="./dtw.png" alt="" />
&lt;/figure>
&lt;/p>
&lt;p>对于序列 s1 和 s2，s1 中的一个点可能对应 s2 中的多个点，同时 s2 中的一个点也可能对应 s1 中的多个点。所以，两者对齐后，会得到一个新的长度的序列。而 shapelet 与 segment 的距离，其实就是计算两者对齐后的欧氏距离。&lt;/p>
&lt;ol start="2">
&lt;li>shapelet 与 time series 之间的距离&lt;/li>
&lt;/ol>
&lt;p>
&lt;figure>
&lt;img class="mx-auto my-0 rounded-md" src="./dist_series.png" alt="" />
&lt;/figure>
&lt;/p>
&lt;p>这个就比较简单了，&lt;code>d&lt;/code> 即为刚刚介绍的 shapelet 与 segment 之间的距离。shapelet 与 time series 的距离即为与 time series 的所有分段中的权重最小值。&lt;/p>
&lt;h3 id="shapelet-evolution-graph" class="relative group">Shapelet Evolution Graph &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#shapelet-evolution-graph" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h3>
&lt;p>接下来，就是要构建图来表示 shapelet 之间的转移关系。&lt;/p>
&lt;p>
&lt;figure>
&lt;img class="mx-auto my-0 rounded-md" src="./graph.png" alt="" />
&lt;/figure>
&lt;/p>
&lt;p>算法先将 shapelet 与 segment 关联起来，edge 的权重由以下这个公式计算&lt;/p>
&lt;p>
&lt;figure>
&lt;img class="mx-auto my-0 rounded-md" src="./edge_weight.png" alt="" />
&lt;/figure>
&lt;/p>
&lt;p>公式计算的是 shapelet 与 segment 相关的概率，如果 shapelet 与 segment 之间的距离越近，那么相关的概率越高。&lt;/p>
&lt;p>接下来，算法将相邻的 segment 的 shapelet 连接起来，而 edge 的权重则是前后两个 segment 与各自 shapelet 之间关联概率的乘积，代表了从前一个 shapelet 迁移到后一个 shapelet 的概率。&lt;/p>
&lt;p>
&lt;figure>
&lt;img class="mx-auto my-0 rounded-md" src="./transition.png" alt="" />
&lt;/figure>
&lt;/p>
&lt;p>最后，算法将一个 shapelet 所有迁移出去的 edge 的权重进行归一化，使其和为 1。&lt;/p>
&lt;p>算法的伪代码如下所示&lt;/p>
&lt;p>
&lt;figure>
&lt;img class="mx-auto my-0 rounded-md" src="./graph_algo.png" alt="" />
&lt;/figure>
&lt;/p>
&lt;h3 id="representation-learning" class="relative group">Representation Learning &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#representation-learning" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h3>
&lt;p>最后，time2graph 采用了 DeepWalk &lt;a
href="#deepwalk"
>&lt;sup>3&lt;/sup>&lt;/a>的算法，将 shapelet 转换为 embedding 的形式。&lt;/p>
&lt;p>接下来，就可以利用 shapelet 的 embedding 来表示 segment 和 time series。&lt;/p>
&lt;p>segment 的向量化表示如下面这个公式，就是其相关 shapelet 的 embedding 的概率权重和&lt;/p>
&lt;p>
&lt;figure>
&lt;img class="mx-auto my-0 rounded-md" src="./segment.png" alt="" />
&lt;/figure>
&lt;/p>
&lt;p>而 time series 的向量化表示就是将其所有 segment 的 embedding 的拼接。&lt;/p>
&lt;p>算法的伪代码如下所示&lt;/p>
&lt;p>
&lt;figure>
&lt;img class="mx-auto my-0 rounded-md" src="./representation.png" alt="" />
&lt;/figure>
&lt;/p>
&lt;h3 id="apply-time2graph" class="relative group">Apply Time2graph &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#apply-time2graph" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h3>
&lt;p>至此，我们就完整的了解了 time2graph 的训练过程。那么 time2graph 的应用过程也与之类似。&lt;/p>
&lt;p>首先，算法的输入仍然是一个 time series。算法会先对其进行分段，然后为每个 segment 分配相关的 shapelet。接下来，就可以得到 segment 和 time series 的 embedding，进行后续聚类或者其他操作。&lt;/p>
&lt;h2 id="reference" class="relative group">Reference &lt;span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100">&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#reference" aria-label="锚点">#&lt;/a>&lt;/span>&lt;/h2>
&lt;div id="shapelet" />
- [1] Ye, Lexiang, and Eamonn Keogh. "Time series shapelets: a new primitive for data mining." Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 2009.
&lt;div id="time2graph" />
- [2] Cheng, Ziqiang, et al. "Time2graph: Revisiting time series modeling with dynamic shapelets." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.
&lt;div id="deepwalk" />
- [3] Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. "Deepwalk: Online learning of social representations." Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 2014.</description></item></channel></rss>